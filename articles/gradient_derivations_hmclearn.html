<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Gradient Derivations for HMC â€¢ hmclearn</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/sandstone/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Gradient Derivations for HMC">
<meta property="og:description" content="hmclearn">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">hmclearn</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.0.2</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/gradient_derivations_hmclearn.html">Gradient Derivations for HMC</a>
    </li>
    <li>
      <a href="../articles/linear_mixed_effects_hmclearn.html">hmclearn package:  Linear Mixed Effects Regression Example</a>
    </li>
    <li>
      <a href="../articles/linear_regression_hmclearn.html">hmclearn package:  Linear Regression Example</a>
    </li>
    <li>
      <a href="../articles/logistic_mixed_effects_hmclearn.html">hmclearn package:  Logistic Mixed Effects Regression Example</a>
    </li>
    <li>
      <a href="../articles/logistic_regression_hmclearn.html">hmclearn:  Logistic Regression Example</a>
    </li>
    <li>
      <a href="../articles/poisson_regression_hmclearn.html">hmclearn:  Poisson Regression Example</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Gradient Derivations for HMC</h1>
                        <h4 class="author">Samuel Thomas</h4>
            
            <h4 class="date">5/21/2020</h4>
      
      
      <div class="hidden name"><code>gradient_derivations_hmclearn.Rmd</code></div>

    </div>

    
    
<div id="linear-regression" class="section level2">
<h2 class="hasAnchor">
<a href="#linear-regression" class="anchor"></a>Linear Regression</h2>
<p>Likelihood for linear regression</p>
<p><span class="math display">\[
f(\mathbf{y} | \mathbf{X}, \boldsymbol\beta, \sigma^2) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left( -\frac{1}{2\sigma^2} (\mathbf{y} - \mathbf{X}\boldsymbol\beta)^T(\mathbf{y} - \mathbf{X}\boldsymbol\beta) \right)
\]</span></p>
<p>Log likelihood</p>
<p><span class="math display">\[
\log f(\mathbf{y}|\mathbf{X}, \boldsymbol\beta, \sigma^2) = -\frac{n}{2}\log\sigma^2 - \frac{1}{2\sigma^2} (\mathbf{y} - \mathbf{X}\boldsymbol\beta)^T(\mathbf{y} - \mathbf{X}\boldsymbol\beta) 
\]</span></p>
<p>Specify priors</p>
<p><span class="math display">\[
\begin{aligned}
\boldsymbol\beta &amp;\sim N(0, \sigma_\beta^2 \mathbf{I}) \\
\sigma^2 &amp;\sim IG(a, b)
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
f(\boldsymbol\beta) &amp;\propto \exp(-\frac{\boldsymbol\beta^T \mathbf{I} \boldsymbol\beta}{2\sigma_\beta^2})
\\
f(\sigma^2) &amp;= \frac{b^a}{\Gamma{(a)}}(\sigma^2)^{-a-1}\exp\left( -\frac{b}{\sigma^2} \right) \\
&amp;\propto (\sigma^2)^{-a-1}\exp\left( -\frac{b}{\sigma^2} \right)
\end{aligned} 
\]</span> One variable transformation (general)</p>
<p><span class="math display">\[
f_Y(y) = f_X(g^{-1}(y))\left | \frac{dx}{dy}  \right |
\]</span></p>
<p>Transformation for <span class="math inline">\(\sigma^2\)</span> where <span class="math inline">\(\gamma = \log\sigma^2\)</span>, for support on <span class="math inline">\(\mathbb{R}\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\sigma^2 &amp;= g^{-1}(\gamma) = e^{\gamma} \\
f(\gamma) &amp;= \frac{b^a}{\Gamma{(a)}} (e^\gamma)^{-a-1}  \exp\left(- \frac{b}{e^{\gamma}}  \right) e^{\gamma} \\
&amp;\propto \exp{\left(-a\gamma - \frac{b}{e^{\gamma}} \right)}
\end{aligned}
\]</span></p>
<p>Set <span class="math inline">\(\boldsymbol\theta := (\boldsymbol\beta, \gamma)\)</span></p>
<p>Log prior</p>
<p><span class="math display">\[
\begin{aligned}
\log{f(\boldsymbol\theta)} &amp;= \log f(\boldsymbol\beta, \gamma) = \log f(\boldsymbol\beta | \gamma) f(\gamma) = \log f(\boldsymbol\beta) + \log f(\gamma) \\
&amp;\propto -\frac{\boldsymbol\beta^T \mathbf{I} \boldsymbol\beta}{2\sigma_\beta^2} - \left(a\gamma + \frac{b}{e^{\gamma}} \right)
\end{aligned}
\]</span></p>
<p>Log posterior = log likelihood + log prior</p>
<p><span class="math display">\[
\begin{aligned}
\log f(\boldsymbol\theta | \mathbf{y}, \mathbf{X}) &amp;\propto  -\frac{n}{2}\gamma - \frac{1}{2e^\gamma} (\mathbf{y} - \mathbf{X}\boldsymbol\beta)^T(\mathbf{y} - \mathbf{X}\boldsymbol\beta) -\frac{\boldsymbol\beta^T \mathbf{I} \boldsymbol\beta}{2\sigma_\beta^2} - \left(a\gamma + \frac{b}{e^{\gamma}} \right) \\
&amp;\propto -\left (\frac{n}{2} + a  \right)\gamma - \frac{e^{-\gamma}}{2} (\mathbf{y} - \mathbf{X}\boldsymbol\beta)^T(\mathbf{y} - \mathbf{X}\boldsymbol\beta)-\frac{\boldsymbol\beta^T \mathbf{I} \boldsymbol\beta}{2\sigma_\beta^2} - b e^{-\gamma}
\end{aligned}
\]</span></p>
<p>Gradient of the log posterior</p>
<p><span class="math display">\[
\begin{aligned}
\nabla_{\boldsymbol\beta} \log(\boldsymbol\theta) &amp;\propto -\frac{e^{-\gamma}}{2} \left( \mathbf{X}^T (\mathbf{X}\boldsymbol\beta - \mathbf{y})  + \mathbf{X}^T (\mathbf{X}\boldsymbol\beta - \mathbf{y}) \right) - \frac{1}{\sigma_\beta^2} \mathbf{I} \boldsymbol\beta\\
&amp;\propto -e^{-\gamma} \mathbf{X}^T (\mathbf{X}\boldsymbol\beta - \mathbf{y})- \frac{1}{\sigma_\beta^2} \mathbf{I}\boldsymbol\beta\\
&amp;\propto e^{-\gamma} \mathbf{X}^T ( \mathbf{y} - \mathbf{X}\boldsymbol\beta)- \frac{1}{\sigma_\beta^2} \mathbf{I}\boldsymbol\beta  \\
\nabla_\gamma\log\boldsymbol\theta &amp;\propto -\left (\frac{n}{2} + a  \right) +  \frac{e^{-\gamma}}{2} (\mathbf{y} - \mathbf{X}\boldsymbol\beta)^T(\mathbf{y} - \mathbf{X}\boldsymbol\beta) + b e^{-\gamma}
\end{aligned}
\]</span></p>
<div style="page-break-after: always;"></div>
</div>
<div id="logistic-regression" class="section level2">
<h2 class="hasAnchor">
<a href="#logistic-regression" class="anchor"></a>Logistic Regression</h2>
<p>Probability</p>
<p><span class="math display">\[
P(\mathbf{Y} = \mathbf{y}|\mathbf{X}) = [1+\exp(-\mathbf{X}\boldsymbol\beta)]^{-1}
\]</span></p>
<p>Likelihood</p>
<p><span class="math display">\[
\begin{aligned}
f(\mathbf{y} | \mathbf{X}, \boldsymbol\beta) &amp;= \prod_{i=1}^n p(y_i)^{y_i}(1-p(y_i))^{1-y_i} \\
&amp;= \prod_{i=1}^n \left(\frac{1}{1+e^{-\mathbf{x}_i\boldsymbol\beta}}\right)^{y_i} \left(\frac{e^{-\mathbf{x}_i\boldsymbol\beta}}{1+e^{-\mathbf{x}_i\boldsymbol\beta}}\right)^{1-y_i} 
\end{aligned}
\]</span></p>
<p>Log likelihood</p>
<p><span class="math display">\[
\begin{aligned}
\log f(\mathbf{y}|\mathbf{X}, \boldsymbol\beta) &amp;= \sum_{i=1}^n -y_i\log(1 + e^{-\mathbf{x}_i\boldsymbol\beta}) - (1-y_i)\mathbf{x}_i\boldsymbol\beta - (1 - y_i)\log(1 + e^{-\mathbf{x}_i\boldsymbol\beta}) \\
&amp;= \sum_{i=1}^n -y_i\log(1+e^{-\mathbf{x}_i\boldsymbol\beta}) - \mathbf{x}_i\boldsymbol\beta + y_i\mathbf{x}_i\boldsymbol\beta - \log(1 + e^{-\mathbf{x}_i\boldsymbol\beta})+y_i\log(1+e^{-\mathbf{x}_i\boldsymbol\beta}) \\
&amp;= \sum_{i=1}^n -\mathbf{x}_i\boldsymbol\beta + y_i\mathbf{x}_i\boldsymbol\beta - \log(1 + e^{-\mathbf{x}_i\boldsymbol\beta}) \\
&amp;= \sum_{i=1}^n (y_i-1)\mathbf{x}_i\boldsymbol\beta - \log(1 + e^{-\mathbf{x}_i\boldsymbol\beta}) \\
&amp;= (\mathbf{y} - \mathbf{1}_n)^T \mathbf{X}\boldsymbol\beta - \mathbf{1}_n^T\log(1 + e^{-\mathbf{X}\boldsymbol\beta})
\end{aligned}
\]</span></p>
<p>Specify priors</p>
<p><span class="math display">\[
\begin{aligned}
\boldsymbol\beta &amp;\sim N(0, \sigma_\beta^2 \mathbf{I}) 
\end{aligned}
\]</span></p>
<p>Set <span class="math inline">\(\boldsymbol\theta := (\boldsymbol\beta, \gamma)\)</span></p>
<p>Log prior</p>
<p><span class="math display">\[
\begin{aligned}
\log{f(\boldsymbol\theta)} &amp;= \log f(\boldsymbol\beta)  \\
&amp;\propto -\frac{\boldsymbol\beta^T \mathbf{I} \boldsymbol\beta}{2\sigma_\beta^2} 
\end{aligned}
\]</span></p>
<p>Log posterior = log likelihood + log prior</p>
<p><span class="math display">\[
\begin{aligned}
\log f(\boldsymbol\beta | \mathbf{y}, \mathbf{X}) &amp;\propto (\mathbf{y} - \mathbf{1}_n)^T \mathbf{X}\boldsymbol\beta - \mathbf{1}_n^T\log(1 + e^{-\mathbf{X}\boldsymbol\beta}) -\frac{\boldsymbol\beta^T \mathbf{I} \boldsymbol\beta}{2\sigma_\beta^2} \\
&amp;\propto \boldsymbol\beta^T\mathbf{X}^T(\mathbf{y} - \mathbf{1}_n) - [\log(1 + e^{-\mathbf{X}\boldsymbol\beta})]^T\mathbf{1}_n -\frac{\boldsymbol\beta^T \mathbf{I} \boldsymbol\beta}{2\sigma_\beta^2} 
\end{aligned}
\]</span> Gradient of the log posterior</p>
<p>This gradient is a bit complex, we will split into 3 functions</p>
<p><span class="math display">\[
\begin{aligned}
f=\log f(\boldsymbol\beta, \mathbf{y}, \mathbf{X}) &amp;\propto f_1(\mathbf{X}, \boldsymbol\beta) + f_2(\mathbf{X}, \boldsymbol\beta) + f_3(\mathbf{X}, \boldsymbol\beta)
\end{aligned}
\]</span></p>
<p>Gradient</p>
<p><span class="math display">\[
\begin{aligned}
\nabla_\beta f &amp;\propto \nabla_\beta f_1(\mathbf{X},\boldsymbol\beta) + \nabla_\beta f_2(\mathbf{X},\boldsymbol\beta) + \nabla_\beta f_3(\mathbf{X}, \boldsymbol\beta)
\end{aligned}
\]</span> First and third functions simplest</p>
<p><span class="math display">\[
\begin{aligned}
\nabla_\beta f_1(\mathbf{X}, \boldsymbol\beta) &amp;= \nabla_\beta (\mathbf{y} - \mathbf{1}_n)^T \mathbf{X}\boldsymbol\beta \\
&amp;= \mathbf{X}^T (\mathbf{y} - \mathbf{1}_n)\\
\nabla_\beta f_3(\mathbf{X}, \boldsymbol\beta) &amp;= -\nabla_\beta \frac{\boldsymbol\beta^T \mathbf{I} \boldsymbol\beta}{2\sigma_\beta^2} \\
&amp;= -\frac{1}{\sigma_\beta^2}\mathbf{I}\boldsymbol\beta
\end{aligned}
\]</span></p>
<p>Second function more complex</p>
<p><span class="math display">\[
\begin{aligned}
f_2(\mathbf{X}, \boldsymbol\beta) &amp;=  -[\log(1 + e^{-\mathbf{X}\boldsymbol\beta})]^T \mathbf{1}_n  \\
&amp;= -\begin{bmatrix}
\log (1 + e^{-\mathbf{x}_1^T\boldsymbol\beta}) &amp;
\log (1 + e^{-\mathbf{x}_2^T\boldsymbol\beta}) &amp;
... &amp;
\log (1 + e^{-\mathbf{x}_n^T\boldsymbol\beta})
\end{bmatrix} \mathbf{1}_n\\
\nabla_\beta f_2(\mathbf{X}, \boldsymbol\beta) &amp;= -
\begin{bmatrix}
\frac{1}{1 + e^{-\mathbf{x}_1^T\boldsymbol\beta}}e^{-\mathbf{x}_1^T\boldsymbol\beta}(-\mathbf{x}_1) &amp;
\frac{1}{1 + e^{-\mathbf{x}_2^T\boldsymbol\beta}}e^{-\mathbf{x}_2^T\boldsymbol\beta}(-\mathbf{x}_2) &amp;
... &amp;
\frac{1}{1 + e^{-\mathbf{x}_n^T\boldsymbol\beta}}e^{-\mathbf{x}_n^T\boldsymbol\beta}(-\mathbf{x}_n) 
\end{bmatrix} \mathbf{1}_n\\
&amp;= \mathbf{X}^T \left(\frac{e^{-\mathbf{X}\boldsymbol\beta}}{1 + e^{-\mathbf{X}\boldsymbol\beta}}\right) 
\end{aligned}
\]</span></p>
<p>Finally, the gradient of the log posterior</p>
<p><span class="math display">\[
\begin{aligned}
\nabla_\beta \log f(\boldsymbol\beta, \mathbf{y}, \mathbf{X}) &amp;\propto  \mathbf{X}^T (\mathbf{y} - \mathbf{1}_n) +\mathbf{X}^T \left(\frac{e^{-\mathbf{X}\boldsymbol\beta}}{1 + e^{-\mathbf{X}\boldsymbol\beta}}\right)  -\frac{1}{\sigma_\beta^2}\mathbf{I}\boldsymbol\beta \\
&amp;\propto \mathbf{X}^T \left(\mathbf{y} - \mathbf{1}_n + \frac{e^{-\mathbf{X}\boldsymbol\beta}}{1 + e^{-\mathbf{X}\boldsymbol\beta}} \right) -\frac{1}{\sigma_\beta^2}\mathbf{I}\boldsymbol\beta 
\end{aligned}
\]</span></p>
<div style="page-break-after: always;"></div>
</div>
<div id="poisson-regression" class="section level2">
<h2 class="hasAnchor">
<a href="#poisson-regression" class="anchor"></a>Poisson regression</h2>
<p>Probability</p>
<p><span class="math display">\[
p(y | \mu) = \frac{e^{-\mu}\mu^y}{y!}
\]</span></p>
<p>Link function</p>
<p><span class="math display">\[
\begin{aligned}
\boldsymbol\mu &amp;:= E(\mathbf{y} | \mathbf{X}, \boldsymbol\beta) = e^{\mathbf{X}\boldsymbol\beta}  \\
\log \boldsymbol\mu &amp;:= \mathbf{X}\boldsymbol\beta 
\end{aligned}
\]</span></p>
<p>Likelihood</p>
<p><span class="math display">\[
\begin{aligned}
f(\mathbf{y} | \mathbf{X}, \boldsymbol\beta) &amp;= \prod_{i=1}^n \frac{e^{-\mu_i} \mu_i^{y_i}}{y_i!} \\
&amp;= \prod_{i=1}^n \frac{e^{-e^{\mathbf{x}_i^T\boldsymbol\beta}}(e^{\mathbf{x}_i^T\boldsymbol\beta})^{y_i}}{y_i!} \\
&amp;= \prod_{i=1}^n \frac{e^{-e^{\mathbf{x}_i^T\boldsymbol\beta}}e^{\mathbf{x}_i^T\boldsymbol\beta y_i} }{y_i!} 
\end{aligned}
\]</span></p>
<p>Log likelihood without constants</p>
<p><span class="math display">\[
\begin{aligned}
\log f(\mathbf{y} | \mathbf{X}, \boldsymbol\beta) &amp;\propto \sum_{i=1}^n \log e^{-e^{\mathbf{x}_i^T\boldsymbol\beta}} + \log e^{\mathbf{x}_i^T\boldsymbol\beta y_i} \\
&amp;\propto \sum_{i=1}^n -e^{\mathbf{x}_i^T\boldsymbol\beta} + \mathbf{x}_i^T\boldsymbol\beta y_i \\
&amp;\propto \mathbf{y}^T\mathbf{X}\boldsymbol\beta - \mathbf{1}_n^T e^{\mathbf{X}\boldsymbol\beta}
\end{aligned}
\]</span></p>
<p>Specify priors</p>
<p><span class="math display">\[
\begin{aligned}
\boldsymbol\beta &amp;\sim N(0, \sigma_\beta^2 \mathbf{I}) 
\end{aligned}
\]</span></p>
<p>Set <span class="math inline">\(\boldsymbol\theta := (\boldsymbol\beta, \gamma)\)</span></p>
<p>Log prior</p>
<p><span class="math display">\[
\begin{aligned}
\log{f(\boldsymbol\theta)} &amp;= \log f(\boldsymbol\beta)  \\
&amp;\propto -\frac{\boldsymbol\beta^T \mathbf{I} \boldsymbol\beta}{2\sigma_\beta^2} 
\end{aligned}
\]</span></p>
<p>Log posterior = log likelihood + log prior</p>
<p><span class="math display">\[
\begin{aligned}
\log f(\boldsymbol\beta | \mathbf{y}, \mathbf{X}) &amp;\propto \mathbf{y}^T\mathbf{X}\boldsymbol\beta - \mathbf{1}_n^T e^{\mathbf{X}\boldsymbol\beta} -\frac{\boldsymbol\beta^T \mathbf{I} \boldsymbol\beta}{2\sigma_\beta^2} 
\end{aligned}
\]</span></p>
<p>Gradient of the log posterior</p>
<p><span class="math display">\[
\nabla_\beta \log f(\boldsymbol\beta | \mathbf{y}, \mathbf{X}) \propto \mathbf{X}^T (\mathbf{y} - e^{\mathbf{X}\boldsymbol\beta}) - \frac{\mathbf{I}\boldsymbol\beta}{\sigma_\beta^2}
\]</span></p>
<div style="page-break-after: always;"></div>
</div>
<div id="linear-mixed-effects-model" class="section level2">
<h2 class="hasAnchor">
<a href="#linear-mixed-effects-model" class="anchor"></a>Linear mixed effects model</h2>
<p>Specify the model</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{y} &amp;= \mathbf{X}\boldsymbol\beta + \mathbf{Z}\mathbf{u} + \boldsymbol\epsilon \\
\mathbf{u} &amp;\sim N(0, \mathbf{G}) \\
\boldsymbol\epsilon &amp;\sim N(0, \sigma_\epsilon^2)
\end{aligned}
\]</span></p>
<p>Response for each subject is a vector <span class="math inline">\(\mathbf{y} = (\mathbf{y}_1, ..., \mathbf{y}_n)\)</span> for <span class="math inline">\(n\)</span> subjects <span class="math inline">\(i= 1, ..., n\)</span>. Each subject has <span class="math inline">\(d\)</span> observations <span class="math inline">\(\mathbf{y}_i = (y_{i1}, ..., y_{id})\)</span> and we let <span class="math inline">\(j = 1, ..., d\)</span>. The fixed effect design matrix is composed of matrices for each subject, <span class="math inline">\(\mathbf{X} = (\mathbf{X}_1, ..., \mathbf{X}_n)\)</span>, and <span class="math inline">\(\mathbf{X}_i \in \mathbb{R}^{d\times (q+1)}\)</span> for the fixed effects parameters <span class="math inline">\(\boldsymbol\beta = (\beta_0, ..., \beta_q)\)</span>. The full fixed effects design matrix is therefore <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{nd \times (q+1)}\)</span>.</p>
<p>For random effects, <span class="math inline">\(\mathbf{Z} = \text{diag}(\mathbf{Z}_1, ..., \mathbf{Z}_n)\)</span>, with individual random effects matrices <span class="math inline">\(\mathbf{Z}_i\)</span> for each of the <span class="math inline">\(i\)</span> subjects. A random intercept model specifies <span class="math inline">\(\mathbf{Z}_i\)</span> as a column vector of ones where <span class="math inline">\(\mathbf{Z}_i = \mathbf{z}_i = \mathbf{1}_d\)</span>. The full random effects design matrix <span class="math inline">\(\mathbf{Z} \in \mathbb{R}^{nd\times n}\)</span>. The parameterization for random effects is <span class="math inline">\(\mathbf{u} = (\mathbf{u}_1, ..., \mathbf{u}_n)^T\)</span> with vectors <span class="math inline">\(\mathbf{u}_i\)</span> for each subject. A random intercept model is somewhat simplified where <span class="math inline">\(\mathbf{u}_i = u_i\)</span> denotes a single random intercept parameter for each subject <span class="math inline">\(i\)</span>, and <span class="math inline">\(\mathbf{u} = (u_1, ..., u_n)^T\)</span>.</p>
<p>We set <span class="math inline">\(\mathbf{u}\)</span> as one of our priors, following a multivariate normal distribution, <span class="math inline">\(\mathbf{u} \sim N(0, \mathbf{G})\)</span>. For our random intercept model, the specification of the covariance matrix <span class="math inline">\(\mathbf{G}\)</span> is expanded to facilitate efficient sampling using HMC. We let <span class="math inline">\(\mathbf{u} = \mathbf{G}^{1/2}\boldsymbol\tau\)</span> where <span class="math inline">\(\mathbf{G}^{1/2} = \lambda \mathbf{I}_n\)</span>. An additional parameter <span class="math inline">\(\boldsymbol\tau = (\tau_1, ..., \tau_n)^T\)</span> where each of these parameters is standard normal <span class="math inline">\(\tau_i \sim N(0, 1)\)</span>. The full covariance matrix is then <span class="math inline">\(\mathbf{G} = \lambda^2 \mathbf{I}_n \boldsymbol\tau\)</span>.</p>
<p>The error for each subject <span class="math inline">\(i\)</span> is <span class="math inline">\(\boldsymbol\epsilon_i \sim N(0, \sigma_{\epsilon}^2 \mathbf{I}_d)\)</span>. Since the error distribution is constant for each observations,</p>
<p><span class="math display">\[
\boldsymbol\epsilon \sim N(0, \sigma_{\epsilon}^2 \mathbf{I}_{nd})
\]</span></p>
<p>For pedagogical purposes, we derive and code the log posterior and gradient for the random intercept model only. Additional functions will be developed for more general mixed effect model specifications.</p>
<p>Random intercept model:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{y}_i &amp;= \mathbf{X}_i\boldsymbol\beta + \mathbf{z}_i u_i + \boldsymbol\epsilon_i \\
 &amp;= \mathbf{X}_i\boldsymbol\beta + \mathbf{1}_d u_i + \boldsymbol\epsilon_i
\end{aligned}
\]</span></p>
<p>Expand by individual observation</p>
<p><span class="math display">\[
y_{ij} = x_{ij}\boldsymbol\beta + z_{ij} u_i + \epsilon_{ij}
\]</span></p>
<p>Likelihood for random intercept model</p>
<p><span class="math display">\[
f(\mathbf{y} | \mathbf{X}, \mathbf{Z}, \boldsymbol\beta, \mathbf{u}, \sigma^2) = \frac{1}{(2\pi\sigma_\epsilon^2)^{nd/2}}\exp\left( -\frac{1}{2\sigma^2} (\mathbf{y} - \mathbf{X}\boldsymbol\beta - \mathbf{Z}\mathbf{u})^T(\mathbf{y} - \mathbf{X}\boldsymbol\beta - \mathbf{Z}\mathbf{u}) \right)
\]</span></p>
<p>Log likelihood excluding constants</p>
<p><span class="math display">\[
\log f(\mathbf{y}|\mathbf{X}, \mathbf{Z}, \boldsymbol\beta, \mathbf{u}, \sigma_\epsilon) = -\frac{nd}{2}\log\sigma_\epsilon^2 - \frac{1}{2\sigma_\epsilon^2} (\mathbf{y} - \mathbf{X}\boldsymbol\beta - \mathbf{Z}\mathbf{u})^T(\mathbf{y} - \mathbf{X}\boldsymbol\beta - \mathbf{Z}\mathbf{u}) 
\]</span></p>
<p>We adjust the formulation of the log likelihood in terms of <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\boldsymbol\tau\)</span>.</p>
<p><span class="math display">\[
\log f(\mathbf{y}|\mathbf{X}, \mathbf{Z}, \boldsymbol\beta, \lambda, \boldsymbol\tau, \sigma_\epsilon) = -\frac{nd}{2}\log\sigma_\epsilon^2 - \frac{1}{2\sigma_\epsilon^2} (\mathbf{y} - \mathbf{X}\boldsymbol\beta - \lambda\mathbf{Z}\boldsymbol\tau)^T(\mathbf{y} - \mathbf{X}\boldsymbol\beta - \lambda\mathbf{Z}\boldsymbol\tau) 
\]</span></p>
<p>Specify priors explicitly next. We let <span class="math inline">\(\lambda\)</span> follow a 2-parameter half-t distribution with hyperparameters <span class="math inline">\(\nu_\lambda\)</span>, <span class="math inline">\(A_\lambda\)</span>. Again, <span class="math inline">\(\boldsymbol\tau\)</span> are standard normal. We also let <span class="math inline">\(\epsilon\)</span> follow a 2-parameter half-t with hyperparameters <span class="math inline">\(\nu_\epsilon\)</span> and <span class="math inline">\(A_\epsilon\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
\boldsymbol\beta &amp;\sim N(0, \sigma_\beta^2 \mathbf{I}_{q+1}) \\
\lambda &amp;\sim \text{half-t}(\nu_\lambda, A_\lambda) \\
\boldsymbol\tau &amp;\sim N(0, I_n) \\
\sigma_\epsilon &amp;\sim \text{half-t}(\nu_\epsilon, A_\epsilon)
\end{aligned}
\]</span></p>
<p>Next, we specify the densities for the priors, omitting constants</p>
<p><span class="math display">\[
\begin{aligned}
\pi(\boldsymbol\beta) &amp;\propto e^{-\frac{1}{2}\boldsymbol\beta^T \mathbf{I}_{(q+1)}\boldsymbol\beta}, \qquad \boldsymbol\beta \in \mathbb{R}^{q+1} \\
\pi(\lambda) &amp;\propto \left(1 + \frac{1}{\nu_\lambda}\left(\frac{\lambda}{A_\lambda} \right)^2 \right)^{-(\nu_\lambda + 1)/2}, \qquad \lambda \in (0, \infty)\\
\pi(\boldsymbol\tau) &amp;\propto e^{-\frac{1}{2}\boldsymbol\tau^T \mathbf{I}_n \boldsymbol\tau}, \qquad \boldsymbol\tau \in \mathbb{R}^{n} \\
\pi(\sigma_\epsilon) &amp;\propto \left(1 + \frac{1}{\nu_\epsilon}\left(\frac{\sigma_\epsilon}{A_\epsilon} \right)^2 \right)^{-(\nu_\epsilon + 1)/2}, \qquad \sigma_\epsilon \in (0, \infty)
\end{aligned}
\]</span></p>
<p>Proposals for all parameters must be over <span class="math inline">\(\mathbb{R}\)</span>. However, the half-t priors <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\sigma_\epsilon\)</span> are restricted to strictly positive numbers. We apply a <span class="math inline">\(\log\)</span> transform such that these parameters span <span class="math inline">\(\mathbb{R}\)</span>.</p>
<p>Re-parameterize <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\sigma_\epsilon\)</span> to <span class="math inline">\(\xi\)</span> and <span class="math inline">\(\gamma\)</span>, respectively.</p>
<p><span class="math display">\[
\begin{aligned}
\xi &amp;= \log\lambda, \quad \lambda = e^\xi \\
\gamma &amp;= \log\sigma_\epsilon, \quad \sigma_\epsilon = e^\gamma
\end{aligned}
\]</span></p>
<p>Apply the Jacobian to obtain the distribution of <span class="math inline">\(\xi\)</span></p>
<p><span class="math display">\[
 \begin{aligned}
 \pi_\xi(\xi) &amp;= \pi_\lambda(g^{-1}(\xi))\left | \frac{d\lambda}{d\xi} \right | \\
 &amp;= \pi_\lambda(e^{\xi}) | e^\xi |\\
 &amp;\propto \left(1 + \frac{1}{\nu_\xi}\left(\frac{\xi}{A_\xi} \right)^2 \right)^{-(\nu_\xi + 1)/2} e^{\xi}
 \end{aligned}
 \]</span></p>
<p>Apply the Jacobian to obtain the distribution of <span class="math inline">\(\gamma\)</span></p>
<p><span class="math display">\[
 \begin{aligned}
 \pi_\gamma(\gamma) &amp;= \pi_{\sigma_\epsilon}(g^{-1}(\gamma))\left | \frac{d\sigma_\epsilon}{d\gamma} \right | \\
 &amp;= \pi_{\sigma_\epsilon}(e^{\gamma}) | e^\gamma |\\
 &amp;\propto \left(1 + \frac{1}{\nu_\gamma}\left(\frac{\gamma}{A_\gamma} \right)^2 \right)^{-(\nu_\gamma + 1)/2} e^{\gamma}
 \end{aligned}
 \]</span></p>
<p>Next, the log priors that we use with transformations, again omitting constants. Note that the log densities of the priors with log densities include an additive term for the transformed distribution (i.e.Â <span class="math inline">\(\xi\)</span> and <span class="math inline">\(\gamma\)</span>).</p>
<p><span class="math display">\[
\begin{aligned}
\log \pi(\beta) &amp;\propto -\frac{\boldsymbol\beta^T \mathbf{I}_{(q+1)}\boldsymbol\beta}{2\sigma_\beta^2} \\
\log \pi(\xi) &amp;\propto -\frac{\nu_\xi + 1}{2} \log \left( 1 + \frac{1}{\nu_\xi} \left(\frac{e^\xi}{A_\xi} \right)^2 \right) + \xi \\
\log \pi(\boldsymbol\tau) &amp;\propto -\frac{1}{2}\boldsymbol\tau^T \mathbf{I}_n \boldsymbol\tau \\
\log \pi(\gamma) &amp;\propto -\frac{\nu_\gamma + 1}{2} \log \left( 1 + \frac{1}{\nu_\gamma} \left(\frac{e^\gamma}{A_\gamma} \right)^2 \right) + \gamma
\end{aligned}
\]</span></p>
<p>Our final adjustment to the log likelihood replaces <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\sigma_\epsilon\)</span> with <span class="math inline">\(\xi\)</span> and <span class="math inline">\(\gamma\)</span>, respectively.</p>
<p><span class="math display">\[
\log f(\mathbf{y}|\mathbf{X}, \mathbf{Z}, \boldsymbol\beta, \xi, \boldsymbol\tau, \gamma) = -nd\gamma - \frac{1}{2 e^{2\gamma}} (\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau)^T(\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau) 
\]</span></p>
<p>The log posterior is the log likelihood plus the log prior. In this example, the prior parameters are independent of each other.</p>
<p><span class="math display">\[
\begin{aligned}
\log f(\boldsymbol\beta, \xi, \boldsymbol\tau, \gamma | \mathbf{y}, \mathbf{X}, \mathbf{Z}) &amp;\propto \log f(\mathbf{y} | \mathbf{X}, \mathbf{Z}, \boldsymbol\beta, \xi, \boldsymbol\tau, \gamma)  + \log \pi(\boldsymbol\beta, \xi, \boldsymbol\tau, \gamma) \\ 
&amp;\propto \log f(\mathbf{y} | \mathbf{X}, \mathbf{Z}, \boldsymbol\beta, \xi, \boldsymbol\tau, \gamma) + \log \pi(\boldsymbol\beta) + \log \pi(\xi) + \log\pi(\boldsymbol\tau) + \log\pi(\gamma) \\
&amp;\propto -nd\gamma - \frac{1}{2 e^{2\gamma}} (\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau)^T(\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau) - \\
&amp;\quad \frac{\boldsymbol\beta^T \mathbf{I}_{(q+1)}\boldsymbol\beta}{2\sigma_\beta^2} -\frac{\nu_\xi + 1}{2} \log \left( 1 + \frac{1}{\nu_\xi} \left(\frac{e^\xi}{A_\xi} \right)^2 \right) + \xi - \\
&amp;\quad \frac{1}{2}\boldsymbol\tau^T \mathbf{I}_n \boldsymbol\tau -\frac{\nu_\gamma + 1}{2} \log \left( 1 + \frac{1}{\nu_\gamma} \left(\frac{e^\gamma}{A_\gamma} \right)^2 \right) + \gamma
\end{aligned}
\]</span></p>
<p>Next, we compute the gradient of the log posterior</p>
<p>First <span class="math inline">\(\nabla_\beta\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\nabla_\beta \log f(\boldsymbol\beta, \xi, \boldsymbol\tau, \gamma | \mathbf{y}, \mathbf{X}, \mathbf{Z}) &amp;\propto -\frac{1}{2e^{2\gamma}} (-2)\mathbf{X}^T(y-\mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau)- \frac{1}{\sigma_\beta^2}\mathbf{I}_{(q+1)}\boldsymbol\beta \\
&amp;\propto e^{-2\gamma}\mathbf{X}^T(y-\mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau)- \frac{1}{\sigma_\beta^2}\mathbf{I}_{(q+1)}\boldsymbol\beta
\end{aligned}
\]</span></p>
<p>Next <span class="math inline">\(\nabla_\xi\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\nabla_\xi \log f(\boldsymbol\beta, \xi, \boldsymbol\tau, \gamma | \mathbf{y}, \mathbf{X}, \mathbf{Z}) &amp;\propto -\frac{e^{-2\gamma}}{2}(-2)(\mathbf{Z}\boldsymbol\tau)^T e^{\xi} (\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau) - \frac{\nu_\xi + 1}{2}\left( 1 + \frac{1}{\nu_\xi} \left(\frac{e^\xi}{A_\xi} \right)^2 \right)^{-1}\frac{2\xi}{\nu_\xi A_\xi^2} + 1 \\
&amp;\propto e^{-2\gamma + \xi} \boldsymbol\tau^T \mathbf{Z}^T (\mathbf{y}-\mathbf{X}\boldsymbol\beta -e^{\xi}\mathbf{Z}\boldsymbol\tau) - \frac{\nu_\xi + 1}{1 + \nu_\xi A_\xi^2 e^{-2\xi}} + 1
\end{aligned}
\]</span></p>
<p>Next <span class="math inline">\(\nabla_\tau\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\nabla_\tau \log f(\boldsymbol\beta, \xi, \boldsymbol\tau, \gamma | \mathbf{y}, \mathbf{X}, \mathbf{Z}) &amp;\propto -\frac{e^{-2\gamma}}{2}(-2)(\mathbf{Z}\boldsymbol)^T e^{\xi}(\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau) - \frac{1}{2}\mathbf{I}_n\boldsymbol\tau \\
&amp;\propto e^{-2\gamma} \mathbf{Z}^T(\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau) - \frac{1}{2}\mathbf{I}_n \boldsymbol\tau
\end{aligned}
\]</span></p>
<p>Next <span class="math inline">\(\nabla_\gamma\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\nabla_\gamma \log f(\boldsymbol\beta, \xi, \boldsymbol\tau, \gamma | \mathbf{y}, \mathbf{X}, \mathbf{Z}) &amp;\propto -nd + e^{-2\gamma}(\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau)^T (\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau) - \frac{\nu_\gamma + 1}{2}\left( 1 + \frac{1}{\nu_\gamma} \left(\frac{e^\gamma}{A_\gamma} \right)^2 \right)^{-1}\frac{2\gamma}{\nu_\gamma A_\gamma^2} + 1 \\ 
&amp;\propto -nd + e^{-2\gamma}(\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau)^T (\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau) - \frac{\nu_\gamma + 1}{1 + \nu_\gamma A_\gamma^2 e^{-2\gamma}} + 1
\end{aligned}
\]</span></p>
<p>Full gradient of the log posterior specified</p>
<p><span class="math display">\[
\begin{aligned}
\nabla_\beta \log f(\boldsymbol\beta, \xi, \boldsymbol\tau, \gamma | \mathbf{y}, \mathbf{X}, \mathbf{Z})
&amp;\propto e^{-2\gamma}\mathbf{X}^T(y-\mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau)- \frac{1}{\sigma_\beta^2}\mathbf{I}_{(q+1)}\boldsymbol\beta \\
\nabla_\xi \log f(\boldsymbol\beta, \xi, \boldsymbol\tau, \gamma | \mathbf{y}, \mathbf{X}, \mathbf{Z}) 
&amp;\propto e^{-2\gamma + \xi} \boldsymbol\tau^T \mathbf{Z}^T (\mathbf{y}-\mathbf{X}\boldsymbol\beta -e^{\xi}\mathbf{Z}\boldsymbol\tau) - \frac{\nu_\xi + 1}{1 + \nu_\xi A_\xi^2 e^{-2\xi}} + 1 \\
\nabla_\tau \log f(\boldsymbol\beta, \xi, \boldsymbol\tau, \gamma | \mathbf{y}, \mathbf{X}, \mathbf{Z}) &amp;\propto e^{-2\gamma} \mathbf{Z}^T(\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau) - \frac{1}{2}\mathbf{I}_n \boldsymbol\tau \\
\nabla_\gamma \log f(\boldsymbol\beta, \xi, \boldsymbol\tau, \gamma | \mathbf{y}, \mathbf{X}, \mathbf{Z}) &amp;\propto  -nd + e^{-2\gamma}(\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau)^T (\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau) - \frac{\nu_\gamma + 1}{1 + \nu_\gamma A_\gamma^2 e^{-2\gamma}} + 1
\end{aligned}
\]</span></p>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Samuel Thomas.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.5.1.9000.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
