<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Sample log posterior and gradient functions for select generalized linear models
and mixed effect models — hmclearn-glm-posterior • hmclearn</title>


<!-- jquery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
<!-- Bootstrap -->
<link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/sandstone/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous" />


<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script>

<!-- bootstrap-toc -->
<link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script>

<!-- Font Awesome icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous" />

<!-- clipboard.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script>

<!-- headroom.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script>

<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script>




<meta property="og:title" content="Sample log posterior and gradient functions for select generalized linear models
and mixed effect models — hmclearn-glm-posterior" />
<meta property="og:description" content="These functions can be used to fit common generalized linear models and mixed effect models.
See the accompanying vignettes for details on the derivations of the log posterior and gradient.
In addition, these functions can be used as templates to build custom models to fit using HMC.
# extract parameters from theta vector
  beta_param &amp;lt;- as.numeric(theta[1:p])
  tau_param &amp;lt;- theta[(p+1):(p+m*q)]
# extract parameters from theta vector
  beta_param &amp;lt;- as.numeric(theta[1:p])
  tau_param &amp;lt;- theta[(p+1):(p+m*q)]
# prior covariance for beta
  Sig_inv_beta &amp;lt;- diag(1/sig2beta, p, p)
# prior covariance for beta
  Sig_inv_beta &amp;lt;- diag(1/sig2beta, p, p)" />




<!-- mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->



  </head>

  <body data-spy="scroll" data-target="#toc">
    <div class="container template-reference-topic">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">hmclearn</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.0.2</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="../articles/gradient_derivations_hmclearn.html">Gradient Derivations for HMC</a>
    </li>
    <li>
      <a href="../articles/linear_mixed_effects_hmclearn.html">hmclearn package:  Linear Mixed Effects Regression Example</a>
    </li>
    <li>
      <a href="../articles/linear_regression_hmclearn.html">hmclearn package:  Linear Regression Example</a>
    </li>
    <li>
      <a href="../articles/logistic_mixed_effects_hmclearn.html">hmclearn package:  Logistic Mixed Effects Regression Example</a>
    </li>
    <li>
      <a href="../articles/logistic_regression_hmclearn.html">hmclearn:  Logistic Regression Example</a>
    </li>
    <li>
      <a href="../articles/poisson_regression_hmclearn.html">hmclearn:  Poisson Regression Example</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
      
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header>

<div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Sample log posterior and gradient functions for select generalized linear models
and mixed effect models</h1>
    
    <div class="hidden name"><code>hmclearn-glm-posterior.Rd</code></div>
    </div>

    <div class="ref-description">
    <p>These functions can be used to fit common generalized linear models and mixed effect models.
See the accompanying vignettes for details on the derivations of the log posterior and gradient.
In addition, these functions can be used as templates to build custom models to fit using HMC.</p>
<p># extract parameters from theta vector
  beta_param &lt;- as.numeric(theta[1:p])
  tau_param &lt;- theta[(p+1):(p+m*q)]</p>
<p># extract parameters from theta vector
  beta_param &lt;- as.numeric(theta[1:p])
  tau_param &lt;- theta[(p+1):(p+m*q)]</p>
<p># prior covariance for beta
  Sig_inv_beta &lt;- diag(1/sig2beta, p, p)</p>
<p># prior covariance for beta
  Sig_inv_beta &lt;- diag(1/sig2beta, p, p)</p>
    </div>

    <pre class="usage"><span class='fu'>linear_posterior</span>(<span class='no'>theta</span>, <span class='no'>y</span>, <span class='no'>X</span>, <span class='kw'>a</span> <span class='kw'>=</span> <span class='fl'>1e-04</span>, <span class='kw'>b</span> <span class='kw'>=</span> <span class='fl'>1e-04</span>, <span class='kw'>sig2beta</span> <span class='kw'>=</span> <span class='fl'>1000</span>)

<span class='fu'>g_linear_posterior</span>(<span class='no'>theta</span>, <span class='no'>y</span>, <span class='no'>X</span>, <span class='kw'>a</span> <span class='kw'>=</span> <span class='fl'>1e-04</span>, <span class='kw'>b</span> <span class='kw'>=</span> <span class='fl'>1e-04</span>, <span class='kw'>sig2beta</span> <span class='kw'>=</span> <span class='fl'>1000</span>)

<span class='fu'>logistic_posterior</span>(<span class='no'>theta</span>, <span class='no'>y</span>, <span class='no'>X</span>, <span class='kw'>sig2beta</span> <span class='kw'>=</span> <span class='fl'>100</span>)

<span class='fu'>g_logistic_posterior</span>(<span class='no'>theta</span>, <span class='no'>y</span>, <span class='no'>X</span>, <span class='kw'>sig2beta</span> <span class='kw'>=</span> <span class='fl'>100</span>)

<span class='fu'>poisson_posterior</span>(<span class='no'>theta</span>, <span class='no'>y</span>, <span class='no'>X</span>, <span class='kw'>sig2beta</span> <span class='kw'>=</span> <span class='fl'>100</span>)

<span class='fu'>g_poisson_posterior</span>(<span class='no'>theta</span>, <span class='no'>y</span>, <span class='no'>X</span>, <span class='kw'>sig2beta</span> <span class='kw'>=</span> <span class='fl'>100</span>)

<span class='fu'>lmm_posterior</span>(
  <span class='no'>theta</span>,
  <span class='no'>y</span>,
  <span class='no'>X</span>,
  <span class='no'>Z</span>,
  <span class='no'>n</span>,
  <span class='no'>d</span>,
  <span class='kw'>nrandom</span> <span class='kw'>=</span> <span class='fl'>1</span>,
  <span class='kw'>nugamma</span> <span class='kw'>=</span> <span class='fl'>1</span>,
  <span class='kw'>nuxi</span> <span class='kw'>=</span> <span class='fl'>1</span>,
  <span class='kw'>Agamma</span> <span class='kw'>=</span> <span class='fl'>25</span>,
  <span class='kw'>Axi</span> <span class='kw'>=</span> <span class='fl'>25</span>,
  <span class='kw'>sig2beta</span> <span class='kw'>=</span> <span class='fl'>1000</span>
)

<span class='fu'>g_lmm_posterior</span>(
  <span class='no'>theta</span>,
  <span class='no'>y</span>,
  <span class='no'>X</span>,
  <span class='no'>Z</span>,
  <span class='no'>n</span>,
  <span class='no'>d</span>,
  <span class='kw'>nrandom</span> <span class='kw'>=</span> <span class='fl'>1</span>,
  <span class='kw'>nugamma</span> <span class='kw'>=</span> <span class='fl'>1</span>,
  <span class='kw'>nuxi</span> <span class='kw'>=</span> <span class='fl'>1</span>,
  <span class='kw'>Agamma</span> <span class='kw'>=</span> <span class='fl'>25</span>,
  <span class='kw'>Axi</span> <span class='kw'>=</span> <span class='fl'>25</span>,
  <span class='kw'>sig2beta</span> <span class='kw'>=</span> <span class='fl'>1000</span>
)

<span class='fu'>glmm_bin_posterior</span>(
  <span class='no'>theta</span>,
  <span class='no'>y</span>,
  <span class='no'>X</span>,
  <span class='no'>Z</span>,
  <span class='no'>n</span>,
  <span class='kw'>nrandom</span> <span class='kw'>=</span> <span class='fl'>1</span>,
  <span class='kw'>nuxi</span> <span class='kw'>=</span> <span class='fl'>1</span>,
  <span class='kw'>Axi</span> <span class='kw'>=</span> <span class='fl'>25</span>,
  <span class='kw'>sig2beta</span> <span class='kw'>=</span> <span class='fl'>10000</span>
)

<span class='fu'>g_glmm_bin_posterior</span>(
  <span class='no'>theta</span>,
  <span class='no'>y</span>,
  <span class='no'>X</span>,
  <span class='no'>Z</span>,
  <span class='no'>n</span>,
  <span class='kw'>nrandom</span> <span class='kw'>=</span> <span class='fl'>1</span>,
  <span class='kw'>nuxi</span> <span class='kw'>=</span> <span class='fl'>1</span>,
  <span class='kw'>Axi</span> <span class='kw'>=</span> <span class='fl'>25</span>,
  <span class='kw'>sig2beta</span> <span class='kw'>=</span> <span class='fl'>10000</span>
)

<span class='fu'>glmm_poisson_posterior</span>(
  <span class='no'>theta</span>,
  <span class='no'>y</span>,
  <span class='no'>X</span>,
  <span class='no'>Z</span>,
  <span class='no'>m</span>,
  <span class='kw'>q</span> <span class='kw'>=</span> <span class='fl'>1</span>,
  <span class='kw'>A</span> <span class='kw'>=</span> <span class='fl'>10000</span>,
  <span class='kw'>nulambda</span> <span class='kw'>=</span> <span class='fl'>1</span>,
  <span class='kw'>Alambda</span> <span class='kw'>=</span> <span class='fl'>25</span>,
  <span class='kw'>sig2beta</span> <span class='kw'>=</span> <span class='fl'>10000</span>
)

<span class='fu'>g_glmm_poisson_posterior</span>(
  <span class='no'>theta</span>,
  <span class='no'>y</span>,
  <span class='no'>X</span>,
  <span class='no'>Z</span>,
  <span class='no'>m</span>,
  <span class='kw'>q</span> <span class='kw'>=</span> <span class='fl'>1</span>,
  <span class='kw'>A</span> <span class='kw'>=</span> <span class='fl'>10000</span>,
  <span class='kw'>nulambda</span> <span class='kw'>=</span> <span class='fl'>1</span>,
  <span class='kw'>Alambda</span> <span class='kw'>=</span> <span class='fl'>25</span>,
  <span class='kw'>sig2beta</span> <span class='kw'>=</span> <span class='fl'>10000</span>
)</pre>

    <h2 class="hasAnchor" id="arguments"><a class="anchor" href="#arguments"></a>Arguments</h2>
    <table class="ref-arguments">
    <colgroup><col class="name" /><col class="desc" /></colgroup>
    <tr>
      <th>theta</th>
      <td><p>vector of parameters.  Stored as a single vector in order fixed effect, random effect, log-transformed diagonal \(\lambda\), and off-diagonal of <code>G</code> vector <code>a</code></p></td>
    </tr>
    <tr>
      <th>y</th>
      <td><p>numeric vector for the dependent variable</p></td>
    </tr>
    <tr>
      <th>X</th>
      <td><p>numeric design matrix of fixed effect parameters</p></td>
    </tr>
    <tr>
      <th>a</th>
      <td><p>hyperprior for the Inverse Gamma shape parameter</p></td>
    </tr>
    <tr>
      <th>b</th>
      <td><p>hyperprior for the Inverse Gamma scale parameter</p></td>
    </tr>
    <tr>
      <th>sig2beta</th>
      <td><p>diagonal covariance of prior for linear predictors is multivariate normal with mean 0</p></td>
    </tr>
    <tr>
      <th>Z</th>
      <td><p>numeric design matrix of random effect parameters</p></td>
    </tr>
    <tr>
      <th>n</th>
      <td><p>number of subjects for mixed effect models, or number of observations for standard glm</p></td>
    </tr>
    <tr>
      <th>d</th>
      <td><p>number of observations per subject</p></td>
    </tr>
    <tr>
      <th>nugamma</th>
      <td><p>hyperprior \(\nu\) for the half-t prior of the log transformed error</p></td>
    </tr>
    <tr>
      <th>nuxi</th>
      <td><p>hyperprior \(\nu\) for the half-t prior of the random effects diagonal</p></td>
    </tr>
    <tr>
      <th>Agamma</th>
      <td><p>hyperprior \(A\) for the half-t prior of the log transformed error</p></td>
    </tr>
    <tr>
      <th>Axi</th>
      <td><p>hyperprior \(A\) for the half-t prior of the random effects diagonal</p></td>
    </tr>
    <tr>
      <th>m</th>
      <td><p>number of random effect linear parameters</p></td>
    </tr>
    <tr>
      <th>q</th>
      <td><p>number of random effects covariance parameters</p></td>
    </tr>
    <tr>
      <th>A</th>
      <td><p>hyperprior numeric vector for the random effects off-diagonal <code>a</code></p></td>
    </tr>
    <tr>
      <th>nulambda</th>
      <td><p>hyperprior \(\nu\) for the half-t prior of the random effects diagonal</p></td>
    </tr>
    <tr>
      <th>Alambda</th>
      <td><p>hyperprior \(A\) for the half-t prior of the random effects diagonal</p></td>
    </tr>
    <tr>
      <th>nueps</th>
      <td><p>hyperprior \(\nu\) for the half-t prior of the error parameter</p></td>
    </tr>
    <tr>
      <th>Aeps</th>
      <td><p>hyperprior \(A\) for the half-t prior of the error parameter</p></td>
    </tr>
    </table>

    <h2 class="hasAnchor" id="value"><a class="anchor" href="#value"></a>Value</h2>

    <p>Numeric vector for the log posterior or gradient of the log posterior</p>
    <h2 class="hasAnchor" id="details"><a class="anchor" href="#details"></a>Details</h2>

    








    <h2 class="hasAnchor" id="generalized-linear-models-with-available-posterior-and-gradient-functions"><a class="anchor" href="#generalized-linear-models-with-available-posterior-and-gradient-functions"></a>Generalized Linear Models with available posterior and gradient functions</h2>

    

<dl>
  <dt>`linear_posterior(theta, y, X, a=1e-4, b=1e-4, B=0.001)`</dt><dd><p>The log posterior function for linear regression
   $$p(y | X, \beta; \sigma^2) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp{\left(-\frac{1}{2\sigma^2} (y - X\beta)^T(y-X\beta) \right)}$$
   with priors \(p(\sigma^2) \sim IG(a, b)\) and \(\beta \sim N(0, BI)\).  The variance term is log transformed \(\gamma = \log\sigma^2\)
   The input parameter vector <code>theta</code> is of length <code>k</code>.  The first <code>k-1</code> parameters are for \(\beta\), and the last parameter is \(\gamma\)
   Note that the Inverse Gamma prior can be problematic for certain applications with low variance.  See Gelman (2006)</p></dd>
  <dt>`g_linear_posterior(theta, y, X, a = 1e-04, b = 1e-04, B = 0.001)`</dt><dd><p>Gradient of the log posterior for a linear regression model with Normal prior for the linear parameters and Inverse Gamma for the error term.
   $$p(y | X, \beta; \sigma^2) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp{\left(-\frac{1}{2\sigma^2} (y - X\beta)^T(y-X\beta) \right)}$$
   with priors \(p(\sigma^2) \sim IG(a, b)\) and \(\beta \sim N(0, BI)\).  The variance term is log transformed \(\gamma = \log\sigma^2\)
   The input parameter vector <code>theta</code> is of length <code>k</code>.  The first <code>k-1</code> parameters are for \(\beta\), and the last parameter is \(\gamma\)
   Note that the Inverse Gamma prior can be problematic for certain applications with low variance.  See Gelman (2006)</p></dd>
  <dt>`logistic_posterior(theta, y, X, B = 0.01) `</dt><dd><p>Log posterior for a logistic regression model with Normal prior for the linear parameters.
   The likelihood function for logistic regression
   $$L(\beta; X, y) = \prod_{i=1}^{n} \left(\frac{1}{1+e^{-X_i\beta}}\right)^{y_i} \left(\frac{e^{-X_i\beta}}{1+e^{-X_i\beta}}\right)^{1-y_i}$$
   with priors \(\beta \sim N(0, BI)\).
   The input parameter vector <code>theta</code> is of length <code>k</code>, containing parameter values for \(\beta\)</p></dd>
  <dt>`g_logistic_posterior(theta, y, X, B = 0.01) `</dt><dd><p>Gradient of the log posterior for a logistic regression model with Normal prior for the linear parameters.
   The likelihood function for logistic regression
   $$L(\beta; X, y) = \prod_{i=1}^{n} \left(\frac{1}{1+e^{-X_i\beta}}\right)^{y_i} \left(\frac{e^{-X_i\beta}}{1+e^{-X_i\beta}}\right)^{1-y_i}$$
   with priors \(\beta \sim N(0, BI)\).
   The input parameter vector <code>theta</code> is of length <code>k</code>, containing parameter values for \(\beta\)</p></dd>
  <dt>`poisson_posterior(theta, y, X, B = 0.01) `</dt><dd><p>Log posterior for a Poisson regression model with Normal prior for the linear parameters.
   The likelihood function for poisson regression
   $$L(\beta; y, X) = \prod_{i=1}^n \frac{e^{-e^{X_i\beta}}e^{y_iX_i\beta}}{y_i!}$$
   with priors \(\beta \sim N(0, BI)\).
   The input parameter vector <code>theta</code> is of length <code>k</code>, containing parameter values for \(\beta\)</p></dd>
  <dt>`g_poisson_posterior(theta, y, X, B = 0.01) `</dt><dd><p>Gradient of the log posterior for a Poisson regression model with Normal prior for the linear parameters.
   The likelihood function for poisson regression
   $$L(\beta; y, X) = \prod_{i=1}^n \frac{e^{-e^{X_i\beta}}e^{y_iX_i\beta}}{y_i!}$$
   with priors \(\beta \sim N(0, BI)\).
   The input parameter vector <code>theta</code> is of length <code>k</code>, containing parameter values for \(\beta\)</p></dd>
 
</dl>

    <h2 class="hasAnchor" id="generalized-linear-mixed-effect-with-available-posterior-and-gradient-functions"><a class="anchor" href="#generalized-linear-mixed-effect-with-available-posterior-and-gradient-functions"></a>Generalized Linear Mixed Effect with available posterior and gradient functions</h2>

    

<dl>
  <dt>`lmm_posterior(theta, y, X, Z, m, q = 1, A = 10000, nueps = 1, nulambda = 1, Aeps = 25, Alambda = 25, B = 0.001) `</dt><dd><p>The log posterior function for linear mixed effects regression
   $$p(y | \beta, u, \sigma_\epsilon^2) \propto (\sigma_\epsilon^2)^{-n/2} e^{-\frac{1}{2\sigma_\epsilon^2}(y - X\beta - Zu)^T (y - X\beta - Zu)}$$
   with priors \(\beta \sim N(0, BI)\), \(\sigma_\epsilon \sim half-t(A_\epsilon, nu_\epsilon)\), \(\lambda \sim half-t(A_\lambda, nu_\lambda )\).
   The vector \(\lambda\) is the diagonal of the covariance <code>G</code> hyperprior where \(u \sim N(0, G\).  The off-diagonal hyperpriors are stored in a vector \(a \sim N(0, A\).  See Chan, Jeliazkov (2009) for details.
   The input parameter vector <code>theta</code> is of length <code>k</code>.  The first <code>k-1</code> parameters are for \(\beta\), and the last parameter is \(\gamma\)</p></dd>
  <dt>`g_lmm_posterior(theta, y, X, Z, m, q = 1, A = 10000, nueps = 1, nulambda = 1, Aeps = 25, Alambda = 25, B = 0.001)`</dt><dd><p>Gradient of the log posterior for a linear mixed effects regression model
   $$p(y | \beta, u, \sigma_\epsilon^2) \propto (\sigma_\epsilon^2)^{-n/2} e^{-\frac{1}{2\sigma_\epsilon^2}(y - X\beta - Zu)^T (y - X\beta - Zu)}$$
   with priors \(\beta \sim N(0, BI)\), \(\sigma_\epsilon \sim half-t(A_\epsilon, nu_\epsilon)\), \(\lambda \sim half-t(A_\lambda, nu_\lambda )\).
   The vector \(\lambda\) is the diagonal of the covariance <code>G</code> hyperprior where \(u \sim N(0, G\).  The off-diagonal hyperpriors are stored in a vector \(a \sim N(0, A\).  See Chan, Jeliazkov (2009) for details.
   The input parameter vector <code>theta</code> is of length <code>k</code>.  The first <code>k-1</code> parameters are for \(\beta\), and the last parameter is \(\gamma\)</p></dd>
  <dt>`glmm_bin_posterior(theta, y, X, Z, m, q = 1, A = 10000, nulambda = 1, Alambda = 25, B = 10000)`</dt><dd><p>The log posterior function for logistic mixed effects regression
   $$p(y | X, Z, \beta, u) = \prod_{i=1}^n\prod_{j=1}^m \left(\frac{1}{1 + e^{-X_{i}\beta - Z_{ij}u_i}}\right)^{y_{ij}} \left(\frac{e^{-X_i\beta - Z_{ij}u_i}}{1 + e^{-X_{i}\beta - Z_{ij}u_i}}\right)^{1-y_{ij}} $$
   with priors \(\beta \sim N(0, BI)\), \(\sigma_\epsilon \sim half-t(A_\epsilon, nu_\epsilon)\), \(\lambda \sim half-t(A_\lambda, nu_\lambda )\).
   The vector \(\lambda\) is the diagonal of the covariance <code>G</code> hyperprior where \(u \sim N(0, G\).  The off-diagonal hyperpriors are stored in a vector \(a \sim N(0, A\).  See Chan, Jeliazkov (2009) for details.
   The input parameter vector <code>theta</code> is of length <code>k</code>.  The first <code>k-1</code> parameters are for \(\beta\), and the last parameter is \(\gamma\)</p></dd>
  <dt>`g_glmm_bin_posterior(theta, y, X, Z, m, q = 1, A = 10000, nulambda = 1, Alambda = 25, B = 10000) `</dt><dd><p>Gradient of the log posterior function for logistic mixed effects regression
   $$p(y | X, Z, \beta, u) = \prod_{i=1}^n\prod_{j=1}^m \left(\frac{1}{1 + e^{-X_{i}\beta - Z_{ij}u_i}}\right)^{y_{ij}} \left(\frac{e^{-X_i\beta - Z_{ij}u_i}}{1 + e^{-X_{i}\beta - Z_{ij}u_i}}\right)^{1-y_{ij}} $$
   with priors \(\beta \sim N(0, BI)\), \(\sigma_\epsilon \sim half-t(A_\epsilon, nu_\epsilon)\), \(\lambda \sim half-t(A_\lambda, nu_\lambda )\).
   The vector \(\lambda\) is the diagonal of the covariance <code>G</code> hyperprior where \(u \sim N(0, G\).  The off-diagonal hyperpriors are stored in a vector \(a \sim N(0, A\).  See Chan, Jeliazkov (2009) for details.
   The input parameter vector <code>theta</code> is of length <code>k</code>.  The first <code>k-1</code> parameters are for \(\beta\), and the last parameter is \(\gamma\)</p></dd>
  <dt>`glmm_poisson_posterior(theta, y, X, Z, m, q = 1, A = 10000, nulambda = 1, Alambda = 25, B = 10000) `</dt><dd><p>Log posterior for a Poisson mixed effect regression
   $$L(\beta; y, X) = \prod_{i=1}^n \prod_{j=1}^m \frac{e^{-e^{X_i\beta + Z_{ij}u_{ij}}}e^{y_i(X_i\beta + Z_{ij}u_{ij})}}{y_i!} $$
   with priors \(\beta \sim N(0, BI)\), \(\sigma_\epsilon \sim half-t(A_\epsilon, nu_\epsilon)\), \(\lambda \sim half-t(A_\lambda, nu_\lambda )\).
   The vector \(\lambda\) is the diagonal of the covariance <code>G</code> hyperprior where \(u \sim N(0, G\).  The off-diagonal hyperpriors are stored in a vector \(a \sim N(0, A\).  See Chan, Jeliazkov (2009) for details.
   The input parameter vector <code>theta</code> is of length <code>k</code>.  The first <code>k-1</code> parameters are for \(\beta\), and the last parameter is \(\gamma\)</p></dd>
  <dt>`g_glmm_poisson_posterior(theta, y, X, Z, m, q = 1, A = 10000, nulambda = 1, Alambda = 25, B = 10000) `</dt><dd><p>Gradient of the log posterior for a Poisson mixed effect regression
   $$L(\beta; y, X) = \prod_{i=1}^n \prod_{j=1}^m \frac{e^{-e^{X_i\beta + Z_{ij}u_{ij}}}e^{y_i(X_i\beta + Z_{ij}u_{ij})}}{y_i!} $$
   with priors \(\beta \sim N(0, BI)\), \(\sigma_\epsilon \sim half-t(A_\epsilon, nu_\epsilon)\), \(\lambda \sim half-t(A_\lambda, nu_\lambda )\).
   The vector \(\lambda\) is the diagonal of the covariance <code>G</code> hyperprior where \(u \sim N(0, G\).  The off-diagonal hyperpriors are stored in a vector \(a \sim N(0, A\).  See Chan, Jeliazkov (2009) for details.
   The input parameter vector <code>theta</code> is of length <code>k</code>.  The first <code>k-1</code> parameters are for \(\beta\), and the last parameter is \(\gamma\)</p></dd>
 
</dl>

    <h2 class="hasAnchor" id="references"><a class="anchor" href="#references"></a>References</h2>

    <p>Gelman, A. (2006). <em>Prior distributions for variance parameters in hierarchical models (comment on article by Browne and Draper)</em>. Bayesian analysis, 1(3), 515-534.</p>
<p>Chan, J. C. C., &amp; Jeliazkov, I. (2009). <em>MCMC estimation of restricted covariance matrices</em>. Journal of Computational and Graphical Statistics, 18(2), 457-480.</p>
<p>Betancourt, M., &amp; Girolami, M. (2015). <em>Hamiltonian Monte Carlo for hierarchical models</em>. Current trends in Bayesian methodology with applications, 79, 30.</p>

    <h2 class="hasAnchor" id="examples"><a class="anchor" href="#examples"></a>Examples</h2>
    <pre class="examples"><div class='input'><span class='co'># Linear regression example</span>
<span class='fu'><a href='https://rdrr.io/r/base/Random.html'>set.seed</a></span>(<span class='fl'>521</span>)
<span class='no'>X</span> <span class='kw'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/base/cbind.html'>cbind</a></span>(<span class='fl'>1</span>, <span class='fu'><a href='https://rdrr.io/r/base/matrix.html'>matrix</a></span>(<span class='fu'><a href='https://rdrr.io/r/stats/Normal.html'>rnorm</a></span>(<span class='fl'>300</span>), <span class='kw'>ncol</span><span class='kw'>=</span><span class='fl'>3</span>))
<span class='no'>betavals</span> <span class='kw'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span>(<span class='fl'>0.5</span>, -<span class='fl'>1</span>, <span class='fl'>2</span>, -<span class='fl'>3</span>)
<span class='no'>y</span> <span class='kw'>&lt;-</span> <span class='no'>X</span><span class='kw'>%*%</span><span class='no'>betavals</span> + <span class='fu'><a href='https://rdrr.io/r/stats/Normal.html'>rnorm</a></span>(<span class='fl'>100</span>, <span class='kw'>sd</span><span class='kw'>=</span><span class='fl'>.2</span>)

<span class='no'>f1_hmc</span> <span class='kw'>&lt;-</span> <span class='fu'><a href='hmc.html'>hmc</a></span>(<span class='kw'>N</span> <span class='kw'>=</span> <span class='fl'>500</span>,
          <span class='kw'>theta.init</span> <span class='kw'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span>(<span class='fu'><a href='https://rdrr.io/r/base/rep.html'>rep</a></span>(<span class='fl'>0</span>, <span class='fl'>4</span>), <span class='fl'>1</span>),
          <span class='kw'>epsilon</span> <span class='kw'>=</span> <span class='fl'>0.01</span>,
          <span class='kw'>L</span> <span class='kw'>=</span> <span class='fl'>10</span>,
          <span class='kw'>logPOSTERIOR</span> <span class='kw'>=</span> <span class='no'>linear_posterior</span>,
          <span class='kw'>glogPOSTERIOR</span> <span class='kw'>=</span> <span class='no'>g_linear_posterior</span>,
          <span class='kw'>varnames</span> <span class='kw'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span>(<span class='fu'><a href='https://rdrr.io/r/base/paste.html'>paste0</a></span>(<span class='st'>"beta"</span>, <span class='fl'>0</span>:<span class='fl'>3</span>), <span class='st'>"log_sigma_sq"</span>),
          <span class='kw'>param</span><span class='kw'>=</span><span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span>(<span class='kw'>y</span><span class='kw'>=</span><span class='no'>y</span>, <span class='kw'>X</span><span class='kw'>=</span><span class='no'>X</span>), <span class='kw'>parallel</span><span class='kw'>=</span><span class='fl'>FALSE</span>, <span class='kw'>chains</span><span class='kw'>=</span><span class='fl'>1</span>)

<span class='fu'><a href='https://rdrr.io/r/base/summary.html'>summary</a></span>(<span class='no'>f1_hmc</span>, <span class='kw'>burnin</span><span class='kw'>=</span><span class='fl'>100</span>)</div><div class='output co'>#&gt; Summary of MCMC simulation
#&gt; </div><div class='output co'>#&gt;                      5%       25%        50%        75%        95%
#&gt; beta0         0.4847789  0.518149  0.5321988  0.5474873  0.5726653
#&gt; beta1        -1.0398451 -1.022090 -1.0099795 -0.9969146 -0.9695882
#&gt; beta2         1.9834760  2.003620  2.0201366  2.0330953  2.0521270
#&gt; beta3        -3.0146309 -2.996771 -2.9813806 -2.9668917 -2.9442674
#&gt; log_sigma_sq -3.2522864 -3.150465 -3.0570739 -2.9441514 -2.8255475</div><div class='input'>

<span class='co'># poisson regression example</span>
<span class='fu'><a href='https://rdrr.io/r/base/Random.html'>set.seed</a></span>(<span class='fl'>7363</span>)
<span class='no'>X</span> <span class='kw'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/base/cbind.html'>cbind</a></span>(<span class='fl'>1</span>, <span class='fu'><a href='https://rdrr.io/r/base/matrix.html'>matrix</a></span>(<span class='fu'><a href='https://rdrr.io/r/stats/Normal.html'>rnorm</a></span>(<span class='fl'>40</span>), <span class='kw'>ncol</span><span class='kw'>=</span><span class='fl'>2</span>))
<span class='no'>betavals</span> <span class='kw'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span>(<span class='fl'>0.8</span>, -<span class='fl'>0.5</span>, <span class='fl'>1.1</span>)
<span class='no'>lmu</span> <span class='kw'>&lt;-</span> <span class='no'>X</span> <span class='kw'>%*%</span> <span class='no'>betavals</span>
<span class='no'>y</span> <span class='kw'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/base/lapply.html'>sapply</a></span>(<span class='fu'><a href='https://rdrr.io/r/base/Log.html'>exp</a></span>(<span class='no'>lmu</span>), <span class='kw'>FUN</span> <span class='kw'>=</span> <span class='no'>rpois</span>, <span class='kw'>n</span><span class='kw'>=</span><span class='fl'>1</span>)

<span class='no'>f2_hmc</span> <span class='kw'>&lt;-</span> <span class='fu'><a href='hmc.html'>hmc</a></span>(<span class='kw'>N</span> <span class='kw'>=</span> <span class='fl'>500</span>,
          <span class='kw'>theta.init</span> <span class='kw'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/rep.html'>rep</a></span>(<span class='fl'>0</span>, <span class='fl'>3</span>),
          <span class='kw'>epsilon</span> <span class='kw'>=</span> <span class='fl'>0.01</span>,
          <span class='kw'>L</span> <span class='kw'>=</span> <span class='fl'>10</span>,
          <span class='kw'>logPOSTERIOR</span> <span class='kw'>=</span> <span class='no'>poisson_posterior</span>,
          <span class='kw'>glogPOSTERIOR</span> <span class='kw'>=</span> <span class='no'>g_poisson_posterior</span>,
          <span class='kw'>varnames</span> <span class='kw'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/paste.html'>paste0</a></span>(<span class='st'>"beta"</span>, <span class='fl'>0</span>:<span class='fl'>2</span>),
          <span class='kw'>param</span> <span class='kw'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span>(<span class='kw'>y</span><span class='kw'>=</span><span class='no'>y</span>, <span class='kw'>X</span><span class='kw'>=</span><span class='no'>X</span>),
          <span class='kw'>parallel</span><span class='kw'>=</span><span class='fl'>FALSE</span>, <span class='kw'>chains</span><span class='kw'>=</span><span class='fl'>1</span>)</div></pre>
  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <nav id="toc" data-toggle="toc" class="sticky-top">
      <h2 data-toc-skip>Contents</h2>
    </nav>
  </div>
</div>


      <footer>
      <div class="copyright">
  <p>Developed by Samuel Thomas.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.5.1.9000.</p>
</div>

      </footer>
   </div>

  


  </body>
</html>


