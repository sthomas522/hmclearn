% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sample_posterior_gradients.R
\name{hmclearn-glm-posterior}
\alias{hmclearn-glm-posterior}
\alias{linear_posterior}
\alias{g_linear_posterior}
\alias{logistic_posterior}
\alias{g_logistic_posterior}
\alias{poisson_posterior}
\alias{g_poisson_posterior}
\title{Sample log posterior and gradient functions for select generalized linear models}
\usage{
linear_posterior(theta, y, X, a = 1e-04, b = 1e-04, B = 0.001)

g_linear_posterior(theta, y, X, a = 1e-04, b = 1e-04, B = 0.001)

logistic_posterior(theta, y, X, B = 0.01)

g_logistic_posterior(theta, y, X, B = 0.01)

poisson_posterior(theta, y, X, B = 0.01)

g_poisson_posterior(theta, y, X, B = 0.01)
}
\arguments{
\item{theta}{vector of parameters.  Stored as a single vector in order fixed effect, random effect, log-transformed diagonal \eqn{\lambda}, and off-diagonal of \code{G} vector \code{a}}

\item{y}{numeric vector for the dependent variable}

\item{X}{numeric design matrix of fixed effect parameters}

\item{a}{hyperprior for the Inverse Gamma shape parameter}

\item{b}{hyperprior for the Inverse Gamma scale parameter}

\item{B}{prior for linear predictors is multivariate Normal with mean 0 with diagonal covariance B^-1}
}
\value{
numeric value for the log posterior or gradient of the log posterior
}
\description{
Sample log posterior and gradient functions for select generalized linear models
}
\section{Models with available posterior and gradient functions}{

\describe{
  \item{`linear_posterior(theta, y, X, a=1e-4, b=1e-4, B=0.001)`}{
   The likelihood function for linear regression
   \deqn{p(y | X, \beta; \sigma^2) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp{\left(-\frac{1}{2\sigma^2} (y - X\beta)^T(y-X\beta) \right)}}
   with priors \eqn{p(\sigma^2) \sim IG(a, b)} and \eqn{\beta \sim N(0, BI)}.  The variance term is log transformed \eqn{\gamma = \log\sigma^2}
   The input parameter vector \code{theta} is of length \code{k}.  The first \code{k-1} parameters are for \eqn{\beta}, and the last parameter is \eqn{\gamma}
   Note that the Inverse Gamma prior can be problematic for certain applications with low variance.  See Gelman (2006)
   @return numeric value for the log posterior
  }
  \item{`g_linear_posterior(theta, y, X, a = 1e-04, b = 1e-04, B = 0.001)`}{
   Gradient of the log posterior for a linear regression model with Normal prior for the linear parameters and Inverse Gamma for the error term.
   \deqn{p(y | X, \beta; \sigma^2) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp{\left(-\frac{1}{2\sigma^2} (y - X\beta)^T(y-X\beta) \right)}}
   with priors \eqn{p(\sigma^2) \sim IG(a, b)} and \eqn{\beta \sim N(0, BI)}.  The variance term is log transformed \eqn{\gamma = \log\sigma^2}
   The input parameter vector \code{theta} is of length \code{k}.  The first \code{k-1} parameters are for \eqn{\beta}, and the last parameter is \eqn{\gamma}
   Note that the Inverse Gamma prior can be problematic for certain applications with low variance.  See Gelman (2006)
  }
  \item{`logistic_posterior(theta, y, X, B = 0.01) `}{
   Log posterior for a logistic regression model with Normal prior for the linear parameters.
   The likelihood function for logistic regression
   \deqn{L(\beta; X, y) = \prod_{i=1}^{n} \left(\frac{1}{1+e^{-X_i\beta}}\right)^{y_i} \left(\frac{e^{-X_i\beta}}{1+e^{-X_i\beta}}\right)^{1-y_i}}
   with priors \eqn{\beta \sim N(0, BI)}.
   The input parameter vector \code{theta} is of length \code{k}, containing parameter values for \eqn{\beta}
  }
  \item{`g_logistic_posterior(theta, y, X, B = 0.01) `}{
   Gradient of the log posterior for a logistic regression model with Normal prior for the linear parameters.
   The likelihood function for logistic regression
   \deqn{L(\beta; X, y) = \prod_{i=1}^{n} \left(\frac{1}{1+e^{-X_i\beta}}\right)^{y_i} \left(\frac{e^{-X_i\beta}}{1+e^{-X_i\beta}}\right)^{1-y_i}}
   with priors \eqn{\beta \sim N(0, BI)}.
   The input parameter vector \code{theta} is of length \code{k}, containing parameter values for \eqn{\beta}
  }
  \item{`poisson_posterior(theta, y, X, B = 0.01) `}{
   Log posterior for a Poisson regression model with Normal prior for the linear parameters.
   The likelihood function for poisson regression
   \deqn{L(\beta; y, X) = \prod_{i=1}^n \frac{e^{-e^{X_i\beta}}e^{y_iX_i\beta}}{y_i!}}
   with priors \eqn{\beta \sim N(0, BI)}.
   The input parameter vector \code{theta} is of length \code{k}, containing parameter values for \eqn{\beta}
  }
  \item{`g_poisson_posterior(theta, y, X, B = 0.01) `}{
   Gradient of the log posterior for a Poisson regression model with Normal prior for the linear parameters.
   The likelihood function for poisson regression
   \deqn{L(\beta; y, X) = \prod_{i=1}^n \frac{e^{-e^{X_i\beta}}e^{y_iX_i\beta}}{y_i!}}
   with priors \eqn{\beta \sim N(0, BI)}.
   The input parameter vector \code{theta} is of length \code{k}, containing parameter values for \eqn{\beta}
  }
 }
}

\references{
Gelman, A. (2006). \emph{Prior distributions for variance parameters in hierarchical models (comment on article by Browne and Draper)}. Bayesian analysis, 1(3), 515-534.
}
