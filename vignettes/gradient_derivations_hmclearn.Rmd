---
title: "Gradient Derivations for HMC"
author: "Samuel Thomas"
date: "5/21/2020"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{gradient_derivations_hmclearn}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Linear Regression

Likelihood for linear regression

$$
f(\mathbf{y} | \mathbf{X}, \boldsymbol\beta, \sigma^2) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left( -\frac{1}{2\sigma^2} (\mathbf{y} - \mathbf{X}\boldsymbol\beta)^T(\mathbf{y} - \mathbf{X}\boldsymbol\beta) \right)
$$

Log likelihood

$$
\log f(\mathbf{y}|\mathbf{X}, \boldsymbol\beta, \sigma^2) = -\frac{n}{2}\log\sigma^2 - \frac{1}{2\sigma^2} (\mathbf{y} - \mathbf{X}\boldsymbol\beta)^T(\mathbf{y} - \mathbf{X}\boldsymbol\beta) 
$$

Specify priors

$$
\begin{aligned}
\boldsymbol\beta &\sim N(0, \sigma_\beta^2 \mathbf{I}) \\
\sigma^2 &\sim IG(a, b)
\end{aligned}
$$

$$
\begin{aligned}
f(\boldsymbol\beta) &\propto \exp(-\frac{\boldsymbol\beta^T \mathbf{I} \boldsymbol\beta}{2\sigma_\beta^2})
\\
f(\sigma^2) &= \frac{b^a}{\Gamma{(a)}}(\sigma^2)^{-a-1}\exp\left( -\frac{b}{\sigma^2} \right) \\
&\propto (\sigma^2)^{-a-1}\exp\left( -\frac{b}{\sigma^2} \right)
\end{aligned} 
$$
One variable transformation (general)

$$
f_Y(y) = f_X(g^{-1}(y))\left | \frac{dx}{dy}  \right |
$$


Transformation for $\sigma^2$ where $\gamma = \log\sigma^2$, for support on $\mathbb{R}$

$$
\begin{aligned}
\sigma^2 &= g^{-1}(\gamma) = e^{\gamma} \\
f(\gamma) &= \frac{b^a}{\Gamma{(a)}} (e^\gamma)^{-a-1}  \exp\left(- \frac{b}{e^{\gamma}}  \right) e^{\gamma} \\
&\propto \exp{\left(-a\gamma - \frac{b}{e^{\gamma}} \right)}
\end{aligned}
$$

Set $\boldsymbol\theta := (\boldsymbol\beta, \gamma)$

Log prior

$$
\begin{aligned}
\log{f(\boldsymbol\theta)} &= \log f(\boldsymbol\beta, \gamma) = \log f(\boldsymbol\beta | \gamma) f(\gamma) = \log f(\boldsymbol\beta) + \log f(\gamma) \\
&\propto -\frac{\boldsymbol\beta^T \mathbf{I} \boldsymbol\beta}{2\sigma_\beta^2} - \left(a\gamma + \frac{b}{e^{\gamma}} \right)
\end{aligned}
$$

Log posterior = log likelihood + log prior

$$
\begin{aligned}
\log f(\boldsymbol\theta | \mathbf{y}, \mathbf{X}) &\propto  -\frac{n}{2}\gamma - \frac{1}{2e^\gamma} (\mathbf{y} - \mathbf{X}\boldsymbol\beta)^T(\mathbf{y} - \mathbf{X}\boldsymbol\beta) -\frac{\boldsymbol\beta^T \mathbf{I} \boldsymbol\beta}{2\sigma_\beta^2} - \left(a\gamma + \frac{b}{e^{\gamma}} \right) \\
&\propto -\left (\frac{n}{2} + a  \right)\gamma - \frac{e^{-\gamma}}{2} (\mathbf{y} - \mathbf{X}\boldsymbol\beta)^T(\mathbf{y} - \mathbf{X}\boldsymbol\beta)-\frac{\boldsymbol\beta^T \mathbf{I} \boldsymbol\beta}{2\sigma_\beta^2} - b e^{-\gamma}
\end{aligned}
$$

Gradient of the log posterior

$$
\begin{aligned}
\nabla_{\boldsymbol\beta} \log(\boldsymbol\theta) &\propto -\frac{e^{-\gamma}}{2} \left( \mathbf{X}^T (\mathbf{X}\boldsymbol\beta - \mathbf{y})  + \mathbf{X}^T (\mathbf{X}\boldsymbol\beta - \mathbf{y}) \right) - \frac{1}{\sigma_\beta^2} \mathbf{I} \boldsymbol\beta\\
&\propto -e^{-\gamma} \mathbf{X}^T (\mathbf{X}\boldsymbol\beta - \mathbf{y})- \frac{1}{\sigma_\beta^2} \mathbf{I}\boldsymbol\beta\\
&\propto e^{-\gamma} \mathbf{X}^T ( \mathbf{y} - \mathbf{X}\boldsymbol\beta)- \frac{1}{\sigma_\beta^2} \mathbf{I}\boldsymbol\beta  \\
\nabla_\gamma\log\boldsymbol\theta &\propto -\left (\frac{n}{2} + a  \right) +  \frac{e^{-\gamma}}{2} (\mathbf{y} - \mathbf{X}\boldsymbol\beta)^T(\mathbf{y} - \mathbf{X}\boldsymbol\beta) + b e^{-\gamma}
\end{aligned}
$$

\pagebreak

## Logistic Regression

Probability

$$
P(\mathbf{Y} = \mathbf{y}|\mathbf{X}) = [1+\exp(-\mathbf{X}\boldsymbol\beta)]^{-1}
$$

Likelihood

$$
\begin{aligned}
f(\mathbf{y} | \mathbf{X}, \boldsymbol\beta) &= \prod_{i=1}^n p(y_i)^{y_i}(1-p(y_i))^{1-y_i} \\
&= \prod_{i=1}^n \left(\frac{1}{1+e^{-\mathbf{x}_i\boldsymbol\beta}}\right)^{y_i} \left(\frac{e^{-\mathbf{x}_i\boldsymbol\beta}}{1+e^{-\mathbf{x}_i\boldsymbol\beta}}\right)^{1-y_i} 
\end{aligned}
$$

Log likelihood

$$
\begin{aligned}
\log f(\mathbf{y}|\mathbf{X}, \boldsymbol\beta) &= \sum_{i=1}^n -y_i\log(1 + e^{-\mathbf{x}_i\boldsymbol\beta}) - (1-y_i)\mathbf{x}_i\boldsymbol\beta - (1 - y_i)\log(1 + e^{-\mathbf{x}_i\boldsymbol\beta}) \\
&= \sum_{i=1}^n -y_i\log(1+e^{-\mathbf{x}_i\boldsymbol\beta}) - \mathbf{x}_i\boldsymbol\beta + y_i\mathbf{x}_i\boldsymbol\beta - \log(1 + e^{-\mathbf{x}_i\boldsymbol\beta})+y_i\log(1+e^{-\mathbf{x}_i\boldsymbol\beta}) \\
&= \sum_{i=1}^n -\mathbf{x}_i\boldsymbol\beta + y_i\mathbf{x}_i\boldsymbol\beta - \log(1 + e^{-\mathbf{x}_i\boldsymbol\beta}) \\
&= \sum_{i=1}^n (y_i-1)\mathbf{x}_i\boldsymbol\beta - \log(1 + e^{-\mathbf{x}_i\boldsymbol\beta}) \\
&= (\mathbf{y} - \mathbf{1}_n)^T \mathbf{X}\boldsymbol\beta - \mathbf{1}_n^T\log(1 + e^{-\mathbf{X}\boldsymbol\beta})
\end{aligned}
$$

Specify priors

$$
\begin{aligned}
\boldsymbol\beta &\sim N(0, \sigma_\beta^2 \mathbf{I}) 
\end{aligned}
$$

Set $\boldsymbol\theta := (\boldsymbol\beta, \gamma)$

Log prior

$$
\begin{aligned}
\log{f(\boldsymbol\theta)} &= \log f(\boldsymbol\beta)  \\
&\propto -\frac{\boldsymbol\beta^T \mathbf{I} \boldsymbol\beta}{2\sigma_\beta^2} 
\end{aligned}
$$

Log posterior = log likelihood + log prior

$$
\begin{aligned}
\log f(\boldsymbol\beta | \mathbf{y}, \mathbf{X}) &\propto (\mathbf{y} - \mathbf{1}_n)^T \mathbf{X}\boldsymbol\beta - \mathbf{1}_n^T\log(1 + e^{-\mathbf{X}\boldsymbol\beta}) -\frac{\boldsymbol\beta^T \mathbf{I} \boldsymbol\beta}{2\sigma_\beta^2} \\
&\propto \boldsymbol\beta^T\mathbf{X}^T(\mathbf{y} - \mathbf{1}_n) - [\log(1 + e^{-\mathbf{X}\boldsymbol\beta})]^T\mathbf{1}_n -\frac{\boldsymbol\beta^T \mathbf{I} \boldsymbol\beta}{2\sigma_\beta^2} 
\end{aligned}
$$
Gradient of the log posterior

This gradient is a bit complex, we will split into 3 functions

$$
\begin{aligned}
f=\log f(\boldsymbol\beta, \mathbf{y}, \mathbf{X}) &\propto f_1(\mathbf{X}, \boldsymbol\beta) + f_2(\mathbf{X}, \boldsymbol\beta) + f_3(\mathbf{X}, \boldsymbol\beta)
\end{aligned}
$$

Gradient

$$
\begin{aligned}
\nabla_\beta f &\propto \nabla_\beta f_1(\mathbf{X},\boldsymbol\beta) + \nabla_\beta f_2(\mathbf{X},\boldsymbol\beta) + \nabla_\beta f_3(\mathbf{X}, \boldsymbol\beta)
\end{aligned}
$$
First and third functions simplest

$$
\begin{aligned}
\nabla_\beta f_1(\mathbf{X}, \boldsymbol\beta) &= \nabla_\beta (\mathbf{y} - \mathbf{1}_n)^T \mathbf{X}\boldsymbol\beta \\
&= \mathbf{X}^T (\mathbf{y} - \mathbf{1}_n)\\
\nabla_\beta f_3(\mathbf{X}, \boldsymbol\beta) &= -\nabla_\beta \frac{\boldsymbol\beta^T \mathbf{I} \boldsymbol\beta}{2\sigma_\beta^2} \\
&= -\frac{1}{\sigma_\beta^2}\mathbf{I}\boldsymbol\beta
\end{aligned}
$$


Second function more complex

$$
\begin{aligned}
f_2(\mathbf{X}, \boldsymbol\beta) &=  -[\log(1 + e^{-\mathbf{X}\boldsymbol\beta})]^T \mathbf{1}_n  \\
&= -\begin{bmatrix}
\log (1 + e^{-\mathbf{x}_1^T\boldsymbol\beta}) &
\log (1 + e^{-\mathbf{x}_2^T\boldsymbol\beta}) &
... &
\log (1 + e^{-\mathbf{x}_n^T\boldsymbol\beta})
\end{bmatrix} \mathbf{1}_n\\
\nabla_\beta f_2(\mathbf{X}, \boldsymbol\beta) &= -
\begin{bmatrix}
\frac{1}{1 + e^{-\mathbf{x}_1^T\boldsymbol\beta}}e^{-\mathbf{x}_1^T\boldsymbol\beta}(-\mathbf{x}_1) &
\frac{1}{1 + e^{-\mathbf{x}_2^T\boldsymbol\beta}}e^{-\mathbf{x}_2^T\boldsymbol\beta}(-\mathbf{x}_2) &
... &
\frac{1}{1 + e^{-\mathbf{x}_n^T\boldsymbol\beta}}e^{-\mathbf{x}_n^T\boldsymbol\beta}(-\mathbf{x}_n) 
\end{bmatrix} \mathbf{1}_n\\
&= \mathbf{X}^T \left(\frac{e^{-\mathbf{X}\boldsymbol\beta}}{1 + e^{-\mathbf{X}\boldsymbol\beta}}\right) 
\end{aligned}
$$


Finally, the gradient of the log posterior

$$
\begin{aligned}
\nabla_\beta \log f(\boldsymbol\beta, \mathbf{y}, \mathbf{X}) &\propto  \mathbf{X}^T (\mathbf{y} - \mathbf{1}_n) +\mathbf{X}^T \left(\frac{e^{-\mathbf{X}\boldsymbol\beta}}{1 + e^{-\mathbf{X}\boldsymbol\beta}}\right)  -\frac{1}{\sigma_\beta^2}\mathbf{I}\boldsymbol\beta \\
&\propto \mathbf{X}^T \left(\mathbf{y} - \mathbf{1}_n + \frac{e^{-\mathbf{X}\boldsymbol\beta}}{1 + e^{-\mathbf{X}\boldsymbol\beta}} \right) -\frac{1}{\sigma_\beta^2}\mathbf{I}\boldsymbol\beta 
\end{aligned}
$$

\pagebreak

## Poisson regression

Probability

$$
p(y | \mu) = \frac{e^{-\mu}\mu^y}{y!}
$$

Link function

$$
\begin{aligned}
\boldsymbol\mu &:= E(\mathbf{y} | \mathbf{X}, \boldsymbol\beta) = e^{\mathbf{X}\boldsymbol\beta}  \\
\log \boldsymbol\mu &:= \mathbf{X}\boldsymbol\beta 
\end{aligned}
$$


Likelihood

$$
\begin{aligned}
f(\mathbf{y} | \mathbf{X}, \boldsymbol\beta) &= \prod_{i=1}^n \frac{e^{-\mu_i} \mu_i^{y_i}}{y_i!} \\
&= \prod_{i=1}^n \frac{e^{-e^{\mathbf{x}_i^T\boldsymbol\beta}}(e^{\mathbf{x}_i^T\boldsymbol\beta})^{y_i}}{y_i!} \\
&= \prod_{i=1}^n \frac{e^{-e^{\mathbf{x}_i^T\boldsymbol\beta}}e^{\mathbf{x}_i^T\boldsymbol\beta y_i} }{y_i!} 
\end{aligned}
$$

Log likelihood without constants

$$
\begin{aligned}
\log f(\mathbf{y} | \mathbf{X}, \boldsymbol\beta) &\propto \sum_{i=1}^n \log e^{-e^{\mathbf{x}_i^T\boldsymbol\beta}} + \log e^{\mathbf{x}_i^T\boldsymbol\beta y_i} \\
&\propto \sum_{i=1}^n -e^{\mathbf{x}_i^T\boldsymbol\beta} + \mathbf{x}_i^T\boldsymbol\beta y_i \\
&\propto \mathbf{y}^T\mathbf{X}\boldsymbol\beta - \mathbf{1}_n^T e^{\mathbf{X}\boldsymbol\beta}
\end{aligned}
$$

Specify priors

$$
\begin{aligned}
\boldsymbol\beta &\sim N(0, \sigma_\beta^2 \mathbf{I}) 
\end{aligned}
$$

Set $\boldsymbol\theta := (\boldsymbol\beta, \gamma)$

Log prior

$$
\begin{aligned}
\log{f(\boldsymbol\theta)} &= \log f(\boldsymbol\beta)  \\
&\propto -\frac{\boldsymbol\beta^T \mathbf{I} \boldsymbol\beta}{2\sigma_\beta^2} 
\end{aligned}
$$

Log posterior = log likelihood + log prior

$$
\begin{aligned}
\log f(\boldsymbol\beta | \mathbf{y}, \mathbf{X}) &\propto \mathbf{y}^T\mathbf{X}\boldsymbol\beta - \mathbf{1}_n^T e^{\mathbf{X}\boldsymbol\beta} -\frac{\boldsymbol\beta^T \mathbf{I} \boldsymbol\beta}{2\sigma_\beta^2} 
\end{aligned}
$$

Gradient of the log posterior

$$
\nabla_\beta \log f(\boldsymbol\beta | \mathbf{y}, \mathbf{X}) \propto \mathbf{X}^T (\mathbf{y} - e^{\mathbf{X}\boldsymbol\beta}) - \frac{\mathbf{I}\boldsymbol\beta}{\sigma_\beta^2}
$$

\pagebreak

## Linear mixed effects model

Specify the model

$$
\begin{aligned}
\mathbf{y} &= \mathbf{X}\boldsymbol\beta + \mathbf{Z}\mathbf{u} + \boldsymbol\epsilon \\
\mathbf{u} &\sim N(0, \mathbf{G}) \\
\boldsymbol\epsilon &\sim N(0, \sigma_\epsilon^2)
\end{aligned}
$$

Response for each subject is a vector $\mathbf{y} = (\mathbf{y}_1, ..., \mathbf{y}_n)$ for $n$ subjects $i= 1, ..., n$.  Each subject has $d$ observations $\mathbf{y}_i = (y_{i1}, ..., y_{id})$ and we let $j = 1, ..., d$. The fixed effect design matrix is composed of matrices for each subject, $\mathbf{X} = (\mathbf{X}_1, ..., \mathbf{X}_n)$, and $\mathbf{X}_i \in \mathbb{R}^{d\times (q+1)}$ for the fixed effects parameters $\boldsymbol\beta = (\beta_0, ..., \beta_q)$. The full fixed effects design matrix is therefore $\mathbf{X} \in \mathbb{R}^{nd \times (q+1)}$.

For random effects, $\mathbf{Z} = \text{diag}(\mathbf{Z}_1, ..., \mathbf{Z}_n)$, with individual random effects matrices $\mathbf{Z}_i$ for each of the $i$ subjects. A random intercept model specifies $\mathbf{Z}_i$ as a column vector of ones where $\mathbf{Z}_i = \mathbf{z}_i = \mathbf{1}_d$. The full random effects design matrix $\mathbf{Z} \in \mathbb{R}^{nd\times n}$.  The parameterization for random effects is $\mathbf{u} = (\mathbf{u}_1, ..., \mathbf{u}_n)^T$ with vectors $\mathbf{u}_i$ for each subject. A random intercept model is somewhat simplified where $\mathbf{u}_i = u_i$ denotes a single random intercept parameter for each subject $i$, and $\mathbf{u} = (u_1, ..., u_n)^T$.

We set $\mathbf{u}$ as one of our priors, following a multivariate normal distribution, $\mathbf{u} \sim N(0, \mathbf{G})$. For our random intercept model, the specification of the covariance matrix $\mathbf{G}$ is expanded to facilitate efficient sampling using HMC.  We let $\mathbf{u} = \mathbf{G}^{1/2}\boldsymbol\tau$ where $\mathbf{G}^{1/2} = \lambda \mathbf{I}_n$. An additional parameter $\boldsymbol\tau = (\tau_1, ..., \tau_n)^T$ where each of these parameters is standard normal $\tau_i \sim N(0, 1)$. The full covariance matrix is then $\mathbf{G} = \lambda^2 \mathbf{I}_n \boldsymbol\tau$.

The error for each subject $i$ is $\boldsymbol\epsilon_i \sim N(0, \sigma_{\epsilon}^2 \mathbf{I}_d)$. Since the error distribution is constant for each observations, 

$$
\boldsymbol\epsilon \sim N(0, \sigma_{\epsilon}^2 \mathbf{I}_{nd})
$$

For pedagogical purposes, we derive and code the log posterior and gradient for the random intercept model only. Additional functions will be developed for more general mixed effect model specifications.

Random intercept model:

$$
\begin{aligned}
\mathbf{y}_i &= \mathbf{X}_i\boldsymbol\beta + \mathbf{z}_i u_i + \boldsymbol\epsilon_i \\
 &= \mathbf{X}_i\boldsymbol\beta + \mathbf{1}_d u_i + \boldsymbol\epsilon_i
\end{aligned}
$$

Expand by individual observation

$$
y_{ij} = x_{ij}\boldsymbol\beta + z_{ij} u_i + \epsilon_{ij}
$$

Likelihood for random intercept model

$$
f(\mathbf{y} | \mathbf{X}, \mathbf{Z}, \boldsymbol\beta, \mathbf{u}, \sigma^2) = \frac{1}{(2\pi\sigma_\epsilon^2)^{nd/2}}\exp\left( -\frac{1}{2\sigma^2} (\mathbf{y} - \mathbf{X}\boldsymbol\beta - \mathbf{Z}\mathbf{u})^T(\mathbf{y} - \mathbf{X}\boldsymbol\beta - \mathbf{Z}\mathbf{u}) \right)
$$

Log likelihood excluding constants

$$
\log f(\mathbf{y}|\mathbf{X}, \mathbf{Z}, \boldsymbol\beta, \mathbf{u}, \sigma_\epsilon) = -\frac{nd}{2}\log\sigma_\epsilon^2 - \frac{1}{2\sigma_\epsilon^2} (\mathbf{y} - \mathbf{X}\boldsymbol\beta - \mathbf{Z}\mathbf{u})^T(\mathbf{y} - \mathbf{X}\boldsymbol\beta - \mathbf{Z}\mathbf{u}) 
$$

We adjust the formulation of the log likelihood in terms of $\lambda$ and $\boldsymbol\tau$.  

$$
\log f(\mathbf{y}|\mathbf{X}, \mathbf{Z}, \boldsymbol\beta, \lambda, \boldsymbol\tau, \sigma_\epsilon) = -\frac{nd}{2}\log\sigma_\epsilon^2 - \frac{1}{2\sigma_\epsilon^2} (\mathbf{y} - \mathbf{X}\boldsymbol\beta - \lambda\mathbf{Z}\boldsymbol\tau)^T(\mathbf{y} - \mathbf{X}\boldsymbol\beta - \lambda\mathbf{Z}\boldsymbol\tau) 
$$

Specify priors explicitly next.  We let $\lambda$ follow a 2-parameter half-t distribution with hyperparameters $\nu_\lambda$, $A_\lambda$. Again, $\boldsymbol\tau$ are standard normal. We also let $\epsilon$ follow a 2-parameter half-t with hyperparameters $\nu_\epsilon$ and $A_\epsilon$. 

$$
\begin{aligned}
\boldsymbol\beta &\sim N(0, \sigma_\beta^2 \mathbf{I}_{q+1}) \\
\lambda &\sim \text{half-t}(\nu_\lambda, A_\lambda) \\
\boldsymbol\tau &\sim N(0, I_n) \\
\sigma_\epsilon &\sim \text{half-t}(\nu_\epsilon, A_\epsilon)
\end{aligned}
$$

Next, we specify the densities for the priors, omitting constants

$$
\begin{aligned}
\pi(\boldsymbol\beta) &\propto e^{-\frac{1}{2}\boldsymbol\beta^T \mathbf{I}_{(q+1)}\boldsymbol\beta}, \qquad \boldsymbol\beta \in \mathbb{R}^{q+1} \\
\pi(\lambda) &\propto \left(1 + \frac{1}{\nu_\lambda}\left(\frac{\lambda}{A_\lambda} \right)^2 \right)^{-(\nu_\lambda + 1)/2}, \qquad \lambda \in (0, \infty)\\
\pi(\boldsymbol\tau) &\propto e^{-\frac{1}{2}\boldsymbol\tau^T \mathbf{I}_n \boldsymbol\tau}, \qquad \boldsymbol\tau \in \mathbb{R}^{n} \\
\pi(\sigma_\epsilon) &\propto \left(1 + \frac{1}{\nu_\epsilon}\left(\frac{\sigma_\epsilon}{A_\epsilon} \right)^2 \right)^{-(\nu_\epsilon + 1)/2}, \qquad \sigma_\epsilon \in (0, \infty)
\end{aligned}
$$

Proposals for all parameters must be over $\mathbb{R}$. However, the half-t priors $\lambda$ and $\sigma_\epsilon$ are restricted to strictly positive numbers. We apply a $\log$ transform such that these parameters span $\mathbb{R}$. 

Re-parameterize $\lambda$ and $\sigma_\epsilon$ to $\xi$ and $\gamma$, respectively.

$$
\begin{aligned}
\xi &= \log\lambda, \quad \lambda = e^\xi \\
\gamma &= \log\sigma_\epsilon, \quad \sigma_\epsilon = e^\gamma
\end{aligned}
$$
 
 Apply the Jacobian to obtain the distribution of $\xi$
 
 $$
 \begin{aligned}
 \pi_\xi(\xi) &= \pi_\lambda(g^{-1}(\xi))\left | \frac{d\lambda}{d\xi} \right | \\
 &= \pi_\lambda(e^{\xi}) | e^\xi |\\
 &\propto \left(1 + \frac{1}{\nu_\xi}\left(\frac{\xi}{A_\xi} \right)^2 \right)^{-(\nu_\xi + 1)/2} e^{\xi}
 \end{aligned}
 $$

 Apply the Jacobian to obtain the distribution of $\gamma$
 
 $$
 \begin{aligned}
 \pi_\gamma(\gamma) &= \pi_{\sigma_\epsilon}(g^{-1}(\gamma))\left | \frac{d\sigma_\epsilon}{d\gamma} \right | \\
 &= \pi_{\sigma_\epsilon}(e^{\gamma}) | e^\gamma |\\
 &\propto \left(1 + \frac{1}{\nu_\gamma}\left(\frac{\gamma}{A_\gamma} \right)^2 \right)^{-(\nu_\gamma + 1)/2} e^{\gamma}
 \end{aligned}
 $$

Next, the log priors that we use with transformations, again omitting constants. Note that the log densities of the priors with log densities include an additive term for the transformed distribution (i.e. $\xi$ and $\gamma$). 

$$
\begin{aligned}
\log \pi(\beta) &\propto -\frac{\boldsymbol\beta^T \mathbf{I}_{(q+1)}\boldsymbol\beta}{2\sigma_\beta^2} \\
\log \pi(\xi) &\propto -\frac{\nu_\xi + 1}{2} \log \left( 1 + \frac{1}{\nu_\xi} \left(\frac{e^\xi}{A_\xi} \right)^2 \right) + \xi \\
\log \pi(\boldsymbol\tau) &\propto -\frac{1}{2}\boldsymbol\tau^T \mathbf{I}_n \boldsymbol\tau \\
\log \pi(\gamma) &\propto -\frac{\nu_\gamma + 1}{2} \log \left( 1 + \frac{1}{\nu_\gamma} \left(\frac{e^\gamma}{A_\gamma} \right)^2 \right) + \gamma
\end{aligned}
$$

Our final adjustment to the log likelihood replaces $\lambda$ and $\sigma_\epsilon$ with $\xi$ and $\gamma$, respectively.

$$
\log f(\mathbf{y}|\mathbf{X}, \mathbf{Z}, \boldsymbol\beta, \xi, \boldsymbol\tau, \gamma) = -nd\gamma - \frac{1}{2 e^{2\gamma}} (\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau)^T(\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau) 
$$

The log posterior is the log likelihood plus the log prior. In this example, the prior parameters are independent of each other.

$$
\begin{aligned}
\log f(\boldsymbol\beta, \xi, \boldsymbol\tau, \gamma | \mathbf{y}, \mathbf{X}, \mathbf{Z}) &\propto \log f(\mathbf{y} | \mathbf{X}, \mathbf{Z}, \boldsymbol\beta, \xi, \boldsymbol\tau, \gamma)  + \log \pi(\boldsymbol\beta, \xi, \boldsymbol\tau, \gamma) \\ 
&\propto \log f(\mathbf{y} | \mathbf{X}, \mathbf{Z}, \boldsymbol\beta, \xi, \boldsymbol\tau, \gamma) + \log \pi(\boldsymbol\beta) + \log \pi(\xi) + \log\pi(\boldsymbol\tau) + \log\pi(\gamma) \\
&\propto -nd\gamma - \frac{1}{2 e^{2\gamma}} (\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau)^T(\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau) - \\
&\quad \frac{\boldsymbol\beta^T \mathbf{I}_{(q+1)}\boldsymbol\beta}{2\sigma_\beta^2} -\frac{\nu_\xi + 1}{2} \log \left( 1 + \frac{1}{\nu_\xi} \left(\frac{e^\xi}{A_\xi} \right)^2 \right) + \xi - \\
&\quad \frac{1}{2}\boldsymbol\tau^T \mathbf{I}_n \boldsymbol\tau -\frac{\nu_\gamma + 1}{2} \log \left( 1 + \frac{1}{\nu_\gamma} \left(\frac{e^\gamma}{A_\gamma} \right)^2 \right) + \gamma
\end{aligned}
$$

Next, we compute the gradient of the log posterior

First $\nabla_\beta$

$$
\begin{aligned}
\nabla_\beta \log f(\boldsymbol\beta, \xi, \boldsymbol\tau, \gamma | \mathbf{y}, \mathbf{X}, \mathbf{Z}) &\propto -\frac{1}{2e^{2\gamma}} (-2)\mathbf{X}^T(y-\mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau)- \frac{1}{\sigma_\beta^2}\mathbf{I}_{(q+1)}\boldsymbol\beta \\
&\propto e^{-2\gamma}\mathbf{X}^T(y-\mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau)- \frac{1}{\sigma_\beta^2}\mathbf{I}_{(q+1)}\boldsymbol\beta
\end{aligned}
$$

Next $\nabla_\xi$

$$
\begin{aligned}
\nabla_\xi \log f(\boldsymbol\beta, \xi, \boldsymbol\tau, \gamma | \mathbf{y}, \mathbf{X}, \mathbf{Z}) &\propto -\frac{e^{-2\gamma}}{2}(-2)(\mathbf{Z}\boldsymbol\tau)^T e^{\xi} (\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau) - \frac{\nu_\xi + 1}{2}\left( 1 + \frac{1}{\nu_\xi} \left(\frac{e^\xi}{A_\xi} \right)^2 \right)^{-1}\frac{2\xi}{\nu_\xi A_\xi^2} + 1 \\
&\propto e^{-2\gamma + \xi} \boldsymbol\tau^T \mathbf{Z}^T (\mathbf{y}-\mathbf{X}\boldsymbol\beta -e^{\xi}\mathbf{Z}\boldsymbol\tau) - \frac{\nu_\xi + 1}{1 + \nu_\xi A_\xi^2 e^{-2\xi}} + 1
\end{aligned}
$$

Next $\nabla_\tau$

$$
\begin{aligned}
\nabla_\tau \log f(\boldsymbol\beta, \xi, \boldsymbol\tau, \gamma | \mathbf{y}, \mathbf{X}, \mathbf{Z}) &\propto -\frac{e^{-2\gamma}}{2}(-2)(\mathbf{Z}\boldsymbol)^T e^{\xi}(\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau) - \frac{1}{2}\mathbf{I}_n\boldsymbol\tau \\
&\propto e^{-2\gamma} \mathbf{Z}^T(\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau) - \frac{1}{2}\mathbf{I}_n \boldsymbol\tau
\end{aligned}
$$

Next $\nabla_\gamma$

$$
\begin{aligned}
\nabla_\gamma \log f(\boldsymbol\beta, \xi, \boldsymbol\tau, \gamma | \mathbf{y}, \mathbf{X}, \mathbf{Z}) &\propto -nd + e^{-2\gamma}(\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau)^T (\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau) - \frac{\nu_\gamma + 1}{2}\left( 1 + \frac{1}{\nu_\gamma} \left(\frac{e^\gamma}{A_\gamma} \right)^2 \right)^{-1}\frac{2\gamma}{\nu_\gamma A_\gamma^2} + 1 \\ 
&\propto -nd + e^{-2\gamma}(\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau)^T (\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau) - \frac{\nu_\gamma + 1}{1 + \nu_\gamma A_\gamma^2 e^{-2\gamma}} + 1
\end{aligned}
$$


Full gradient of the log posterior specified

$$
\begin{aligned}
\nabla_\beta \log f(\boldsymbol\beta, \xi, \boldsymbol\tau, \gamma | \mathbf{y}, \mathbf{X}, \mathbf{Z})
&\propto e^{-2\gamma}\mathbf{X}^T(y-\mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau)- \frac{1}{\sigma_\beta^2}\mathbf{I}_{(q+1)}\boldsymbol\beta \\
\nabla_\xi \log f(\boldsymbol\beta, \xi, \boldsymbol\tau, \gamma | \mathbf{y}, \mathbf{X}, \mathbf{Z}) 
&\propto e^{-2\gamma + \xi} \boldsymbol\tau^T \mathbf{Z}^T (\mathbf{y}-\mathbf{X}\boldsymbol\beta -e^{\xi}\mathbf{Z}\boldsymbol\tau) - \frac{\nu_\xi + 1}{1 + \nu_\xi A_\xi^2 e^{-2\xi}} + 1 \\
\nabla_\tau \log f(\boldsymbol\beta, \xi, \boldsymbol\tau, \gamma | \mathbf{y}, \mathbf{X}, \mathbf{Z}) &\propto e^{-2\gamma} \mathbf{Z}^T(\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau) - \frac{1}{2}\mathbf{I}_n \boldsymbol\tau \\
\nabla_\gamma \log f(\boldsymbol\beta, \xi, \boldsymbol\tau, \gamma | \mathbf{y}, \mathbf{X}, \mathbf{Z}) &\propto  -nd + e^{-2\gamma}(\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau)^T (\mathbf{y} - \mathbf{X}\boldsymbol\beta - e^{\xi}\mathbf{Z}\boldsymbol\tau) - \frac{\nu_\gamma + 1}{1 + \nu_\gamma A_\gamma^2 e^{-2\gamma}} + 1
\end{aligned}
$$
