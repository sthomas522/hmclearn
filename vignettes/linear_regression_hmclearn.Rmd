---
title: "hmclearn package:  Linear Regression Example"
author: "Samuel Thomas"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{linear_regression_hmclearn}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction

This vignette demonstrates fitting a linear regression model via Hamiltonian Monte Carlo (HMC) using the **hmclearn** package. 

$$
\begin{aligned}
y &= X\beta + \epsilon \\
\epsilon &\sim N(0, \sigma^2)
\end{aligned}
$$

# Linear Regression Example Data

```{r setup}
library(hmclearn)
```

The user must define provide the design matrix directly for use in **hmclearn**.  Our first step is to load the data and store the design matrix $X$ and dependent variable vector $y$. 

Load Scots Race data (Atkinson 1986, Venables and Ripley 2002) and display the first few rows of the design matrix $X$.  This example also appears in Agresti (2015), and we compare results to his.  

```{r, echo=TRUE}
ScotsRaces <- MASS::hills

X <- cbind(1, as.matrix(ScotsRaces[, -which(colnames(ScotsRaces) == "time")]))

# add interaction
X <- cbind(X, X[, "dist"] * X[, "climb"])
colnames(X)[ncol(X)] <- "climb_distance"
colnames(X)[1] <- "intercept"

y <- ScotsRaces$time

head(X, 3)
```


# Frequentist linear model

To compare results, we first fit a standard linear model using the frequentist function *lm*.

```{r, echo=TRUE}
f <- lm(y ~ X-1)
summary(f)

```

Next, we store the parameter values from the Frequentist fit

```{r, echo=TRUE}
beta.freq <- as.numeric(coefficients(f))
sigma2.freq <- sigma(f)^2
theta.freq <- c(beta.freq, log(sigma2.freq))

```

# Prep for HMC

HMC requires the specification of the log posterior to a proportional constant.  In addition, HMC uses the gradient of the log posterior to guide simulations.  The full derivations are provided here.

## Log posterior and gradient

First, we specify the likelihood function for linear regression

$$
p(y | X, \beta; \sigma^2) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp{\left(-\frac{1}{2\sigma^2} (y - X\beta)^T(y-X\beta) \right)}
$$

The posterior is defined with priors for $\beta$ and $\sigma^2$.  

$$
\begin{aligned}
p(\beta, \sigma^2|y; X) &\propto p(y|X, \beta, \sigma^2) p(\beta, \sigma^2) \\
&\propto p(y|X, \beta, \sigma^2)p(\beta)p(\sigma^2)
\end{aligned}
$$

We specify a multivariate Normal prior for $\beta$ and Inverse Gamma for $\sigma^2$.  Note that Inverse Gamma can be problematic in certain models where the variance is low (Gelman 2006).  

$$
\begin{aligned}
p(\beta) &\propto N(0, BI) \\
p(\sigma^2) &\sim IG(a, b)
\end{aligned}
$$

The pdf for $\sigma^2$ prior

$$
p(\sigma^2) = \frac{b^a}{\Gamma(a)} (\sigma^2)^{-a-1} \exp\left(-\frac{b}{\sigma^2} \right)
$$

We perform a log-transformation of $\sigma^2$ to allow proposals across entire real number line.  For unconstrained HMC simulations, all parameters must support all real numbers.  

$$
\begin{aligned}
\gamma &= \log \sigma^2 \\
\sigma^2 &= g^{-1}(\gamma) = e^\gamma \\
p_\gamma(\gamma) &= p_{\sigma^2}(g^{-1}(\gamma))\left\lvert \frac{d\sigma^2}{d\gamma} \right\rvert \\
&= \frac{b^a}{\Gamma(a)}(e^\gamma)^{-a-1} \exp\left(-\frac{b}{\sigma^2} \right) \left\lvert e^\gamma  \right\rvert \\
&= \frac{b^a}{\Gamma(a)}e^{-a\gamma}\exp\left(-\frac{b}{e^\gamma} \right) \\
&\propto e^{-a\gamma}\exp\left(-\frac{b}{e
^\gamma} \right) \\
\log p(\gamma) &\propto -a\gamma - be^{-\gamma}
\end{aligned}
$$

The posterior and log posterior for linear regression, based on $\beta$ and $\gamma$, can now be derived.  

$$
\begin{aligned}
p(\beta, \gamma|y, X) &\propto p(y|X, \beta, \gamma) p(\beta, \gamma) \\
&\propto p(y|X, \beta, \gamma)p(\beta)p(\gamma) \\
&\propto \frac{1}{(2\pi e^\gamma)^{n/2}}\exp{\left(-\frac{1}{2 e^\gamma} (y - X\beta)^T(y-X\beta) \right)} e^{-a\gamma}\exp\left(-\frac{b}{e^\gamma} \right) \\
&\propto \frac{e^{-\gamma n/2}}{(2\pi)^{n/2}}\exp{\left(-\frac{e^{-\gamma}}{2} (y - X\beta)^T(y-X\beta) \right)} e^{-\frac{1}{2}\beta^T\Sigma_\beta^{-1}\beta} e^{-a\gamma} e^{-b e^{-\gamma}} \\
\log p(\beta, \gamma | y, X) &\propto -\frac{\gamma n}{2} -\frac{e^{-\gamma}}{2} (y - X\beta)^T(y-X\beta) - a\gamma - b e^{-\gamma}  \\
&\propto -\left(\frac{n}{2} + a \right)\gamma  -\frac{e^{-\gamma}}{2} (y - X\beta)^T(y-X\beta)- b e^{-\gamma} - \frac{1}{2}\beta^T \Sigma_\beta^{-1}\beta
\end{aligned}
$$

Next, we derive the gradient of the log posterior


$$
\begin{aligned}
\log p(\beta, \gamma | y, X) &\propto -\frac{\gamma n}{2} -\frac{e^{-\gamma}}{2} (y - X\beta)^T(y-X\beta) - a\gamma - b e^{-\gamma}  \\
&\propto -\left(\frac{n}{2} + a \right)\gamma  -\frac{e^{-\gamma}}{2} (y - X\beta)^T(y-X\beta)- b e^{-\gamma} \\
\frac{\partial}{\partial \beta} \log p(\beta, \gamma | y, X) &\propto -\frac{e^{-\gamma}}{2}(-2)X^T(y-X\beta) - \Sigma_\beta^{-1}\beta\\
&\propto e^{-\gamma} X^T(y-X\beta) \\
\frac{\partial}{\partial\gamma}\log p(\beta, \gamma | y, X) &\propto -\left(\frac{n}{2} + a \right) + \frac{e^{-\gamma}}{2} (y - X\beta)^T(y-X\beta)+ b e^{-\gamma}
\end{aligned}
$$

The functions for the log posterior and its gradient are coded in the *linear_posterior* and *g_linear_posterior* functions, respectively, in **hmclearn**.

# Fit model using hmc

Next, we fit the linear regression model using HMC.  A vector of $\epsilon$ values are specified to align with the data.  

```{r, echo=TRUE}
N <- 1e4

eps_vals <- c(1e-1, 1e-2, 1e-4, 5e-6, 3e-3)

set.seed(412)
t1.hmc <- Sys.time()
 f_hmc <- hmc(N=N, theta.init = rep(0, 5), 
                    epsilon = eps_vals, L = 20, 
                    logPOSTERIOR = linear_posterior, 
                    glogPOSTERIOR = g_linear_posterior, 
                    param=list(y = y, X=X), 
                    varnames = c(colnames(X), "log_sigma2"),
                    parallel=TRUE, chains=2)
t2.hmc <- Sys.time()
t2.hmc - t1.hmc

```

The acceptance ratio for each of the HMC chains is sufficiently high for an efficient simulation.  

```{r, echo=TRUE}
f_hmc$accept/N
```

The posterior quantiles are summarized after removing an initial *burnin* period.  

```{r, echo=TRUE}
summary(f_hmc, burnin=2000)
```

Trace plots provide a visual indication of stationarity.  These plots indicate that the MCMC chains are reasonably stationary.  


```{r, echo=TRUE, fig.width=6, fig.height=4}
mcmc_trace(f_hmc, burnin=2000)
```

Histograms of the posterior distribution show that Bayesian parameter estimates align with Frequentist estimates for this example. 


```{r, echo=TRUE, fig.width=6, fig.height=4}
diagplots(f_hmc, comparison.theta=theta.freq, burnin=2000)


```

# Source

A.C. Atkinson (1986) Comment: Aspects of diagnostic regression analysis. *Statistical Science* 1, 397â€“402.

# References

Venables, W. N. and Ripley, B. D. (2002) *Modern Applied Statistics with S*. Fourth edition. Springer.  ISBN 0-387-95457-0

Gelman, Andrew. (2006) "Prior distributions for variance parameters in hierarchical models (comment on article by Browne and Draper)." *Bayesian analysis* 1.3: 515-534.

Agresti, A. (2015). *Foundations of linear and generalized linear models*. John Wiley & Sons.  ISBN: 978-1-118-73003-4


