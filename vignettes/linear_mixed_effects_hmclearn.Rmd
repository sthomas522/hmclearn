---
title: "hmclearn package:  Linear Mixed Effects Regression Example"
author:  "Samuel Thomas"
date: "``r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{linear_mixed_effects_hmclearn}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(hmclearn)
```


This update re-parameterizes the random effects parameterization $u$ per Betancourt, Girolami (2013), a paper on Hamiltonian Monte Carlo for Hierarchical Models.  

Further, the uniform parameterization of the variance parameters is replaced by a half-t family of distributions per Gelman (2006), Prior distributions for Variance parameters in hierarchical models.  This parameterization is well-behaved around 0, in contrast to inverse gamma, and provides flexibility for more informed priors than a uniform distribution.

The data and application are the same.  See linear_mixed_effect_model_LDL_with_Correlation_no_gibbs_v5.Rmd for the prior parameterization and results.

## Random Intercept Model

\textbf{Fit LMM model}

Data preparation

See Agresti 9.2.5 - sample data

Where $i = 1, ..., m$ and $j = 1, ..., d$ 


```{r, echo=TRUE}
library(lme4)
library(Matrix)
data(sleepstudy)

# dependent variable
y <- sleepstudy$Reaction
n <- length(y)

yi.lst <- split(sleepstudy$Reaction, sleepstudy$Subject)

# fixed effects
ss2 <- sleepstudy
ss2$int <- 1
ss2 <- ss2[, c(4, 1:3)] # rearrange columns to store in list
Xi.lst <- split(ss2[, which(colnames(ss2) %in% c("Days", "int"))], 
                ss2$Subject)
Xi.lst <- lapply(Xi.lst, as.matrix)

X <- as.matrix(do.call(rbind, Xi.lst))

# random effects
m <- length(unique(sleepstudy$Subject))

##########
# intercept
Zi.lst <- replicate(m, matrix(rep(1, n/m), ncol=1), simplify=FALSE)
Z <- as.matrix(bdiag(Zi.lst))
```


Frequentist model

Random intercept model

This model assumes independence for random effects

```{r, echo=TRUE}
# random intercept 

## linear mixed models - reference values from older code
(fm1 <- lmer(Reaction ~ Days + (1 | Subject), sleepstudy, REML = FALSE))
summary(fm1)# (with its own print method; see class?merMod % ./merMod-class.Rd

```

## Parameterization for $G$

We select a parameterization of $G$ such that the likelihood can be derived for Metropolis-Hastings.  Further, we want a likelihood for which we can directly calculate the gradient of the log likelihood in preparation for Hamiltonian Monte Carlo.  To this end, we uses LDL decomposition of $G$ to form a flexibile parameterization that can easily handle restrictions.  

Ref:  MCMC Estimation of Restricted Covariance Matrices by Chan and Jelizkov, 2009, Journal of Computational and Graphical Statistics


## Updated parameterization

The distribution of the random effects are defined as normal with a mean of zero. 

Note that in this parameterization, we directly decompose $G$ instead of $G^{-1}$ as in Chan and Jelikzhov (2009).

$$
\begin{aligned}
u &\sim N(0, G)  \\
G &= L D L^T \\
&= L D^{1/2} D^{1/2} L^T \\
\end{aligned}
$$

Let $\lambda_k$ where $k = 1, ... p$ denote the diagonal entries of $D^{1/2}$ and let $a_{kj}$ where $1 \leq j < k \leq p$ denote free elements of lower unitrangular matrix $L$

$$
D^{1/2} := 
\begin{pmatrix}
\lambda_1 & 0 & ... & 0 \\
0 & \lambda_2 & 0 ... & 0 \\
... & ... & ... & ... \\
0 & 0 & ... & \lambda_p
\end{pmatrix}, 
L :=
\begin{pmatrix}
1 & 0 & 0 & ... & 0 \\
a_{21} & 1 & 0 & ... & 0 \\
a_{31} & a_{32} & 1 & ... & ... \\
... & ... & ... & ... & ... \\
a_{p1} & a_{p2} & ... & ... & 1 \\
\end{pmatrix}
$$

Also define $\lambda := (\lambda_1, ..., \lambda_p)^T$ and $a_k := (a_{k1}, ..., a_{k, k-1})^T$ and $a := (a_2^T, ..., a_p^T)^T$

Consider priors where $k = 1...p$.  The prior for $\lambda_k$ is half-t per Gelman (2006).  The previous parameterization was inverse gamma, which can be unintentionally informative for hierarchical models.  Another previous parameterization was uniform, which was also recommended by Gelman for more than 3 groups, but has the consequence of a positive bias.  

Prior distributions for variance parameters in hierarchical models.  Andrew Gelman (2006)

We use a half-t prior for standard deviation $\lambda_k$

$$
\begin{aligned}
p(\lambda_k) &\sim \left(1 + \frac{1}{\nu}\left(\frac{\lambda_k}{A} \right)^2 \right)^{-(\nu+1)/2} \\
a|\lambda &\sim N(a_0, A_0)
\end{aligned}
$$

The hyperparameter $a_0$ does not need to be zero, and $A_0$ can be correlated and may depend on $\lambda$.  In this model, we define $a$ independent of $\lambda$. 

In prior parameterizations, we directly defined the likelihood of the random effects $u$.  Per Betancourt, Girolami (2013), we re-parameterize $u$ using a standard normal parameterization we define as $\tau = (\tau_1, ..., \tau_q)$.  Here, $u$ is a deterministic function of $G$ and $\tau$.

$$
\begin{aligned}
\tau &\sim N(0, I_q) \\
u &:= L D^{1/2} \tau \\
&\sim N(0, LD^{1/2} I (L D^{1/2})^T) \\
&\sim N(0, L D^{1/2} D^{1/2} L^T) \\
&\sim N(0, G)
\end{aligned}
$$

The distribution of $u$ therefore does not change with this parameterization.  The intent of our re-parameterization is to allow $G$ and $\tau$ to be largely independent in the MCMC sampling.  A direct parameterization of $u$ and $G$ can introduced what has been called the Funnel from hell. See http://twiecki.github.io/blog/2017/02/08/bayesian-hierchical-non-centered/ Why hierarchical models are awesome, tricky, and Bayesian.  

## Derive the full log posterior

For MH, we need to derive the full log posterior

First, the likelihood and log-likelihood 

$$
\begin{aligned}
p(y | \beta, u_1, ..., u_c, \sigma_\epsilon^2) &\propto (\sigma_\epsilon^2)^{-n/2} e^{-\frac{1}{2\sigma_\epsilon^2}(y - X\beta - Zu)^T (y - X\beta - Zu)} \\
\log p(y | \beta, u_1, ..., u_c, \sigma_\epsilon^2) &\propto -{n}\log(\sigma_\epsilon) -\frac{1}{2\sigma_\epsilon^2}(y - X\beta - Zu)^T (y - X\beta - Zu)
\end{aligned}
$$

Bayes rule to determine posterior

$$
\begin{aligned}
p(\beta, u, \sigma_\epsilon^2, G | y, X, Z) &\propto p(y | \beta, u, \sigma_\epsilon^2, G)  p(\beta, u, \sigma_\epsilon^2, G) \\
&\propto p(y | \beta, u, \sigma_\epsilon^2, G) p(\beta) p(\sigma_\epsilon^2) p(u, G) \\
&\propto p(y | \beta, u, \sigma_\epsilon^2, G) p(\beta) p(\sigma_\epsilon^2) p(u | G) p(G) \\
\log p(\beta, u, \sigma_\epsilon^2, G | y, X, Z) &\propto \log p(y | \beta, u, \sigma_\epsilon^2, G) + \log p(\beta) + \log p(\sigma_\epsilon^2)+ \log p(u|G) + \log p(G) \\
\log p(\beta, \sigma_\epsilon, \tau, \lambda_1...\lambda_q,a | y, X, Z) &\propto \log p(y | \beta, \tau, \sigma_\epsilon, \lambda, a) + \log p(\beta) + \log p(\sigma_\epsilon)+ \log p(\tau) + \log p(\lambda) + \log p(a)
\end{aligned}
$$

Select priors.  We parameterize all standard deviations (instead of variance) as from the half-t family.  

$$
\begin{aligned}
p(\beta) &\propto N(0, BI) \\
p(\sigma_\epsilon) &\sim  \left(1 + \frac{1}{\nu_\epsilon}\left(\frac{\sigma_\epsilon}{A_\epsilon} \right)^2 \right)^{-(\nu_\epsilon+1)/2}  \\
p(\lambda_k) &\sim  \left(1 + \frac{1}{\nu_{\lambda_k}}\left(\frac{\lambda_k}{A_{\lambda_k}} \right)^2 \right)^{-(\nu_{\lambda_k}+1)/2}  \\
a &\sim N(0, A_a)
\end{aligned}
$$

We want proposals of $\sigma_\epsilon^2$ over the real number line.  Therfore we derive the distribution of the log Inverse Gamma based on a simple change of variable

$$
\begin{aligned}
\gamma &:= \log \sigma_\epsilon \\
e^\gamma &= \sigma_\epsilon 
\end{aligned}
$$


We need to compute the Jacobian of the transformation

$$
\begin{aligned}
p_{\gamma}(\gamma) &= p_{\sigma_\epsilon}(g^{-1}(\gamma))\lvert \frac{d\sigma_\epsilon}{d\gamma} \rvert \\
&= \left(1 + \frac{1}{\nu_\epsilon} \left(\frac{e^\gamma}{A_\epsilon} \right)^2 \right)^{-\frac{\nu_\epsilon +1}{2}} e^{\gamma} \\
&= \left(1 + \frac{1}{\nu_\epsilon} \frac{e^{2\gamma}}{A_\epsilon^2} \right)^{-\frac{\nu_\epsilon +1}{2}} e^{\gamma} \\
\log p_{\gamma}(\gamma) &\propto -\frac{\nu_\epsilon+1}{2}\log\left(1 + \frac{1}{\nu_\epsilon} \frac{e^{2\gamma}}{A_\epsilon^2} \right) + \gamma
\end{aligned}
$$


We have hyperpriors $\lambda$ and $a$ for $G$

Re-parameterize $\xi_k = \log \lambda_k$ and $e^{\xi_k} = \lambda_k$

Recall that we parameterized $\lambda_k$ as uniform

$$
\begin{aligned}
\lambda_k &\sim \text{half-t}(\nu_{\lambda_k}, A_{\lambda_k}) \\
p_{\xi_k}(\xi_k) &= p_{\lambda_k}(g^{-1}(\xi_k)) \left\lvert \frac{d\lambda_k}{d\xi_k}  \right\rvert \\
&= p_{\lambda_k} (e^{\xi_k})\lvert e^{\xi_k}\rvert \\
&= \left(1 + \frac{1}{\nu_{\lambda_k}} \frac{e^{2\xi_k}}{A_{\lambda_k}^2}\right)^{-\frac{\nu_{\lambda_k}+1}{2}}e^{\xi_k} \\
\log p(\xi_k) &\propto -\frac{\nu_{\lambda_k}+1}{2}\log\left(1 + \frac{1}{\nu_{\lambda_k}} \frac{e^{2\xi_k}}{A_{\lambda_k}^2}\right)+ \xi_k
\end{aligned}
$$

Leveraging an idea from Chan, Jeliazkov (2009) MCMC estimation of restricted covariance matrices

We write the matrix form of $u = L D^{1/2} \tau$ in a form with a parameter vector $a$ by which the gradient can be computed directly

$$
\begin{aligned}
u &= L D^{1/2} \tau \\
&= \widetilde{\tau} + \widetilde{T}a \\
&=\begin{pmatrix}
e^\xi_1\tau_1 \\
e^\xi_2\tau_2 \\
e^\xi_3\tau_3 \\
... \\
... \\
e^\xi_q\tau_q
\end{pmatrix} + 
\begin{pmatrix}
0 & ... & & & & & & ... & 0 \\
e^{\xi_1}\tau_1 & 0 & ... & & & & & & ... \\
0 & e^{\xi_1}\tau_1 & e^{\xi_2}\tau_2 & 0 & ... & & & & 0 \\
0 & ... & 0 & e^{\xi_1}\tau_1 & e^{\xi_2}\tau_2 & e^{\xi_3}\tau_3 & 0 & ... & ... \\
... & ... & ... & ... & ... & ... & ... & ... & ... \\
0 & ... & ... & 0 & ... & ... & e^{\xi_1}\tau_1 & ... & e^{\xi_q}\tau_q \\
\end{pmatrix}
\begin{pmatrix}
a_{21} \\
a_{31} \\
a_{32} \\
... \\
... \\
a_{q,q-1}
\end{pmatrix}
\end{aligned}
$$


Since we decompose $G$ using $L D L^T$.  We make use of the determinant property of a square matrix, where the determinant of a square matrix is equal to the determinant of its transpose.  Also, from above $\lvert L \rvert = 1$

$$
\begin{aligned}
\lvert G \rvert &= \lvert L D L^T \rvert \\
&= \lvert L \rvert \lvert D \rvert \lvert L^T \rvert \\
&= \lvert D \rvert \\
&= \prod_{k=1}^q e^{2\xi_k} \\
&= e^{2\sum_{k=1}^q \xi_k} \\
\log \lvert G \rvert &= 2\sum_{k=1}^q \xi_k
\end{aligned}
$$

Assign relatively uniformative prior for $a$

$$
\begin{aligned}
a_k &\sim N(0, A) \\
p(a_k) &\propto \lvert A  \rvert^{-1/2} e^{-\frac{1}{2} a_k^T A^{-1} a_k} \\
\log p(a_k) &\propto -\frac{1}{2}a_k^T A^{-1} a_k
\end{aligned}
$$

Recall that the prior for $\tau$ is standard Normal (multivariate)

$$
\begin{aligned}
\tau &\sim N(0, I) \\
p(\tau) &\sim e^{-\frac{1}{2}\tau^T I \tau} \\
\log p(\tau) &\sim -\frac{1}{2} \tau^T \tau
\end{aligned}
$$


Write the full log posterior

$$
\begin{aligned}
\log p(\beta, \sigma_\epsilon, \tau, \lambda_1...\lambda_q,a | y, X, Z) &\propto \log p(y | \beta, \tau, \sigma_\epsilon, \lambda, a) + \log p(\beta) + \log p(\sigma_\epsilon)+ \log p(\tau) + \log p(\lambda) + \log p(a) \\
\log p(\beta, \gamma, \tau, \xi_1...\xi_q,a | y, X, Z) &\propto \log p(y | \beta, \tau, \gamma, \xi, a) + \log p(\beta) + \log p(\gamma)+ \log p(\tau) + \log p(\xi) + \log p(a) \\
&\propto  -n\gamma -\frac{e^{-2\gamma}}{2}(y - X\beta - Zu)^T (y - X\beta - Zu)  - \frac{1}{2}\beta^T\Sigma_\beta^{-1}\beta \\
&-\frac{\nu_\epsilon+1}{2}\log\left(1 + \frac{1}{\nu_\epsilon} \frac{e^{2\gamma}}{A_\epsilon^2} \right) + \gamma  -\frac{1}{2} a^T A^{-1} a -\frac{1}{2} \tau^T \tau\\
&-\sum_{k=1}^q \left(\frac{\nu_{\lambda_k}+1}{2}\log\left(1 + \frac{1}{\nu_{\lambda_k}} \frac{e^{2\xi_k}}{A_{\lambda_k}^2}\right)+ \xi_k\right)\\
&\propto  -n\gamma -\frac{e^{-2\gamma}}{2}(y - X\beta - ZL D^{1/2}\tau)^T (y - X\beta - ZL D^{1/2}\tau) - \frac{1}{2}\beta^T\Sigma_\beta^{-1}\beta \\
&-\frac{\nu_\epsilon+1}{2}\log\left(1 + \frac{1}{\nu_\epsilon} \frac{e^{2\gamma}}{A_\epsilon^2} \right) + \gamma  -\frac{1}{2} a^T A^{-1} a -\frac{1}{2} \tau^T \tau\\
&-\sum_{k=1}^q \left(\frac{\nu_{\lambda_k}+1}{2}\log\left(1 + \frac{1}{\nu_{\lambda_k}} \frac{e^{2\xi_k}}{A_{\lambda_k}^2}\right)+ \xi_k\right)\\
\end{aligned}
$$

\pagebreak

# Hamiltonian Monte Carlo

Derive the gradient of the log posterior

Gradient of $\tau$ 

$$
\begin{aligned}
\frac{\partial l}{\partial\tau} &\propto \frac{\partial}{\partial\tau}\left(-\frac{e^{-2\gamma}}{2}(y-X\beta-ZLD^{1/2}\tau)^T(y-X\beta-ZLD^{1/2}\tau) - \frac{1}{2}\tau^T\tau\right) \\
&\propto e^{-2\gamma}(ZLD^{1/2})^T (y-X\beta-ZLD^{1/2}\tau) - \tau \\
&\propto e^{-2\gamma} D^{1/2}L^T Z^T(y - X\beta - ZLD^{1/2}\tau) - 1
\end{aligned}
$$

Gradient of $\xi_k$.  Here $J^{kk}$ is the singular entry matrix of $e^{\xi_k}$ at $kk$, with the remaining matrix elements zero.

$$
\begin{aligned}
\frac{\partial l}{\partial\xi_k} &\propto \frac{\partial}{\partial\xi_k}\left(-\frac{e^{-2\gamma}}{2}(y-X\beta-ZLD^{1/2}\tau)^T(y - X\beta-ZLD^{1/2}\tau) - \frac{\nu_{\lambda_k}+1}{2}\log\left(1 + \frac{1}{\nu_{\lambda_k}}\frac{e^{2\xi_k}}{A_{\lambda_k}^2} \right) +\xi_k\right) \\
&\propto e^{-2\gamma} \text{tr}\left((y - X\beta - ZLD^{1/2}\tau) (Z L J^{kk} \tau)^T\right) - (\nu_{\lambda_k}+1)\frac{1}{1 + \nu_{\lambda_k}A_{\lambda_k}^2 e^{-2\xi_k}}+1
\end{aligned}
$$


The gradient with respect to $a$:

Note that the following are equivalent

$$
\begin{aligned}
L D^{1/2}\tau = D^{1/2} \tau + \widetilde{T} a
\end{aligned}
$$

Therefore, the portion of the log likelihood dependent on $a$ becomes

$$
\begin{aligned}
l(a, ..) &= -\frac{e^{-2\gamma}}{2}(y-X\beta-ZLD^{1/2}\tau)^T(y - X\beta-ZLD^{1/2}\tau) \\
&= -\frac{e^{-2\gamma}}{2}(y - X\beta - Z(D^{1/2}\tau + \widetilde{T}a))^T(y - X\beta - Z(D^{1/2}\tau + \widetilde{T}a)) \\
&= -\frac{e^{-2\gamma}}{2}(y - X\beta - ZD^{1/2}\tau - Z\widetilde{T}a)^T (y - X\beta - ZD^{1/2}\tau - Z\widetilde{T}a) \\
\frac{\partial l}{\partial a} &= e^{-2\gamma}(Z\widetilde{T})^T (y - X\beta-ZD^{1/2}\tau - Z\widetilde{T}a) \\
&= e^{-2\gamma}\widetilde{T}^T Z^T(y - X\beta-ZD^{1/2}\tau - Z\widetilde{T}a)
\end{aligned}
$$


$$
\begin{aligned}
\frac{\partial l}{\partial a} &\propto \frac{\partial}{\partial a}\left(-\frac{e^{-2\gamma}}{2}(y-X\beta - \widetilde\tau - \widetilde{T}a)^T(y-X\beta - \widetilde\tau - \widetilde{T}a) -\frac{1}{2}a^T A^{-1} a \right) \\
&\propto e^{-2\gamma} \widetilde{T}^T(y-X\beta-\widetilde\tau - \widetilde{T}a) - A^{-1}a 
\end{aligned}
$$

Gradient of full log posterior

$$
\begin{aligned}
\log p(\beta, \gamma, \tau, \xi_1...\xi_q,a | y, X, Z) &\propto \log p(y | \beta, \tau, \gamma, \xi, a) + \log p(\beta) + \log p(\gamma)+ \log p(\tau) + \log p(\xi) + \log p(a) \\
&\propto  -n\gamma -\frac{e^{-2\gamma}}{2}(y - X\beta - ZLD^{1/2}\tau)^T (y - X\beta - ZLD^{1/2}\tau)  -\frac{1}{2}\beta^T\Sigma_\beta^{-1}\beta\\
&-\frac{\nu_\epsilon+1}{2}\log\left(1 + \frac{1}{\nu_\epsilon} \frac{e^{2\gamma}}{A_\epsilon^2} \right) + \gamma  -\frac{1}{2} a^T A^{-1} a -\frac{1}{2} \tau^T \tau\\
&-\sum_{k=1}^q \left(\frac{\nu_{\lambda_k}+1}{2}\log\left(1 + \frac{1}{\nu_{\lambda_k}} \frac{e^{2\xi_k}}{A_{\lambda_k}^2}\right)+ \xi_k\right)\\
\frac{\partial l}{\partial\beta} &= e^{-2\gamma}X^T(y-X\beta-ZLD^{1/2}\tau)-\Sigma_\beta^{-1}\beta \\
\frac{\partial l}{\partial\gamma} &= -(n-1)+e^{-2\gamma}(y-X\beta-ZLD^{1/2}\tau)^T(y-X\beta-ZLD^{1/2}\tau) \\
&-(\nu_\epsilon + 1) \frac{1}{1 + \nu_\epsilon A_\epsilon^2 e^{-2\gamma}} \\
\frac{\partial l}{\partial\tau} &=  e^{-2\gamma} D^{1/2}L^T Z^T(y - X\beta - ZLD^{1/2}\tau) - \tau\\
\frac{\partial l}{\partial\xi_k} &=  e^{-2\gamma}\text{tr}\left((y - X\beta - ZLD^{1/2}\tau) (Z L J^{kk} \tau)^T\right) - (\nu_{\lambda_k}+1)\frac{1}{1 + \nu_{\lambda_k}A_{\lambda_k}^2 e^{-2\xi_k}}+1  \\
\frac{\partial l}{\partial a} &= e^{-2\gamma}\widetilde{T}^T Z^T(y - X\beta-ZD^{1/2}\tau - Z\widetilde{T}a) - A^{-1}a 
\end{aligned}
$$

\pagebreak

# Hamiltonian Monte Carlo with unit diagonal

Initialize the random effects with high variance

```{r, echo=TRUE, eval=T}
N <- 2e3

theta.init <- c(0, 1, # beta 
               rep(0, 18), # tau
               3, # gamma (log sig2eps)
               1) # xi 

vnames <- c(paste0("beta", 0:1), 
            paste0("tau_int", 1:18), 
           "sigeps", "xi")

eps_vals <- c(5e-1, 5e-2,
              rep(3e-2, 18), 
              6e-3,
              5e-2)

set.seed(41132)
t1.hmc <- Sys.time()
 f_hmc <- hmc(N = N, theta.init = theta.init, 
            epsilon = eps_vals, L = 10, 
            logPOSTERIOR = lmm_posterior, 
            glogPOSTERIOR = g_lmm_posterior,  
            varnames = vnames, 
            param=list(y = y, X=X, Z=Z, m=18, q=1, 
                       nueps=4, Aeps=1, 
                       nulambda=1, Alambda=1, B=1e-5),
            parallel=TRUE, chains=2)
t2.hmc <- Sys.time()
t2.hmc - t1.hmc

f_hmc$accept/N
mcmc_trace(f_hmc, burnin=trunc(N*.3), pars=c("beta0", "beta1", "sigeps", "xi"))
```

Estimated precision matrix 

    beta0     beta1    u_int1  u_slope1    su_int2  u_slope2    u_int3  u_slope3    u_int4  u_slope4    u_int5  u_slope5 
    0.040     0.301     0.012     0.150     0.010     0.205     0.003     0.080     0.004     0.105     0.005     0.157 
   u_int6  u_slope6    u_int7  u_slope7    u_int8  u_slope8    u_int9  u_slope9   u_int10 u_slope10   u_int11 u_slope11 
    0.011     0.181     0.007     0.117     0.006     0.109     0.011     0.133     0.011     0.127     0.014     0.216 
  u_int12 u_slope12   u_int13 u_slope13   u_int14 u_slope14   u_int15 u_slope15   u_int16 u_slope16   u_int17 u_slope17 
    0.006     0.139     0.004     0.118     0.007     0.105     0.012     0.193     0.007     0.106     0.010     0.135 
  u_int18 u_slope18   sig2eps      xi_1      xi_2         a 
    0.005     0.113     0.00014     3.338     4.336   101.582 
    



```{r, echo=TRUE}
summary(f_hmc, burnin=trunc(N*.3))
```

```{r, echo=TRUE, fig.width=6, fig.height=4}
mcmc_trace(f_hmc, burnin=trunc(N*.3), pars=c("beta0", "beta1", "sigeps", "xi"))
```

Compare the Frequentist estimates to HMC

```{r, echo=TRUE, fig.width=6, fig.height=4}
beta.true <- fixef(fm1)
diagplots(f_hmc, burnin=trunc(N*.3), 
          actual.mu=beta.true, cols=1:2)
```
