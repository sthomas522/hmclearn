---
title: "hmclearn package:  Linear Mixed Effects Regression Example"
author:  "Samuel Thomas"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{linear_mixed_effects_hmclearn}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction

This vignette demonstrates fitting a Linear mixed effects regression model via Hamiltonian Monte Carlo (HMC) using the **hmclearn** package. 

$$
\begin{aligned}
y &= X\beta + Zu + \epsilon \\
u &\sim N(0, G) \\
\epsilon &\sim N(0, \sigma^2)
\end{aligned}
$$

```{r setup}
library(hmclearn)
```

# Linear Mixed Effects Model Example Data

The user must define provide the design matrix directly for use in **hmclearn**.  Our first step is to load the data and store the fixed effect design matrix $X$, random effects design matrix $Z$, and dependent variable vector $y$. 

We load drug sleepstudy data (Belenky et. al. 2003) and create the design matrices $X$ and $Z$ and dependent vector $y$.  For this model, the random effects design matrix $Z$ is specified for a random intercept model.  

```{r, echo=TRUE}
library(lme4)
library(Matrix)
data(sleepstudy)

# dependent variable
y <- sleepstudy$Reaction
n <- length(y)

yi.lst <- split(sleepstudy$Reaction, sleepstudy$Subject)

# fixed effects
ss2 <- sleepstudy
ss2$int <- 1
ss2 <- ss2[, c(4, 1:3)] # rearrange columns to store in list
Xi.lst <- split(ss2[, which(colnames(ss2) %in% c("Days", "int"))], 
                ss2$Subject)
Xi.lst <- lapply(Xi.lst, as.matrix)

X <- as.matrix(do.call(rbind, Xi.lst))

# random effects
m <- length(unique(sleepstudy$Subject))

##########
# intercept
Zi.lst <- replicate(m, matrix(rep(1, n/m), ncol=1), simplify=FALSE)
Z <- as.matrix(bdiag(Zi.lst))
```


To compare results, we first fit a linear mixed effects model using the frequentist package **lme4** (Bates et. al. 2015).  

```{r, echo=TRUE}
# random intercept 

## linear mixed models - reference values from older code
(fm1 <- lmer(Reaction ~ Days + (1 | Subject), sleepstudy, REML = FALSE))
summary(fm1)

```

# Parameterization for $G$

The parameterization approach for this model uses a strategy recommended by Betancourt, Girolami (2013) to facilitate more efficient sampling in HMC.  

Further, the uniform parameterization of the variance parameters is replaced by a half-t family of distributions per Gelman (2006), Prior distributions for Variance parameters in hierarchical models.  This parameterization is well-behaved around 0, in contrast to inverse gamma, and provides flexibility for more informed priors than a uniform distribution.

We select a parameterization of $G$ such that the likelihood and its gradient can be derived for HMC.  To this end, we uses LDL decomposition of $G$ to form a flexibile parameterization that can easily handle restrictions (Chan, Jelizkov 2009).  

$$
\begin{aligned}
u &\sim N(0, G)  \\
G &= L D L^T \\
&= L D^{1/2} D^{1/2} L^T \\
\end{aligned}
$$

Let $\lambda_k$ where $k = 1, ... p$ denote the diagonal entries of $D^{1/2}$ and let $a_{kj}$ where $1 \leq j < k \leq p$ denote free elements of lower unitrangular matrix $L$

$$
D^{1/2} := 
\begin{pmatrix}
\lambda_1 & 0 & ... & 0 \\
0 & \lambda_2 & 0 ... & 0 \\
... & ... & ... & ... \\
0 & 0 & ... & \lambda_p
\end{pmatrix}, 
L :=
\begin{pmatrix}
1 & 0 & 0 & ... & 0 \\
a_{21} & 1 & 0 & ... & 0 \\
a_{31} & a_{32} & 1 & ... & ... \\
... & ... & ... & ... & ... \\
a_{p1} & a_{p2} & ... & ... & 1 \\
\end{pmatrix}
$$

Also define $\lambda := (\lambda_1, ..., \lambda_p)^T$ and $a_k := (a_{k1}, ..., a_{k, k-1})^T$ and $a := (a_2^T, ..., a_p^T)^T$

Consider priors where $k = 1...p$.  The prior for $\lambda_k$ is half-t per Gelman (2006).  

$$
\begin{aligned}
p(\lambda_k) &\sim \left(1 + \frac{1}{\nu}\left(\frac{\lambda_k}{A} \right)^2 \right)^{-(\nu+1)/2} \\
a|\lambda &\sim N(a_0, A_0)
\end{aligned}
$$

The hyperparameter $a_0$ does not need to be zero, and $A_0$ can be correlated and may depend on $\lambda$.  In this model, we define $a$ independent of $\lambda$. 

Per Betancourt, Girolami (2013), we re-parameterize $u$ using a standard normal parameterization we define as $\tau = (\tau_1, ..., \tau_q)$.  Here, $u$ is a deterministic function of $G$ and $\tau$.

$$
\begin{aligned}
\tau &\sim N(0, I_q) \\
u &:= L D^{1/2} \tau \\
&\sim N(0, LD^{1/2} I (L D^{1/2})^T) \\
&\sim N(0, L D^{1/2} D^{1/2} L^T) \\
&\sim N(0, G)
\end{aligned}
$$

The distribution of $u$ therefore does not change with this parameterization.  The intent of our re-parameterization is to allow $G$ and $\tau$ to be largely independent in the MCMC sampling.   

# Derive the log posterior and gradient

First, we specify the likelihood and log-likelihood 

$$
\begin{aligned}
p(y | \beta, u_1, ..., u_c, \sigma_\epsilon^2) &\propto (\sigma_\epsilon^2)^{-n/2} e^{-\frac{1}{2\sigma_\epsilon^2}(y - X\beta - Zu)^T (y - X\beta - Zu)} \\
\log p(y | \beta, u_1, ..., u_c, \sigma_\epsilon^2) &\propto -{n}\log(\sigma_\epsilon) -\frac{1}{2\sigma_\epsilon^2}(y - X\beta - Zu)^T (y - X\beta - Zu)
\end{aligned}
$$

The posterior is defined for priors on $\beta$, $u$, $\sigma_\epsilon^2$, and $G$, with dependent vector $y$ and design matrices $X$ and $Z$.

$$
\begin{aligned}
p(\beta, u, \sigma_\epsilon^2, G | y, X, Z) &\propto p(y | \beta, u, \sigma_\epsilon^2, G)  p(\beta, u, \sigma_\epsilon^2, G) \\
&\propto p(y | \beta, u, \sigma_\epsilon^2, G) p(\beta) p(\sigma_\epsilon^2) p(u, G) \\
&\propto p(y | \beta, u, \sigma_\epsilon^2, G) p(\beta) p(\sigma_\epsilon^2) p(u | G) p(G) \\
\log p(\beta, u, \sigma_\epsilon^2, G | y, X, Z) &\propto \log p(y | \beta, u, \sigma_\epsilon^2, G) + \log p(\beta) + \log p(\sigma_\epsilon^2)+ \log p(u|G) + \log p(G) \\
\log p(\beta, \sigma_\epsilon, \tau, \lambda_1...\lambda_q,a | y, X, Z) &\propto \log p(y | \beta, \tau, \sigma_\epsilon, \lambda, a) + \log p(\beta) + \log p(\sigma_\epsilon)+ \log p(\tau) + \log p(\lambda) + \log p(a)
\end{aligned}
$$

We parameterize all standard deviations from the half-t family.  

$$
\begin{aligned}
p(\beta) &\propto N(0, BI) \\
p(\sigma_\epsilon) &\sim  \left(1 + \frac{1}{\nu_\epsilon}\left(\frac{\sigma_\epsilon}{A_\epsilon} \right)^2 \right)^{-(\nu_\epsilon+1)/2}  \\
p(\lambda_k) &\sim  \left(1 + \frac{1}{\nu_{\lambda_k}}\left(\frac{\lambda_k}{A_{\lambda_k}} \right)^2 \right)^{-(\nu_{\lambda_k}+1)/2}  \\
a &\sim N(0, A_a)
\end{aligned}
$$

We want proposals of $\sigma_\epsilon^2$ over the real number line.  Therfore we derive the distribution of the transformed parameter $\gamma$ based on a change of variable

$$
\begin{aligned}
\gamma &:= \log \sigma_\epsilon \\
e^\gamma &= \sigma_\epsilon 
\end{aligned}
$$


We need to compute the Jacobian of the transformation

$$
\begin{aligned}
p_{\gamma}(\gamma) &= p_{\sigma_\epsilon}(g^{-1}(\gamma))\lvert \frac{d\sigma_\epsilon}{d\gamma} \rvert \\
&= \left(1 + \frac{1}{\nu_\epsilon} \left(\frac{e^\gamma}{A_\epsilon} \right)^2 \right)^{-\frac{\nu_\epsilon +1}{2}} e^{\gamma} \\
&= \left(1 + \frac{1}{\nu_\epsilon} \frac{e^{2\gamma}}{A_\epsilon^2} \right)^{-\frac{\nu_\epsilon +1}{2}} e^{\gamma} \\
\log p_{\gamma}(\gamma) &\propto -\frac{\nu_\epsilon+1}{2}\log\left(1 + \frac{1}{\nu_\epsilon} \frac{e^{2\gamma}}{A_\epsilon^2} \right) + \gamma
\end{aligned}
$$


We have hyperpriors $\lambda$ and $a$ for $G$. 

Re-parameterize $\xi_k = \log \lambda_k$ and $e^{\xi_k} = \lambda_k$

Recall that we parameterized $\lambda_k$ as uniform

$$
\begin{aligned}
\lambda_k &\sim \text{half-t}(\nu_{\lambda_k}, A_{\lambda_k}) \\
p_{\xi_k}(\xi_k) &= p_{\lambda_k}(g^{-1}(\xi_k)) \left\lvert \frac{d\lambda_k}{d\xi_k}  \right\rvert \\
&= p_{\lambda_k} (e^{\xi_k})\lvert e^{\xi_k}\rvert \\
&= \left(1 + \frac{1}{\nu_{\lambda_k}} \frac{e^{2\xi_k}}{A_{\lambda_k}^2}\right)^{-\frac{\nu_{\lambda_k}+1}{2}}e^{\xi_k} \\
\log p(\xi_k) &\propto -\frac{\nu_{\lambda_k}+1}{2}\log\left(1 + \frac{1}{\nu_{\lambda_k}} \frac{e^{2\xi_k}}{A_{\lambda_k}^2}\right)+ \xi_k
\end{aligned}
$$

Leveraging an idea from Chan, Jeliazkov (2009) MCMC estimation of restricted covariance matrices

We write the matrix form of $u = L D^{1/2} \tau$ in a form with a parameter vector $a$ by which the gradient can be computed directly

$$
\begin{aligned}
u &= L D^{1/2} \tau \\
&= \widetilde{\tau} + \widetilde{T}a \\
&=\begin{pmatrix}
e^\xi_1\tau_1 \\
e^\xi_2\tau_2 \\
e^\xi_3\tau_3 \\
... \\
... \\
e^\xi_q\tau_q
\end{pmatrix} + 
\begin{pmatrix}
0 & ... & & & & & & ... & 0 \\
e^{\xi_1}\tau_1 & 0 & ... & & & & & & ... \\
0 & e^{\xi_1}\tau_1 & e^{\xi_2}\tau_2 & 0 & ... & & & & 0 \\
0 & ... & 0 & e^{\xi_1}\tau_1 & e^{\xi_2}\tau_2 & e^{\xi_3}\tau_3 & 0 & ... & ... \\
... & ... & ... & ... & ... & ... & ... & ... & ... \\
0 & ... & ... & 0 & ... & ... & e^{\xi_1}\tau_1 & ... & e^{\xi_q}\tau_q \\
\end{pmatrix}
\begin{pmatrix}
a_{21} \\
a_{31} \\
a_{32} \\
... \\
... \\
a_{q,q-1}
\end{pmatrix}
\end{aligned}
$$

Since we decompose $G$ using $L D L^T$.  We make use of the determinant property of a square matrix, where the determinant of a square matrix is equal to the determinant of its transpose.  Also, from above $\lvert L \rvert = 1$

$$
\begin{aligned}
\lvert G \rvert &= \lvert L D L^T \rvert \\
&= \lvert L \rvert \lvert D \rvert \lvert L^T \rvert \\
&= \lvert D \rvert \\
&= \prod_{k=1}^q e^{2\xi_k} \\
&= e^{2\sum_{k=1}^q \xi_k} \\
\log \lvert G \rvert &= 2\sum_{k=1}^q \xi_k
\end{aligned}
$$

Assign relatively uniformative prior for $a$

$$
\begin{aligned}
a_k &\sim N(0, A) \\
p(a_k) &\propto \lvert A  \rvert^{-1/2} e^{-\frac{1}{2} a_k^T A^{-1} a_k} \\
\log p(a_k) &\propto -\frac{1}{2}a_k^T A^{-1} a_k
\end{aligned}
$$

Recall that the prior for $\tau$ is standard Normal (multivariate)

$$
\begin{aligned}
\tau &\sim N(0, I) \\
p(\tau) &\sim e^{-\frac{1}{2}\tau^T I \tau} \\
\log p(\tau) &\sim -\frac{1}{2} \tau^T \tau
\end{aligned}
$$


Finally, we write the full log posterior

$$
\begin{aligned}
\log p(\beta, \sigma_\epsilon, \tau, \lambda_1...\lambda_q,a | y, X, Z) &\propto \log p(y | \beta, \tau, \sigma_\epsilon, \lambda, a) + \log p(\beta) + \log p(\sigma_\epsilon)+ \log p(\tau) + \log p(\lambda) + \log p(a) \\
\log p(\beta, \gamma, \tau, \xi_1...\xi_q,a | y, X, Z) &\propto \log p(y | \beta, \tau, \gamma, \xi, a) + \log p(\beta) + \log p(\gamma)+ \log p(\tau) + \log p(\xi) + \log p(a) \\
&\propto  -n\gamma -\frac{e^{-2\gamma}}{2}(y - X\beta - Zu)^T (y - X\beta - Zu)  - \frac{1}{2}\beta^T\Sigma_\beta^{-1}\beta \\
&-\frac{\nu_\epsilon+1}{2}\log\left(1 + \frac{1}{\nu_\epsilon} \frac{e^{2\gamma}}{A_\epsilon^2} \right) + \gamma  -\frac{1}{2} a^T A^{-1} a -\frac{1}{2} \tau^T \tau\\
&-\sum_{k=1}^q \left(\frac{\nu_{\lambda_k}+1}{2}\log\left(1 + \frac{1}{\nu_{\lambda_k}} \frac{e^{2\xi_k}}{A_{\lambda_k}^2}\right)+ \xi_k\right)\\
&\propto  -n\gamma -\frac{e^{-2\gamma}}{2}(y - X\beta - ZL D^{1/2}\tau)^T (y - X\beta - ZL D^{1/2}\tau) - \frac{1}{2}\beta^T\Sigma_\beta^{-1}\beta \\
&-\frac{\nu_\epsilon+1}{2}\log\left(1 + \frac{1}{\nu_\epsilon} \frac{e^{2\gamma}}{A_\epsilon^2} \right) + \gamma  -\frac{1}{2} a^T A^{-1} a -\frac{1}{2} \tau^T \tau\\
&-\sum_{k=1}^q \left(\frac{\nu_{\lambda_k}+1}{2}\log\left(1 + \frac{1}{\nu_{\lambda_k}} \frac{e^{2\xi_k}}{A_{\lambda_k}^2}\right)+ \xi_k\right)\\
\end{aligned}
$$

## Derive the gradient of the log posterior

Derive the partial derivative of $\tau$ 

$$
\begin{aligned}
\frac{\partial l}{\partial\tau} &\propto \frac{\partial}{\partial\tau}\left(-\frac{e^{-2\gamma}}{2}(y-X\beta-ZLD^{1/2}\tau)^T(y-X\beta-ZLD^{1/2}\tau) - \frac{1}{2}\tau^T\tau\right) \\
&\propto e^{-2\gamma}(ZLD^{1/2})^T (y-X\beta-ZLD^{1/2}\tau) - \tau \\
&\propto e^{-2\gamma} D^{1/2}L^T Z^T(y - X\beta - ZLD^{1/2}\tau) - 1
\end{aligned}
$$

Derive the partial derivative of $\xi_k$.  Here $J^{kk}$ is the singular entry matrix of $e^{\xi_k}$ at $kk$, with the remaining matrix elements zero.

$$
\begin{aligned}
\frac{\partial l}{\partial\xi_k} &\propto \frac{\partial}{\partial\xi_k}\left(-\frac{e^{-2\gamma}}{2}(y-X\beta-ZLD^{1/2}\tau)^T(y - X\beta-ZLD^{1/2}\tau) - \frac{\nu_{\lambda_k}+1}{2}\log\left(1 + \frac{1}{\nu_{\lambda_k}}\frac{e^{2\xi_k}}{A_{\lambda_k}^2} \right) +\xi_k\right) \\
&\propto e^{-2\gamma} \text{tr}\left((y - X\beta - ZLD^{1/2}\tau) (Z L J^{kk} \tau)^T\right) - (\nu_{\lambda_k}+1)\frac{1}{1 + \nu_{\lambda_k}A_{\lambda_k}^2 e^{-2\xi_k}}+1
\end{aligned}
$$


Derive the partial derivative with respect to $a$.

Note that the following are equivalent

$$
\begin{aligned}
L D^{1/2}\tau = D^{1/2} \tau + \widetilde{T} a
\end{aligned}
$$

Therefore, the portion of the log likelihood dependent on $a$ becomes


$$
\begin{aligned}
\frac{\partial l}{\partial a} &\propto \frac{\partial}{\partial a}\left(-\frac{e^{-2\gamma}}{2}(y-X\beta - \widetilde\tau - \widetilde{T}a)^T(y-X\beta - \widetilde\tau - \widetilde{T}a) -\frac{1}{2}a^T A^{-1} a \right) \\
&\propto e^{-2\gamma} \widetilde{T}^T(y-X\beta-\widetilde\tau - \widetilde{T}a) - A^{-1}a 
\end{aligned}
$$

The gradient of full log posterior can now be specified

$$
\begin{aligned}
\log p(\beta, \gamma, \tau, \xi_1...\xi_q,a | y, X, Z) &\propto \log p(y | \beta, \tau, \gamma, \xi, a) + \log p(\beta) + \log p(\gamma)+ \log p(\tau) + \log p(\xi) + \log p(a) \\
&\propto  -n\gamma -\frac{e^{-2\gamma}}{2}(y - X\beta - ZLD^{1/2}\tau)^T (y - X\beta - ZLD^{1/2}\tau)  -\frac{1}{2}\beta^T\Sigma_\beta^{-1}\beta\\
&-\frac{\nu_\epsilon+1}{2}\log\left(1 + \frac{1}{\nu_\epsilon} \frac{e^{2\gamma}}{A_\epsilon^2} \right) + \gamma  -\frac{1}{2} a^T A^{-1} a -\frac{1}{2} \tau^T \tau\\
&-\sum_{k=1}^q \left(\frac{\nu_{\lambda_k}+1}{2}\log\left(1 + \frac{1}{\nu_{\lambda_k}} \frac{e^{2\xi_k}}{A_{\lambda_k}^2}\right)+ \xi_k\right)\\
\frac{\partial l}{\partial\beta} &= e^{-2\gamma}X^T(y-X\beta-ZLD^{1/2}\tau)-\Sigma_\beta^{-1}\beta \\
\frac{\partial l}{\partial\gamma} &= -(n-1)+e^{-2\gamma}(y-X\beta-ZLD^{1/2}\tau)^T(y-X\beta-ZLD^{1/2}\tau) \\
&-(\nu_\epsilon + 1) \frac{1}{1 + \nu_\epsilon A_\epsilon^2 e^{-2\gamma}} \\
\frac{\partial l}{\partial\tau} &=  e^{-2\gamma} D^{1/2}L^T Z^T(y - X\beta - ZLD^{1/2}\tau) - \tau\\
\frac{\partial l}{\partial\xi_k} &=  e^{-2\gamma}\text{tr}\left((y - X\beta - ZLD^{1/2}\tau) (Z L J^{kk} \tau)^T\right) - (\nu_{\lambda_k}+1)\frac{1}{1 + \nu_{\lambda_k}A_{\lambda_k}^2 e^{-2\xi_k}}+1  \\
\frac{\partial l}{\partial a} &= e^{-2\gamma}\widetilde{T}^T Z^T(y - X\beta-ZD^{1/2}\tau - Z\widetilde{T}a) - A^{-1}a 
\end{aligned}
$$

# Hamiltonian Monte Carlo

Run HMC for linear mixed effects regression model


```{r, echo=TRUE, eval=T}
N <- 2e3

theta.init <- c(0, 1, # beta 
               rep(0, 18), # tau
               3, # gamma (log sig2eps)
               1) # xi 

vnames <- c(paste0("beta", 0:1), 
            paste0("tau_int", 1:18), 
           "sigeps", "xi")

eps_vals <- c(5e-1, 5e-2,
              rep(3e-2, 18), 
              6e-3,
              5e-2)

set.seed(41132)
t1.hmc <- Sys.time()
 f_hmc <- hmc(N = N, theta.init = theta.init, 
            epsilon = eps_vals, L = 10, 
            logPOSTERIOR = lmm_posterior, 
            glogPOSTERIOR = g_lmm_posterior,  
            varnames = vnames, 
            param=list(y = y, X=X, Z=Z, m=18, q=1, 
                       nueps=4, Aeps=1, 
                       nulambda=1, Alambda=1, sig2beta=1e5),
            parallel=TRUE, chains=2)
t2.hmc <- Sys.time()
t2.hmc - t1.hmc
```

The acceptance ratio for each of the HMC chains is sufficiently high for an efficient simulation.  

```{r, echo=TRUE}
f_hmc$accept/N
```


Trace plots provide a visual indication of stationarity.  These plots indicate that the MCMC chains are reasonably stationary.  

```{r, echo=TRUE, fig.width=6, fig.height=4}
mcmc_trace(f_hmc, burnin=trunc(N*.3), pars=c("beta0", "beta1", "sigeps", "xi"))
```

The posterior quantiles are summarized after removing an initial *burnin* period.  

```{r, echo=TRUE}
summary(f_hmc, burnin=trunc(N*.3))
```

Histograms of the posterior distribution show that Bayesian parameter estimates align with frequentist estimates.  The *cols* parameter specifies the parameters to be displayed in *diagplots*, based on the order provided to the *hmc* function. 

```{r, echo=TRUE, fig.width=6, fig.height=4}
beta.freq <- fixef(fm1)
xi.freq <- 1/2*log(VarCorr(fm1)$Subject[1])
sigeps.freq <- log(sigma(fm1))
theta.freq <- c(beta.freq, sigeps.freq, xi.freq)
diagplots(f_hmc, burnin=trunc(N*.3), 
          actual.mu=theta.freq, cols=c(1:2, 21:22))
```

# Source

Gregory Belenky, Nancy J. Wesensten, David R. Thorne, Maria L. Thomas, Helen C. Sing, Daniel P. Redmond, Michael B. Russo and Thomas J. Balkin (2003) Patterns of performance degradation and restoration during sleep restriction and subsequent recovery: a sleep dose-response study. *Journal of Sleep Research* 12, 1â€“12.

# References

Bates, D., M\"{a}chler, M., Bolker, B., & Walker, S. (2015). Fitting linear mixed-effects models using lme4. *Journal of Statistical Software* 67(1)

Betancourt, M., & Girolami, M. (2015). Hamiltonian Monte Carlo for hierarchical models. *Current trends in Bayesian methodology with applications*, 79(30), 2-4.

Gelman, Andrew. (2006) "Prior distributions for variance parameters in hierarchical models (comment on article by Browne and Draper)." *Bayesian analysis* 1.3: 515-534.

Chan, J. C. C., & Jeliazkov, I. (2009). MCMC estimation of restricted covariance matrices. *Journal of Computational and Graphical Statistics*, 18(2), 457-480.

Agresti, A. (2015). *Foundations of linear and generalized linear models*. John Wiley & Sons.  ISBN: 978-1-118-73003-4
