---
title: "hmclearn:  Poisson Regression Example"
author:  "Samuel Thomas"
date: "``r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{hmclearn_poisson_regression}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(hmclearn)
```

# GLM - Poisson Regression

Poisson distribution

$$
p(y; \mu) = \frac{e^{-\mu}\mu^y}{y!}
$$

Log link function

$$
\begin{aligned}
\mu &:= E(Y | x) = e^{X\beta} \\
\log \mu &= X\beta
\end{aligned}
$$

Develop the likelihood

$$
\begin{aligned}
L(\mu; y) &= \prod_{i=1}^n \frac{e^{-\mu_i}\mu_i^{y_i}}{y_i!} \\
L(\beta; y, X) &= \prod_{i=1}^n \frac{e^{-e^{X_i\beta}}e^{y_iX_i\beta}}{y_i!} \\
\end{aligned}
$$

with log-likelihood

$$
\begin{aligned}
l(\beta; y, X) &= \sum_{i=1}^n -e^{X_i\beta} + y_i X_i \beta - \log y_i! \\
&\propto \sum_{i=1}^n -e^{X_i\beta} + y_i X_i \beta
\end{aligned}
$$

We set a multivariate Normal prior for $\beta$

$$
\begin{aligned}
\beta &\sim N(0, \Sigma_\beta) \\
&\sim N(0, BI)
\end{aligned}
$$


With pdf

$$
\begin{aligned}
p(\beta) &= \frac{1}{\sqrt{\lvert 2\pi \Sigma_\beta \rvert }}e^{-\frac{1}{2}\beta^T \Sigma_\beta^{-1}\beta} \\
\log p(\beta) &= -\frac{1}{2}\log(2\pi \lvert \Sigma_\beta \rvert) - \frac{1}{2}\beta^T \Sigma_\beta^{-1} \beta \\
&\propto -\frac{1}{2}\log \lvert\Sigma_\beta\rvert - \frac{1}{2}\beta^T \Sigma_\beta^{-1} \beta
\end{aligned}
$$

Let $B = 1e3$ for instance, as a relatively uniformative prior.

Now derive the log posterior

$$
\begin{aligned}
p(\beta | X, y) &\propto p(y | X, \beta)  p(\beta) \\
\log p(\beta | X, y) & \propto \log p(y | X, \beta) + \log p(\beta) \\
&\propto \sum_{i=1}^n \left( -e^{X_i\beta} + y_i X_i \beta\right) -\frac{1}{2}\log \lvert\Sigma_\beta\rvert - \frac{1}{2}\beta^T \Sigma_\beta^{-1} \beta \\
&\propto \sum_{i=1}^n \left( -e^{X_i\beta} + y_i X_i \beta\right) - \frac{1}{2}\beta^T \Sigma_\beta^{-1} \beta
\end{aligned}
$$

The log posterior is the sum of the log likelihood and the log of the prior for $\beta$.  

We need to derive the gradient of the log posterior for the leapfrog function

$$
\begin{aligned}
\log p(\beta | X, y) & \propto \log p(y | X, \beta) + \log p(\beta) \\
&\propto \sum_{i=1}^n \left( -e^{X_i\beta} + y_i X_i \beta\right) - \frac{1}{2}\beta^T \Sigma_\beta^{-1} \beta \\
\frac{\partial}{\partial \beta}\log p(\beta|X, y) &\propto \sum_{i=1}^n\left( -e^{X_i\beta}X_i + y_iX_i\right) - \Sigma_\beta^{-1} \beta
\end{aligned}
$$

# Data

Load Drugs data (Agresti - Foundations of Linear and Generalized linear models p. 244)

```{r, echo=TRUE}
data(Drugs)

# design matrix
X <- model.matrix(count ~ A + C + M + A:C + A:M + C:M , data=Drugs)
X <- X[, 1:ncol(X)]

# independent variable is count data
y <- Drugs$count


```


Fit a Frequentist glm

```{r, echo=TRUE}
# matrix representation
f <- glm(y ~ X-1, family=poisson(link=log))
summary(f)

```

# HMC poisson regression

Run HMC for poisson regression model

```{r, echo=TRUE}
N <- 100000

set.seed(412)

t1.hmc <- Sys.time()
 res <- hmc(N = N, theta.init = rep(0, 7), 
                    epsilon = 1e-4, L = 30, 
                    logPOSTERIOR = poisson_posterior,
                    glogPOSTERIOR = g_poisson_posterior, 
                    varnames = colnames(X), 
                    param=list(y = y, X=X)) 
t2.hmc <- Sys.time()
t2.hmc - t1.hmc
res$accept/N
```

```{r, echo=TRUE}
summary(res, burnin=1e4)
```

```{r, echo=TRUE, fig.width=6, fig.height=4}
mcmc_trace(res, burnin=1e4)
```

Compare the Frequentist estimates to HMC

```{r, echo=TRUE, fig.width=6, fig.height=4}
beta.true <- coef(f)
diagplots(res, burnin=1e4, actual.mu=beta.true)
```


