---
title: "HMC in R Appendix"
output: pdf_document
vignette: >
  %\VignetteIndexEntry{HMC_appendix}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
header-includes:
  - \usepackage{setspace}
  - \doublespacing
  - \usepackage{amsmath}
  - \usepackage{algorithm}
  - \usepackage{algpseudocode}
  - \usepackage{enumitem}
  - \usepackage{bm}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(hmclearn)
library(MASS)
```


# Appendix:  HMC R package


## hmc function

First we describe the main \textit{hmc} function itself.  

```{r, echo=TRUE, eval=FALSE}
# Main hmc function
hmc <- function(N, theta.init, epsilon, L, logPOSTERIOR, glogPOSTERIOR, y, X, Z=NULL,
                randlength=FALSE, Mdiag=NULL, verbose=FALSE, ...) {

  p <- length(theta.init) # number of parameters

  # mass matrix
  mu.p <- rep(0, p)

  # epsilon values
  eps_orig <- epsilon
  if (length(epsilon) == 1) {
    eps_orig <- rep(epsilon, p)
  }

  # epsilon matrix
  eps_vals <- matrix(rep(eps_orig, N), ncol=N, byrow=F)

  # number of steps
  nstep_vals <- rep(L, N)

  # randomize epsilon and L
  if (randlength) {
    randvals <- replicate(N, runif(p, -0.1*eps_orig, 0.1*eps_orig), simplify=T)
    eps_vals <- eps_vals + randvals
    nstep_vals <- round(runif(N, 0.5*L, 2.0*L))
  }
  
  # default to unit diagonal matrix
  if (is.null(Mdiag)) {
    M_mx <- diag(p)
  }

  # invert covariance M for leapfrog
  Minv <- qr.solve(M_mx)

  # store theta and momentum (usually not of interest)
  theta <- list()
  theta[[1]] <- theta.init
  r <- list()
  r[[1]] <- NA
  accept <- 0
  for (jj in 2:N) {
    theta[[jj]] <- theta.new <- theta[[jj-1]]
    r0 <- MASS::mvrnorm(1, mu.p, M_mx)
    r.new <- r[[jj]] <- r0
    for (i in 1:L_vals) {
      lstp <- i == L_vals
      lf <- leapfrog(theta_lf = theta.new, r = r.new, epsilon = epsilon, 
                     logPOSTERIOR = logPOSTERIOR,
                     glogPOSTERIOR = glogPOSTERIOR, y = y, X = X, Z = Z,
                     Minv=Minv, constrain=constrain, lastSTEP=lstp, ...)

      theta.new <- lf$theta.new
      r.new <- lf$r.new
    }

    if (verbose) print(jj)

    # standard metropolis-hastings update
    u <- runif(1)

    # use log transform for ratio due to low numbers
    num <- logDENS(theta.new, y=y, X=X, Z=Z, ...) - 0.5*(r.new %*% r.new)
    den <- logDENS(theta[[jj-1]], y=y, X=X, Z=Z, ...) - 0.5*(r0 %*% r0)

    log.alpha <- pmin(0, num - den)

    if (log(u) < log.alpha) {
      theta[[jj]] <- theta.new
      r[[jj]] <- -r.new
      accept <- accept + 1
    } else {
      theta[[jj]] <- theta[[jj-1]]
      r[[jj]] <- r[[jj-1]]
    }

  }
  list(theta=theta,
       r=r,
       accept=accept,
       M=M_mx)
}


```

The arguments of \textit{hmc} are as follows: 

\begin{itemize}
\item N:  Number of MCMC simulations
\item theta.init:  initial values $\Theta^{(0)}$ in vector of length $k$
\item epsilon:  step size $\epsilon$.  Can be a single value or vector length $k$
\item L:  number of leapfrog steps per proposal
\item logPOSTERIOR:  name of function to return the log posterior
\item glogPOSTERIOR:  name of function to return the gradient of the log posterior
\item y:  vector of the dependent variable
\item X:  design matrix of independent variables
\item randlength:  logical to choose whether to randomly vary number of leapfrog steps $L$
\item Mdiag:  optional vector for the diagonal matrix $M$
\item verbose:  logical to print step in the MCMC chain
\item ...:  additional parameters for logPOSTERIOR and glogPOSTERIOR
\end{itemize}

Next, we review the functionality of the \textit{hmc} line-by-line.  

HMC assigns a single latent parameter $p_i$ for each parameter $\theta_i \; \forall i \in 1:k$.  Therefore, \textit{p} is assigned the number of parameters in $\Theta$.   

```{r, echo=TRUE, eval=FALSE}
  p <- length(theta.init)
```

The distribution of the latent variables $\pmb{p}$ is multivariate Normal with mean 0.  The mean of $\pmb{p}$ is assigned a vector 0 for each $p_i$.    

```{r, echo=TRUE, eval=FALSE}
  mu.p <- rep(0, p)
```

In this implementation, $\epsilon$ may be specified as a single number or a vector.  If $\epsilon$ is a single number, it is converted to a vector with a length of the number of parameters.  


```{r, echo=TRUE, eval=FALSE}
  # epsilon values
  eps_orig <- epsilon
  if (length(epsilon) == 1) {
    eps_orig <- rep(epsilon, p)
  }

  # epsilon matrix
  eps_vals <- matrix(rep(eps_orig, N), ncol=N, byrow=F)
```

The number of steps $L$ can also be randomized.  In preparation for this random assignment, the step sizes for each iteration $1...N$ are stored in a vector \textit{L\_vals}.

```{r, echo=TRUE, eval=FALSE}
  # number of steps
  L_vals <- rep(L, N)

```

The \textit{randlength} is an optional parameter to randomize $\epsilon$ and $L$.  If this is set to \textit{TRUE}, then a random uniform distribution is used to add some randomness to $\epsilon$.  The number of steps $L$ is similarly randomized, but restricted to integer values.  

```{r, echo=TRUE, eval=FALSE}
  # randomize epsilon and L
  if (randlength) {
    randvals <- replicate(N, runif(p, -0.1*eps_orig, 0.1*eps_orig), simplify=T)
    eps_vals <- eps_vals + randvals
    L_vals <- round(runif(N, 0.5*L, 2.0*L))
  }
```

If the Mass matrix is not provided, the identity matrix is used as a default. 

```{r, echo=TRUE, eval=FALSE}
  # default to unit diagonal matrix
  if (is.null(Mdiag)) {
    M_mx <- diag(p)
  }
```


In preparation for the leapfrog algorithm, the Mass matrix is inverted.

```{r, echo=TRUE, eval=FALSE}
  # invert covariance M for leapfrog
  Minv <- qr.solve(M_mx)
```

Recall that our objective is to simulate values of $\Theta$ from the posterior.  In this function, the simulated values will be stored in a list \textit{theta}.  The first element of the list \textit{theta[[1]]} is assigned to the initial value provided in the function.  

```{r, echo=TRUE, eval=FALSE}
  theta <- list()
  theta[[1]] <- theta.init
```

A second list is created to store simulated values of the momentum $\pmb{p}$.  These simulated values are typically not of interest in analysis, but can be useful in debugging.  The first value of $\pmb{p}$ is assigned \textit{NA} for the initial value of $\Theta$ only.

```{r, echo=TRUE, eval=FALSE}
  r <- list()
  r[[1]] <- NA
```

The acceptance rate of HMC (and MH) is an important measure in assessing the efficiency of the MCMC simulation.  An internal variable \textit{accept} is initialized to zero and is used to count the number of accepted proposals.  

```{r, echo=TRUE, eval=FALSE}
  accept <- 0
```

With the initialization complete, we begin the loop to perform the HMC simulations.  The loop starts at 2 since 1 is reserved for the initial value $\Theta^{(0)}$. 

```{r, echo=TRUE, eval=FALSE}
  for (jj in 2:N) {
```

The default proposed value of $\Theta^{(t)}$ is the previous value in the chain, $\Theta^{(t-1)}$.  This value may be replaced with the proposal depending on the acceptance ratio calculation later in the loop.  

```{r, echo=TRUE, eval=FALSE}
    theta[[jj]] <- theta.new <- theta[[jj-1]]
```

Each proposal requires a simulated value of $\pmb{p}$.  The standard \textit{mapply} function passes the mean vector \textit{mu.p} and standard deviation vector \textit{Mdiag} to the \textit{rnorm} function.  The additional argument \textit{n = 1} is passed to \textit{rnorm} to simulate one value of $p_i$ for each combination of mean and standard deviation in the corresponding vectors.  

The \textit{mvrnorm} function in the \textit{MASS} package is used to simulate a multivariate Normal with mean \textit{mu.p} and covariance matrix \textit{M\_mx}

```{r, echo=TRUE, eval=FALSE}
  r0 <- MASS::mvrnorm(1, mu.p, M_mx)
```

The initial sampled value of $p_t$ is stored in the momentum list \textit{r[[m]]}.  This value will be replaced if the proposal is accepted later in the loop.

```{r, echo=TRUE, eval=FALSE}
    r.new <- r[[jj]] <- r0
```

With the previous value $\Theta^{(t-1)}$ and simulated momentum, the leapfrog can be evaluted $L$ consecutive times.  The proposal will be the joint value of $(\pmb{\tilde{p}}^{t}, \tilde\Theta^{(t)})$ after progressing through the $L$ leapfrog steps.  

The simulated values for each leapfrog step are stored in \textit{theta.new} and \textit{r.new}.  After $L$ steps, these variables store the proposal. 

```{r, echo=TRUE, eval=FALSE}
    for (i in 1:L) {
      lstp <- i == Nstep
      lf <- leapfrog(theta_lf = theta.new, r = r.new, epsilon = epsilon, 
                     logPOSTERIOR = logPOSTERIOR,
                     glogPOSTERIOR = glogPOSTERIOR, y = y, X = X, Z = Z,
                     Minv=Minv, constrain=constrain, lastSTEP=lstp, ...)

      theta.new <- lf$theta.new
      r.new <- lf$r.new
    }
```

Next is an optional counter to print the count in the \textit{for} loop.

```{r, echo=TRUE, eval=FALSE}
    if (verbose) print(jj)
```

The proposal is then evaluated in a Metropolis-Hastings style acceptance step.  First, a standard uniform random value is simulated and stored in \textit{u}.  The proposal will be accepted if the acceptance ratio is less than this value.

```{r, echo=TRUE, eval=FALSE}
    u <- runif(1)
```

A log transformation is used to calculate the acceptance ratio.  This transformation is used for numerical stability, since the simulated values of the log posterior can be very high. The log ratio is set to a maximum of 0 since $\log(1) = 0$ is the maximum value in the range $u \in [0, 1]$.  

```{r, echo=TRUE, eval=FALSE}
    num <- logPOSTERIOR(theta.new, y=y, X=X, Z=Z, ...) - 0.5*(r.new %*% r.new)
    den <- logPOSTERIOR(theta[[jj-1]], y=y, X=X, Z=Z, ...) - 0.5*(r0 %*% r0)

    log.alpha <- pmin(0, num - den)
```

Finally, a check is performed on the acceptance ratio.  If $\alpha < u$ (or $\log\alpha < \log u$), then the proposal is accepted.  Otherwise, the previous value in the MCMC chain is retained.

If the proposal is accepted, the negative value of momentum is stored in \textit{r[[m]]}.  This is a technicality to assure that the MCMC chain follows detailed balance (cite here), but this value is not used for inference.  Also, the \textit{accept} variable is incremented by one when the proposal is accepted.  

```{r, echo=TRUE, eval=FALSE}
    if (log(u) < log.alpha) {
      theta[[jj]] <- theta.new
      r[[jj]] <- -r.new
      accept <- accept + 1
    } else {
      theta[[jj]] <- theta[[jj-1]]
      r[[jj]] <- r[[jj-1]]
    }
```

After \textit{N} simulations, a list of results is exported from \textit{hmc}:  

\begin{enumerate}
\item theta:  list of simulated values of $\Theta$
\item r:  list of simulated values of the momentum $\pmb{p}$
\item accept:  the number of accepted proposals in \textit{N} simulations
\item M\_mx:  Mass matrix
\end{enumerate}

```{r, echo=TRUE, eval=FALSE}
  list(theta=theta,
       r=r,
       accept=accept,
       M=M_mx)
```

## Leapfrog function

Next, we review the leapfrog function in more detail. 

```{r, echo=TRUE, eval=FALSE}
leapfrog <- function(theta_lf, r, epsilon, logPOSTERIOR, glogPOSTERIOR, y, X, Z, 
                     Minv, constrain, lastSTEP=FALSE, ...) {
  
  # gradient of log posterior for old theta
  g.ld <- glogPOSTERIOR(theta_lf, y=y, X=X, Z=Z, ...)
  
  # first momentum update
  r.new <- r + epsilon/2*g.ld
  
  # theta update
  # note diagonal matrix update
  # theta.new <- theta_lf + as.numeric(epsilon*r.new/ diag(M_mx))
  theta.new <- theta_lf + as.numeric(epsilon* Minv %*% r.new)
  
  # check positive
  switch_sign <- constrain & theta.new < 0
  r.new[switch_sign] <- -r.new[switch_sign]
  theta.new[switch_sign] <- -theta.new[switch_sign]
  
  # gradient of log posterior for new theta
  g.ld.new <- glogPOSTERIOR(theta.new, y=y, X=X, Z=Z, ...)
  
  # if not on last step, second momentum update
  if (!lastSTEP) {
    r.new <- r.new + epsilon/2*g.ld.new
  }
  list(theta.new=theta.new, 
       r.new=as.numeric(r.new))
}

```

The arguments of \textit{leapfrog} are as follows: 

\begin{itemize}
\item theta\_lf:  Starting position of $\Theta^{(t)}$
\item r:  Starting momentum $p$
\item epsilon:  step size $\epsilon$.  Can be a single value or vector length $k$
\item logPOSTERIOR:  name of function to return the log posterior
\item glogPOSTERIOR:  name of function to return the gradient of the log posterior
\item y:  vector of the dependent variable
\item X:  design matrix of independent variables
\item Z:  optional design matrix for random effects
\item constrain:  logical indicator of whether the support of $\Theta$ is positive only (TRUE) or all real numbers (FALSE)
\item lastSTEP: logical indicator of whether this is the last step of the leapfrog loop.  If it is, then the last half step evaluation for the momentum is not evaluated.
\item ...:  additional parameters for logPOSTERIOR and glogPOSTERIOR
\end{itemize}

Next, we review the functionality of the \textit{leapfrog} line-by-line.  

First, the gradient of the log posterior is evaluated at the initial value provided for $\Theta^{(t)}$.  If \textit{Z} is not provided, a NULL value will be passed to this parameter.  Hyperparameters are passed to the \textit{glogPOSTERIOR} function via \textit{...}.

```{r, echo=TRUE, eval=FALSE}
  # gradient of log posterior for old theta
  g.ld <- glogPOSTERIOR(theta_lf, y=y, X=X, Z=Z, ...)
```

Once the gradient is calculated, the first update of the momentum is performed.  The step size for this first update is half of the step size $\epsilon$.

```{r, echo=TRUE, eval=FALSE}
  # first momentum update
  r.new <- r + epsilon/2*g.ld
```

After the momentum half-step update, $\Theta$ is updated by the full step size $\epsilon$.  

```{r, echo=TRUE, eval=FALSE}
  # theta update
  theta.new <- theta_lf + as.numeric(epsilon* Minv %*% r.new)
```

The next few steps are optionally performed to change the sign of the momentum and $\Theta$ if the support of $\Theta$ is strictly positive (cite Neil).  These steps are only relevant if \textit{constrain} is set to TRUE.  Otherwise, the momentum and $\Theta$ are unchanged.

```{r, echo=TRUE, eval=FALSE}
  # check positive
  switch_sign <- constrain & theta.new < 0
  r.new[switch_sign] <- -r.new[switch_sign]
  theta.new[switch_sign] <- -theta.new[switch_sign]
```

Once $\Theta$ is updated, the gradient of the log posterior is re-calculated at the new set of values.

```{r, echo=TRUE, eval=FALSE}
  # gradient of log posterior for new theta
  g.ld.new <- glogPOSTERIOR(theta.new, y=y, X=X, Z=Z, ...)
```

Finally, unless this the final leapfrog step, the momentum is updated by its second half-step.  

```{r, echo=TRUE, eval=FALSE}
  # if not on last step, second momentum update
  if (!lastSTEP) {
    r.new <- r.new + epsilon/2*g.ld.new
  }
```

The \textit{leapfrog} function then returns the new values of $\Theta$ and momentum in a list.  

```{r, echo=TRUE, eval=FALSE}
  list(theta.new=theta.new,
       r.new=as.numeric(r.new))
```


# Appendix: HMC examples

We demonstrate fitting HMC models in R with three examples of increasing complexity.  

## Linear Regression

```{r, echo=TRUE, eval=FALSE}
head(warpbreaks)

summary(warpbreaks)

```


Frequentist version of lm model with warpbreaks

```{r, echo=TRUE, eval=FALSE}
fm1 <- lm(breaks ~ wool*tension, data=warpbreaks)
summary(fm1)

```

Use HMC to fit the same model

Likelihood function for linear regression

$$
p(y | X, \beta; \sigma^2) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp{\left(-\frac{1}{2\sigma^2} (y - X\beta)^T(y-X\beta) \right)}
$$

The posterior for $\beta$ and $\sigma^2$

$$
\begin{aligned}
p(\beta, \sigma^2|y; X) &\propto p(y|X, \beta, \sigma^2) p(\beta, \sigma^2) \\
&\propto p(y|X, \beta, \sigma^2)p(\beta)p(\sigma^2)
\end{aligned}
$$

Flat prior for $\beta$ and Inverse Gamma for $\sigma^2$

$$
\begin{aligned}
p(\beta) &\propto const \\
p(\sigma^2) &\sim IG(a, b)
\end{aligned}
$$

pdf for $\sigma^2$ prior

$$
p(\sigma^2) = \frac{b^a}{\Gamma(a)} (\sigma^2)^{-a-1} \exp\left(-\frac{b}{\sigma^2} \right)
$$

Log-transform $\sigma^2$ to allow proposals across entire real number line

$$
\begin{aligned}
\gamma &= \log \sigma^2 \\
\sigma^2 &= g^{-1}(\gamma) = e^\gamma \\
p_\gamma(\gamma) &= p_{\sigma^2}(g^{-1}(\gamma))\left\lvert \frac{d\sigma^2}{d\gamma} \right\rvert \\
&= \frac{b^a}{\Gamma(a)}(e^\gamma)^{-a-1} \exp\left(-\frac{b}{\sigma^2} \right) \left\lvert e^\gamma  \right\rvert \\
&= \frac{b^a}{\Gamma(a)}e^{-a\gamma}\exp\left(-\frac{b}{e^\gamma} \right) \\
&\propto e^{-a\gamma}\exp\left(-\frac{b}{e
^\gamma} \right) \\
\log p(\gamma) &\propto -a\gamma - be^{-\gamma}
\end{aligned}
$$

The posterior for $\beta$ and $\gamma$

$$
\begin{aligned}
p(\beta, \gamma|y, X) &\propto p(y|X, \beta, \gamma) p(\beta, \gamma) \\
&\propto p(y|X, \beta, \gamma)p(\beta)p(\gamma) \\
&\propto \frac{1}{(2\pi e^\gamma)^{n/2}}\exp{\left(-\frac{1}{2 e^\gamma} (y - X\beta)^T(y-X\beta) \right)} e^{-a\gamma}\exp\left(-\frac{b}{e^\gamma} \right) \\
&\propto \frac{e^{-\gamma n/2}}{(2\pi)^{n/2}}\exp{\left(-\frac{e^{-\gamma}}{2} (y - X\beta)^T(y-X\beta) \right)} e^{-a\gamma} e^{-b e^{-\gamma}} \\
\log p(\beta, \gamma | y, X) &\propto -\frac{\gamma n}{2} -\frac{e^{-\gamma}}{2} (y - X\beta)^T(y-X\beta) - a\gamma - b e^{-\gamma}  \\
&\propto -\left(\frac{n}{2} + a \right)\gamma  -\frac{e^{-\gamma}}{2} (y - X\beta)^T(y-X\beta)- b e^{-\gamma}
\end{aligned}
$$

Prepare data for analysis

```{r, echo=TRUE, eval=FALSE}
X <- model.matrix(breaks ~ wool*tension, data=warpbreaks)
y <- warpbreaks$breaks

```

There are 6 $\beta$ parameters in this model and one $\epsilon$ parameter.

```{r, echo=TRUE, eval=FALSE}
dim(X)

```

```{r, echo=TRUE, eval=FALSE}
N <- 10000
set.seed(143)

eps_vals <- c(rep(2e-1, 6), 2e-2)

fm1_hmc <- hmc(N, theta.init = c(rep(0, 6), 1), epsilon = eps_vals, L = 10, 
                   logPOSTERIOR = linear_posterior, 
                   glogPOSTERIOR = g_linear_posterior, y=y, X=X, 
                   varnames = c(colnames(X), "log_sigma_sq"))

```

Summarize fit

```{r, echo=TRUE, eval=FALSE}
fm1_hmc$accept

summary(fm1_hmc)


```

Plot fit

```{r, echo=TRUE, eval=FALSE}
beta_freq <- coef(fm1)
eps_freq <- 2*log(sigma(fm1))
plot(fm1_hmc, actual.mu = c(beta_freq, eps_freq))

```


## Logistic Regression


For binary response, we let

$$
p = Pr(Y = 1 | X) = [1 + e^{-X\beta}]^{-1}
$$

With likelihood and log-likelihood

$$
\begin{aligned}
L(\beta; X, y) &= \prod_{i=1}^n p^{y_i} (1-p)^{1-y_i} \\
&= \prod_{i=1}^{n} \left(\frac{1}{1+e^{-X_i\beta}}\right)^{y_i} 
\left(\frac{e^{-X_i\beta}}{1+e^{-X_i\beta}}\right)^{1-y_i}  \\
l(\beta;X,y) &= \sum_{i=1}^n -y_i\log(1+e^{-X_i\beta}) + (1-y_i)(-X_i\beta - \log(1+e^{-X_i\beta})) \\
&= \sum_{i=1}^n -\log(1+e^{-X_i\beta}) - X_i\beta(1 - y_i) \\
&= \sum_{i=1}^n X_i\beta(y_i - 1) - \log(1 + e^{-X_i\beta})
\end{aligned}
$$

We set a multivariate Normal prior for $\beta$

$$
\begin{aligned}
\beta &\sim N(0, \Sigma_\beta) \\
&\sim N(0, BI)
\end{aligned}
$$

With pdf

$$
\begin{aligned}
p(\beta) &= \frac{1}{\sqrt{\lvert 2\pi \Sigma_\beta \rvert }}e^{-\frac{1}{2}\beta^T \Sigma_\beta^{-1}\beta} \\
\log p(\beta) &= -\frac{1}{2}\log(2\pi \lvert \Sigma_\beta \rvert) - \frac{1}{2}\beta^T \Sigma_\beta^{-1} \beta \\
&\propto -\frac{1}{2}\log \lvert\Sigma_\beta\rvert - \frac{1}{2}\beta^T \Sigma_\beta^{-1} \beta
\end{aligned}
$$


Let $B = 1e3$ for instance, as a relatively uniformative prior.

Now derive the log posterior

$$
\begin{aligned}
p(\beta | X, y) &\propto p(y | X, \beta)  p(\beta) \\
\log p(\beta | X, y) & \propto \log p(y | X, \beta) + \log p(\beta) \\
&\propto \sum_{i=1}^n X_i\beta(y_i - 1) - \log(1 + e^{-X_i\beta}) -\frac{1}{2}\log \lvert\Sigma_\beta\rvert - \frac{1}{2}\beta^T \Sigma_\beta^{-1} \beta 
\end{aligned}
$$

The log posterior is the sum of the log likelihood and the log of the prior for $\beta$.  

We consider an example from Hosmer and Lemeshow (1989) on a dataset of 189 births at a U.S. hospital.  The dependent variable is an indicator of low birth weight.  Data is available from the MASS package (Modern Applied Statistics with S by Venables and Ripley).  We prepare the data for analysis as noted in the text. 

```{r, echo=TRUE, eval=FALSE}
birthwt2 <- birthwt

# label race variable
birthwt2$race2 <- factor(birthwt2$race, labels = c("white", "black", "other"))

# reduce to indicator variable for positive number of premature labors
birthwt2$ptd <- ifelse(birthwt2$ptl > 0, 1, 0)

# reduce to three levels
birthwt2$ftv2 <- factor(ifelse(birthwt2$ftv > 2, 2, birthwt2$ftv), 
                        labels = c("0", "1", "2+"))

# create design matrix
X <- model.matrix(low ~ age + lwt + race2 + smoke + ptd + ht + ui + ftv2, 
                  data = birthwt2)
y <- birthwt2$low
```

Variables of interest in this dataset are:

\begin{itemize}
\item low:  birth weight less than 2.5kg (0/1)
\item age:  age of mother (yrs)
\item lwt:  weight of mother (lbs)
\item race2:  factor white/black/other
\item smoke:  smoking indicator (0/1)
\item ptd:  premature labor indicator (0/1)
\item ht:  history of hypertension indicator (0/1)
\item ui:  uterine irritability indicator (0/1)
\item ftv2:  number of physician visits factor (0, 1, 2 or more)
\end{itemize}


Next, fit the logistic regression using the frequentist \textit{glm} function in R.

```{r, echo=TRUE, eval=FALSE}
fm2 <- glm(low ~ age + lwt + race2 + smoke + ptd + ht + ui + ftv2, data=birthwt2, 
           family = binomial)
summary(fm2)

```

Now, fit the same model using HMC.


```{r, echo=TRUE, eval=FALSE}
N <- 10000
set.seed(143)

# use different epsilon values for continuous and dichotomous variables
continuous_ind <- c(FALSE, TRUE, TRUE, rep(FALSE, 8))
eps_vals <- ifelse(continuous_ind, 1e-3, 5e-2)

fm2_hmc <- hmc(N, theta.init = rep(0, 11), epsilon = eps_vals, L = 10, 
                   logPOSTERIOR = logistic_posterior, 
                   glogPOSTERIOR = g_logistic_posterior, y=y, X=X, 
                   varnames = colnames(X))

```

Summarize the results

```{r, echo=TRUE, eval=FALSE}
fm2_hmc$accept/N

summary(fm2_hmc)

```

Diagnostic plots

```{r, echo=TRUE, eval=FALSE}
plot(fm2_hmc, actual.mu=coef(fm2))

```

## Mixed effects model

Sophisticated Mixed Effects are also possible using the HMCinR package (may rename).  
Poisson distribution

$$
p(y; \mu) = \frac{e^{-\mu}\mu^y}{y!}
$$

Log link function.  We have $i = 1...n$ observations and $j = 1...m$ groups.

$$
\begin{aligned}
\mu &:= E(Y | X, Z) = e^{X\beta + Zu} \\
\log \mu &= X\beta + Zu
\end{aligned}
$$

Develop the likelihood

$$
\begin{aligned}
L(\mu; y) &= \prod_{i=1}^n \prod_{j=1}^m \frac{e^{-\mu_{ij}}\mu_i^{y_{ij}}}{y_{ij}!} \\
L(\beta; y, X) &= \prod_{i=1}^n \prod_{j=1}^m \frac{e^{-e^{X_i\beta + Z_{ij}u_{ij}}}e^{y_i(X_i\beta + Z_{ij}u_{ij})}}{y_i!} \\
\end{aligned}
$$

with log-likelihood

$$
\begin{aligned}
l(\beta; y, X) &=  -e^{\sum_{i=1}^n \sum_{j=1}^m X_{i}\beta + Z_{ij}u_{ij]}} + y_{ij} (X_i \beta + Z_{ij}u_{ij}) - \log y_{ij}! \\
&\propto -e^{\sum_{i=1}^n \sum_{j=1}^m X_{i}\beta + Z_{ij}u_{ij]}} + y_{ij} (X_i \beta + Z_{ij}u_{ij}) \\
&\propto -e^{\sum_{ij} X\beta + Zu} + y^T(X\beta + Zu)
\end{aligned}
$$

We set a multivariate Normal prior for $\beta$

$$
\begin{aligned}
\beta &\sim N(0, \Sigma_\beta) \\
&\sim N(0, BI)
\end{aligned}
$$


With pdf

$$
\begin{aligned}
p(\beta) &= \frac{1}{\sqrt{\lvert 2\pi \Sigma_\beta \rvert }}e^{-\frac{1}{2}\beta^T \Sigma_\beta^{-1}\beta} \\
\log p(\beta) &= -\frac{1}{2}\log(2\pi \lvert \Sigma_\beta \rvert) - \frac{1}{2}\beta^T \Sigma_\beta^{-1} \beta \\
&\propto -\frac{1}{2}\log \lvert\Sigma_\beta\rvert - \frac{1}{2}\beta^T \Sigma_\beta^{-1} \beta
\end{aligned}
$$

Let $B = 1e3$ for instance, as a relatively uniformative prior.

The distribution of the random effects are defined as normal with a mean of zero. 

Note that in this parameterization, we directly decompose $G$ instead of $G^{-1}$ as in Chan and Jelikzhov (2009).

$$
\begin{aligned}
u &\sim N(0, G)  \\
G &= L D L^T \\
&= L D^{1/2} D^{1/2} L^T \\
\end{aligned}
$$


Let $\lambda_k$ where $k = 1, ... p$ denote the diagonal entries of $D^{1/2}$ and let $a_{kj}$ where $1 \leq j < k \leq p$ denote free elements of lower unitrangular matrix $L$

$$
D^{1/2} := 
\begin{pmatrix}
\lambda_1 & 0 & ... & 0 \\
0 & \lambda_2 & 0 ... & 0 \\
... & ... & ... & ... \\
0 & 0 & ... & \lambda_p
\end{pmatrix}, 
L :=
\begin{pmatrix}
1 & 0 & 0 & ... & 0 \\
a_{21} & 1 & 0 & ... & 0 \\
a_{31} & a_{32} & 1 & ... & ... \\
... & ... & ... & ... & ... \\
a_{p1} & a_{p2} & ... & ... & 1 \\
\end{pmatrix}
$$

Also define $\lambda := (\lambda_1, ..., \lambda_p)^T$ and $a_k := (a_{k1}, ..., a_{k, k-1})^T$ and $a := (a_2^T, ..., a_p^T)^T$.  

For simplicity, we define $a_k = 0\;\; \forall k$ in this application.  $L$ then simplifies to the identity matrix.  

Consider priors where $k = 1...p$.  The prior for $\lambda_k$ is half-t per Gelman (2006).  The previous parameterization was inverse gamma, which can be unintentionally informative for hierarchical models.  Another previous parameterization was uniform, which was also recommended by Gelman for more than 3 groups, but has the consequence of a positive bias.  

Prior distributions for variance parameters in hierarchical models.  Andrew Gelman (2006)

We use a half-t prior for standard deviation $\lambda_k$

$$
\begin{aligned}
p(\lambda_k) &\sim \left(1 + \frac{1}{\nu}\left(\frac{\lambda_k}{A} \right)^2 \right)^{-(\nu+1)/2} 
\end{aligned}
$$

In prior parameterizations, we directly defined the likelihood of the random effects $u$.  Per Betancourt, Girolami (2013), we re-parameterize $u$ using a standard normal parameterization we define as $\tau = (\tau_1, ..., \tau_q)$.  Here, $u$ is a deterministic function of $G$ and $\tau$.

$$
\begin{aligned}
\tau &\sim N(0, I_q) \\
u &:= L D^{1/2} \tau \\
&\sim N(0, LD^{1/2} I (L D^{1/2})^T) \\
&\sim N(0, L D^{1/2} D^{1/2} L^T) \\
&\sim N(0, G)
\end{aligned}
$$

The distribution of $u$ therefore does not change with this parameterization.  The intent of our re-parameterization is to allow $G$ and $\tau$ to be largely independent in the MCMC sampling.  

Now derive the log posterior

Bayes rule to derive log posterior

$$
\begin{aligned}
p(\beta, u, G | y, X, Z) &\propto p(y | \beta, u, G)  p(\beta, u, G) \\
&\propto p(y|\beta, u, G) p(\beta) p(u|G) p(G) \\
\log p(\beta, u, G | y, X, Z) &\propto \log p(y|\beta, u, G) + \log p(\beta) + \log p(u|G) + \log p(G)
\end{aligned}
$$

The log posterior is the sum of the log likelihood and the log of the prior for $\beta$.  

We have hyperpriors $\lambda$ for $G$

Use a half-t prior for variance parameters per Gelman (2006) Prior Distributions for Variance Parameters in Hierarchical Models

Recall that we parameterized $\lambda_k$ as uniform

$$
\begin{aligned}
p(\lambda_k) &\sim \left(1 + \frac{1}{\nu}\left(\frac{\lambda_k}{A} \right)^2 \right)^{-(\nu+1)/2} \\
p_{\xi_k}(\xi_k) &= p_{\lambda_k}(g^{-1}(\xi_k)) \left\lvert \frac{d\lambda_k}{d\xi_k}  \right\rvert \\
&= p_{\lambda_k} (e^{\xi_k})\lvert e^{\xi_k}\rvert \\
&\propto  \left(1 + \frac{1}{\nu}\left(\frac{e^{\xi_k}}{A} \right)^2 \right)^{-(\nu+1)/2} e^{\xi_k}\\
&\propto \left(1 + \frac{1}{\nu}\left(\frac{e^{2\xi_k}}{A^2} \right) \right)^{-(\nu+1)/2} e^{\xi_k}\\
\log p(\xi_k) &\propto -\frac{\nu+1}{2}\log\left(1 + \frac{1}{\nu}\left(\frac{e^{2\xi_k}}{A^2} \right) \right) + \xi_k \\
\frac{\partial}{\partial\xi_k}\log p(\xi_k) &\propto -\frac{\nu+1}{2}\frac{1}{1 + \frac{1}{\nu}\left( \frac{e^{2\xi_k}}{A^2} \right)}\frac{2e^{2\xi_k}}{\nu A^2} + 1 \\
&\propto -(\nu+1)\frac{1}{1 + \nu A^2 e^{-2\xi_k}} + 1
\end{aligned}
$$


Write the full log posterior


$$
\begin{aligned}
\log p(\beta, u, G | y, X, Z) &\propto \log p(y|\beta, u, G) + \log p(\beta) + \log p(u|G) + \log p(G) \\
\log p(\beta, \gamma, \tau, \xi_1...\xi_q,a | y, X, Z) &\propto \log p(y | \beta, \tau, \gamma, \xi, a) + \log p(\beta) + \log p(\gamma)+ \log p(\tau) + \log p(\xi) + \log p(a) \\
&\propto  -e^{\sum_{ij} X\beta + Zu} + y^T(X\beta + Zu) -\frac{1}{2}\log \lvert\Sigma_\beta\rvert - \frac{1}{2}\beta^T \Sigma_\beta^{-1} \beta \\
&-\sum_{k=1}^q \left(\frac{\nu_{\lambda_k}+1}{2}\log\left(1 + \frac{1}{\nu_{\lambda_k}} 
\frac{e^{2\xi_k}}{A_{\lambda_k}^2}\right)+ \xi_k\right) -\frac{1}{2} a^T A^{-1} a -\frac{1}{2} \tau^T \tau\\
&\propto  -e^{\sum_{ij} X\beta + ZLD^{1/2}\tau} + y^T(X\beta + ZLD^{1/2}\tau) -\frac{1}{2}\log \lvert\Sigma_\beta\rvert - \frac{1}{2}\beta^T \Sigma_\beta^{-1} \beta \\
&-\sum_{k=1}^q \left(\frac{\nu_{\lambda_k}+1}{2}\log\left(1 + \frac{1}{\nu_{\lambda_k}} 
\frac{e^{2\xi_k}}{A_{\lambda_k}^2}\right)+ \xi_k\right) -\frac{1}{2} a^T A^{-1} a -\frac{1}{2} \tau^T \tau\\
\end{aligned}
$$


We need to derive the gradient of the log posterior for the leapfrog function.  Let $J^{kk}$ be the single element version of the diagonal matrix $D^{1/2}$ retaining only a single $e^{\xi_k}$ and the remainder of the diagonal zero.

$$
\begin{aligned}
\log p(\beta, \gamma, \tau, \xi_1...\xi_q,a | y, X, Z) &\propto \log p(y | \beta, \tau, \gamma, \xi, a) + \log p(\beta) + \log p(\gamma)+ \log p(\tau) + \log p(\xi) + \log p(a) \\\\
&\propto  -e^{\sum_{ij} X\beta + ZLD^{1/2}\tau} + y^T(X\beta + ZLD^{1/2}\tau) -\frac{1}{2}\log \lvert\Sigma_\beta\rvert - \frac{1}{2}\beta^T \Sigma_\beta^{-1} \beta \\
&-\sum_{k=1}^q \left(\frac{\nu_{\lambda_k}+1}{2}\log\left(1 + \frac{1}{\nu_{\lambda_k}} 
\frac{e^{2\xi_k}}{A_{\lambda_k}^2}\right)+ \xi_k\right) -\frac{1}{2} \tau^T \tau\\
\frac{\partial l}{\partial \beta} &\propto -\left(e^{\sum_{ij}X\beta + ZLD^{1/2}\tau} \right)^T X + y^T X - \Sigma_\beta^{-1} \beta \\
\frac{\partial l}{\partial \tau} &\propto -\left(e^{\sum_{ij}X\beta + ZLD^{1/2}\tau}  \right) ZLD^{1/2} + y^T ZLD^{1/2} - \tau \\
\frac{\partial l}{\partial \xi_k} &\propto -\left(e^{\sum_{ij}X\beta + ZLD^{1/2}\tau}  \right) ZLJ^{kk}\tau + y^TZLJ^{kk}\tau - \frac{\nu_{\lambda_k} +1}{1 + \nu_{\lambda_k}A_{\lambda_k}^2 e^{-2\xi_k}} + 1 
\end{aligned}
$$


```{r, echo=TRUE}
library(lme4)

# Gopher data
data(gopherdat2)

# omit area offset
fm3 <- glmer(shells~prev+factor(year)+(1|Site),
      family=poisson,data=Gdat,
      control=glmerControl(optimizer="bobyqa",
                           check.conv.grad=.makeCC("warning",0.05)))


summary(fm3)

```

Store Frequentist estimates

```{r, echo=TRUE, eval=FALSE}
truevals_fixed <- c(fixef(fm3))
truevals_fixed <- truevals_fixed[c(1, 3, 4, 2)]
truevals_random <- as.numeric(ranef(fm3)$Site[, 1])
truevals <- c(truevals_fixed, truevals_random, 
              log(sqrt(as.numeric(VarCorr(fm3)$Site[, 1]))))
  # sqrt(as.numeric(VarCorr(fm3)$Site[, 1])))
nvar <- length(truevals)

```

Setup data for HMC

```{r, echo=TRUE, eval=FALSE}
library(Matrix)

##########
# block diagonal
Zi.lst <- split(rep(1, nrow(Gdat)), Gdat$Site)
Zi.lst <- lapply(Zi.lst, as.matrix)
Z <- bdiag(Zi.lst)
Z <- as.matrix(Z)
X <- model.matrix(~ factor(year), data=Gdat)
X <- cbind(X, Gdat$prev)
colnames(X)[ncol(X)] <- "prev"
colnames(X) <- make.names(colnames(X))
colnames(X)[1] <- "intercept"
y <- Gdat$shells
p <- ncol(X)

```



Test gradient

```{r, echo=TRUE, eval=FALSE}
library(pracma)
library(numDeriv)
set.seed(143)
thetaInit <- c(rnorm(4), 
                rnorm(10, mean=0, sd=1e-3), 
                -2)

test1 <- g_glmm_poisson_posterior(thetaInit, y=y, X=X, Z=Z, m=10, nuxi=4, Axi=1)

test2 <- numDeriv::grad(func=glmm_poisson_posterior, x=thetaInit, 
                        side=NULL, method.args=list(), y=y, X=X, Z=Z, nuxi=4, Axi=1)

cbind(test1, test2)
all.equal(test1,test2)

```


Run HMC for poisson regression model

Small data set.  We use a more informative prior on the random effects variance, a half-t prior with 4 degrees of freedom and scale parameter of 1. 


```{r, echo=TRUE, eval=FALSE}
N <- 1e5

set.seed(412)
initvals <- c(rep(0, 4), 
                rnorm(10, mean=0, sd=1e-3), 
                0)


M_vals <- c(1e-3, 1e-3, 1e-3, 1, 
            rep(1e-3, 10), 
            1e-3)



t1.hmc <- Sys.time()

eps_vals <- c(rep(1e-3, 3), 1e-4, rep(2e-4, 11))

res <- hmc(N = N, theta.init = initvals, epsilon = eps_vals, L = 10, 
                    logPOSTERIOR  = glmm_poisson_posterior, 
                    Mdiag = M_vals, 
                    varnames=c(colnames(X), paste0("u", 1:ncol(Z)), "lambda"),
                    glogPOSTERIOR  = g_glmm_poisson_posterior, 
                    y = y, X=X, Z=Z, m=10, nuxi=1, Axi=25) 

mypath <- '/Users/samthomas/biostat'
saveRDS(res, file = paste(mypath, "example3.RData", sep="/"))

t2.hmc <- Sys.time()
t2.hmc - t1.hmc
res$accept/N
```

```{r, echo=TRUE, eval=FALSE}
N <- 1e5
mypath <- '/Users/samthomas/biostat'
res <- readRDS(paste(mypath, 'example3.RData', sep='/'))

```

```{r,echo=TRUE, eval=FALSE}
# plot(res, burnin=round(0.3*N), actual.mu=truevals_fixed, cols=1:ncol(X))

# plot(res, actual.mu = truevals_fixed, cols=1:4)
plot(res)
```

DELETE
Original $\beta$ parameter

$$
\beta = R^{*^{-1}}\theta
$$

DELETE
Obtain $\beta$ from QR decomposition

```{r, echo=TRUE, eval=FALSE}
# restore beta from Rstar in QR decomposition
calc_beta <- function(theta_param, Rstarinv) {
  as.numeric(Rstarinv %*% theta_param)
}
```
