---
title: "hmclearn_logistic_regression"
author:  "Samuel Thomas"
date: "``r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{hmclearn_logistic_regression}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(hmclearn)
```


# GLM - Logistic Regression

For binary response, we let

$$
p = Pr(Y = 1 | X) = [1 + e^{-X\beta}]^{-1}
$$


With likelihood and log-likelihood

$$
\begin{aligned}
L(\beta; X, y) &= \prod_{i=1}^n p^{y_i} (1-p)^{1-y_i} \\
&= \prod_{i=1}^{n} \left(\frac{1}{1+e^{-X_i\beta}}\right)^{y_i} 
\left(\frac{e^{-X_i\beta}}{1+e^{-X_i\beta}}\right)^{1-y_i}  \\
l(\beta;X,y) &= \sum_{i=1}^n -y_i\log(1+e^{-X_i\beta}) + (1-y_i)(-X_i\beta - \log(1+e^{-X_i\beta})) \\
&= \sum_{i=1}^n -\log(1+e^{-X_i\beta}) - X_i\beta(1 - y_i) \\
&= \sum_{i=1}^n X_i\beta(y_i - 1) - \log(1 + e^{-X_i\beta})
\end{aligned}
$$

We set a multivariate Normal prior for $\beta$

$$
\begin{aligned}
\beta &\sim N(0, \Sigma_\beta) \\
&\sim N(0, BI)
\end{aligned}
$$

With pdf

$$
\begin{aligned}
p(\beta) &= \frac{1}{\sqrt{\lvert 2\pi \Sigma_\beta \rvert }}e^{-\frac{1}{2}\beta^T \Sigma_\beta^{-1}\beta} \\
\log p(\beta) &= -\frac{1}{2}\log(2\pi \lvert \Sigma_\beta \rvert) - \frac{1}{2}\beta^T \Sigma_\beta^{-1} \beta \\
&\propto -\frac{1}{2}\log \lvert\Sigma_\beta\rvert - \frac{1}{2}\beta^T \Sigma_\beta^{-1} \beta
\end{aligned}
$$


Let $B = 1e3$ for instance, as a relatively uniformative prior.

Now derive the log posterior

$$
\begin{aligned}
p(\beta | X, y) &\propto p(y | X, \beta)  p(\beta) \\
\log p(\beta | X, y) & \propto \log p(y | X, \beta) + \log p(\beta) \\
&\propto \sum_{i=1}^n X_i\beta(y_i - 1) - \log(1 + e^{-X_i\beta}) -\frac{1}{2}\log \lvert\Sigma_\beta\rvert - \frac{1}{2}\beta^T \Sigma_\beta^{-1} \beta 
\end{aligned}
$$

The log posterior is the sum of the log likelihood and the log of the prior for $\beta$.  

Next, we need to derive the gradient of the log posterior for the leapfrog function

$$
\begin{aligned}
p(\beta | X, y) &\propto p(y | X, \beta)  p(\beta) \\
\log p(\beta | X, y) & \propto \log p(y | X, \beta) + \log p(\beta) \\
&\propto \sum_{i=1}^n X_i\beta(y_i - 1) - \log(1 + e^{-X_i\beta}) -\frac{1}{2}\log \lvert\Sigma_\beta\rvert - \frac{1}{2}\beta^T \Sigma_\beta^{-1} \beta \\
\frac{\partial\log p(\beta | X, y) }{\partial\beta} &\propto X(y - 1)+ \frac{e^{-X\beta}}{1 + e^{-X\beta}}X - \Sigma_\beta^{-1} \beta^T
\end{aligned}
$$

# Data

Load Endometrial data

```{r, echo=TRUE}
wk.dir <-  '~/biostat/Dissertation/Summer2018'

Endometrial <- read.table(paste(wk.dir, "endometrial.dat", sep='/'), header=TRUE)

# data prep
Endometrial$PI2 <- with(Endometrial, (PI - mean(PI)) / sd(PI))
Endometrial$EH2 <- with(Endometrial, (EH - mean(EH)) / sd(EH))
Endometrial$NV2 <- Endometrial$NV - 0.5

X <- cbind(1, as.matrix(Endometrial[, which(colnames(Endometrial) %in% c("PI2", "EH2", "NV2"))]))
y <- Endometrial$HG


```

# Frequentist model

```{r, echo=TRUE}
f <- glm(y ~ X-1, family = binomial())
summary(f)

```

# EHMC logistic regression

Run HMC for logistic regression model

```{r, echo=TRUE}
library(profvis)
Mruns <- 20000

set.seed(412)

t1.hmc <- Sys.time()
#profvis({
 hmc.results <- hmc(M = Mruns, thetaInit = rep(1, 4), epsilon = 1e-2, Nstep = 170, 
                    logDENS = logistic_posterior, glPOSTERIOR = g_logistic_posterior, y = y, X=X) 
#})

t2.hmc <- Sys.time()
t2.hmc - t1.hmc
hmc.results$accept/Mruns


hmc.param <- hmc.results$theta
hmc.param <- as.data.frame(do.call(rbind, hmc.param))
colnames(hmc.param) <- c("(Intercept)", "PI2", "EH2", "NV2")

diagplots_mcmc(hmc.param, beta.true, burnin=3000)
```

EHMC Posterior quantiles

```{r, echo=TRUE}
hmc.param.burnin <- hmc.param[-c(1:3000), ]
hmc.intervals <- apply(hmc.param.burnin, 2, quantile, probs=c(0.025, 0.25, 0.5, 0.75, 0.975))
t(hmc.intervals)
```
