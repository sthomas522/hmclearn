---
title: "HMC in R"
output: pdf_document
vignette: >
  %\VignetteIndexEntry{HMC_learn}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
header-includes:
  - \usepackage{setspace}
  - \doublespacing
  - \usepackage{amsmath}
  - \usepackage{algorithm}
  - \usepackage{algpseudocode}
  - \usepackage{enumitem}
  - \usepackage{bm}
bibliography: hmc.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(hmclearn)
library(MASS)
```

# Introduction

Hamiltonian Monte Carlo (HMC) [@duane_hybrid_1987][@neal_handbook_2011][@betancourt_conceptual_2017] has emerged as a general purpose tool for Bayesian practitioners.  A key advantage of HMC over more traditional Markov Chain Monte Carlo (MCMC) algorithms is its improved computational efficiency in fitting high-dimensional models.   While the algorithm itself is not difficult to program, the substantial number of tuning parameters can be daunting to those unfamiliar with the theory behind the method.  Until recently, practical access to HMC was limited to individuals with both the mathematical background to understand the algorithm and the programming skill to implement the simulation in a high-performance environment.   

Modern Bayesian software such as Stan [@gelman_stan:_2015] has made HMC accessible to practitioners who are comfortable with any one of a variety of well-known programming platforms (e.g. R, Python, Matlab).  The Stan language is similar in style to WinBUGS, which is familiar to many Bayesian statisticians.  The software translates Stan code to a lower-level language to maximize speed and efficiency.  In addition, Stan automates the challenging process of tuning the many parameters in HMC.  As a result, Stan has succeeded in making HMC accessible to many Bayesian practitioners around the world in both academia and industry.  

While Stan and other high-performance software (e.g. PyMC [@salvatier_probabilistic_2016], Edward [@tran_edward:_2016]) provide enormous practical value to analysts, the intuition of how HMC works can be lost in the process of fitting models.  HMC can appear to be an opaque, "black-box" algorithm behind the sophisticated automation.  This is an unfortunate consequence.  While understanding HMC is not necessary to use production software, intuition behind the method can be helpful both in fine-tuning the simulation process and in instilling confidence in the results.  

The purpose of this paper is to introduce HMC to analysts using R software only, an open-source statistical environment that is familiar to many.  While many excellent introductions to HMC are available on a conceptual level, this paper will focus on learning HMC by doing.  Familiarity with popular MCMC algorithms such as Metropolis-Hastings (MH) is helpful, but not required.  A companion R package called hmclearn contains the R code for all of the functions used in this introduction is freely available to download.  

# MCMC Basic Concepts

We consider $n$ observations from a simple random sample $\pmb{X} = (X_1, ..., X_n)$, where each element is independent and identically distributed (iid).  From this sample, we want to fit the distribution of our k-dimensional parameter of interest $\Theta = (\theta_1, ..., \theta_k)$.  The posterior distribution $p(\Theta | \pmb{X})$ can be written as a function of the Likelihood $p(\pmb{X} | \Theta)$ and prior $p(\Theta)$ using Bayes formula [@carlin_bayesian_2008].  

$$
\begin{aligned}
p(\Theta | \pmb{X}) &= \frac{p(\pmb{X}|\Theta)p(\Theta)}{\int p(\pmb{X}|\Theta)p(\Theta)d\Theta} \\
&\propto p(\pmb{X}|\Theta) p(\Theta)
\end{aligned}
$$

In many practical data analyses, the integral in the denominator cannot be evaluated directly.  Since the denominator is constant with respect to $\Theta$, only the unnormalized posterior $p(\pmb{X} | \Theta)$ is available.  

## Metropolis-Hastings

The first widely-used MCMC method capable of simulating directly from $p(\pmb{X} | \Theta)$ is called the Metropolis [@metropolis_equation_1953] algorithm, originating in the 1950's from an application to statistical physics.  Nearly two decades later, Hastings generalized the algorithm, which is now called Metropolis-Hastings (MH) [@metropolis_equation_1953][@hastings_monte_1970].  We begin with a brief introduction to MH since HMC builds on many similar concepts.  

The objective of MH is to simulate values of $\Theta$ that accurately reflect the posterior density $p(\Theta | \pmb{X})$.  For brevity, we will shorten our notation of the posterior as $p(\Theta)$. The Markov chain simulates values from this density in sequence from $t = 1, .., N$, provided some starting point $\Theta^{(0)}$ which is typically provided by the analyst or the computer program.  

MH defines a transition probability that produces a Markov chain that is ergodic and satisfies detailed balance.  Values of $\Theta^{(t)}$ in the chain are defined in part by a proposal density, which we define as $q(\Theta^{\text{PROP}} | \Theta^{t-1})$.  Here, $\Theta^{\text{PROP}}$ is a proposal for the next value in the chain.  This proposal density is conditioned on the previously stored value $\Theta^{(t-1)}$.  A variety of proposal functions can be used, with random walk proposals being a common choice.  We now outline the MH algorithm in full.  

\begin{algorithm}
\caption{Metropolis-Hastings}\label{Metropolis-Hastings}
\begin{algorithmic}[1]
\Procedure{MH}{$\theta^{(0)}, \pi^{*}(\theta), q(\theta^{(x)}|\theta^{(y)}), N$} 
   \State Calculate $\pi^{*}(\theta^{(0)})$ \Comment{Initial value for posterior}
   \For{$t = 1, ..., N$}\Comment{Repeat simulation $N$ times}
      \State $\theta^{\text{PROP}} \gets q(\theta^{\text{PROP}} | \theta^{(t-1)})$ \Comment{Randomly sample proposal}
      \State $u \gets U(0, 1)$ \Comment{Randomly sample from a uniform density, 0 to 1}
      \State $\alpha = \min\left(1, \frac{\pi(\theta^{\text{PROP}})q(\theta^{\text{PROP}}|\theta^{(t-1)})}{\pi(\theta^{(t-1)})q(\theta^{(t-1)}|\theta^{\text{PROP}})} \right)$ \Comment{Calculate acceptance proposal probability}
      \State If $\alpha < u$, then $\theta^{(t)} \gets \theta^{\text{PROP}}$.  Otherwise, $\theta^{(t)} \gets \theta^{(t-1)}$ \Comment{Select proposal or previous value}
   \EndFor\label{markovendfor}
   \State \textbf{return} $\theta^{(1)} ... \theta^{(N)}$ \Comment{Return simulated values of $\theta$ from the unnormalized posterior}
\EndProcedure
\end{algorithmic}
\end{algorithm}

Each proposal in MH is accepted at probability

$$
\alpha = \min\left(1, \frac{p(\Theta^{\text{PROP}})q(\Theta^{\text{PROP}}|\Theta^{(t-1)})}{p(\Theta^{(t-1)})q(\Theta^{(t-1)}|\Theta^{\text{PROP}})} \right)
$$

which simplifies when $q(\Theta^{PROP} | \Theta^{(t-1)})$ is symmetric (i.e. the Metropolis algorithm)

$$
\alpha = \min\left(1, \frac{p(\Theta^{\text{PROP}})}{p(\Theta^{(t-1)})} \right).
$$

Recall that the denominator of the posterior is constant with respect to $\Theta$.  As such, the ratio of posterior densities at two different points $\Theta^{PROP}$ and $\Theta^{(t-1)}$ can be formulated even when the denominator is unknown (i.e. the constants in the denominator cancel).  

Intuition into why MH works can be obtained by examining the acceptance ratio $\alpha$ closely.  Two different outcomes are possible depending on the value of the posterior at the proposed $\Theta^{PROP}$:

\begin{enumerate}
\item If $p(\Theta^{PROP}) \geq p(\Theta^{(t-1)})$, then the posterior has a higher density at the proposed value of $\Theta$ than at the previous point in the chain $t-1$.  When this occurs, the proposal is always accepted (i.e. at probability 1).  
\item If $p(\Theta^{PROP}) < p(\Theta^{(t-1)})$, then the posterior has a lower density at the proposed value of $\Theta$ than at the previous point in the chain.  When this occurs, we accept the proposal at random based on the ratio $0 < \alpha < 1$.  If the proposal is not accepted, then the proposal is discarded and the Markov chain remains in place $\Theta^{t} := \Theta^{(t-1)}$.  
\end{enumerate}

As such, MH tends to sample more points in the region of higher posterior values.  However, the tails of the posterior are also sampled based on acceptance ratio.  Given enough samples, the MCMC chain samples $\Theta$ at the proportion of the true posterior density.  The resulting simulated values can then be used for statistical inference.  Much more can be said regarding MH [@carlin_bayesian_2008][@chib_understanding_1995][@gelman_bayesian_2013].

## Limitations of Metropolis-Hastings

The theoretical requirements for fitting models using MH are minimal, making MH an attractive choice for Bayesian inference even today.  The limits of MH are primarily computational in nature.  Since the proposals of $\Theta$ are randomly selected, many simulations are required to accurately describe the true posterior.  Even efficient MH implementations may accept less than 25\% of the proposals [@roberts_weak_1997].  

The limited efficiency of MH can be overcome by high computational power for certain applications.  When the dimensionality of the data is small to moderate, a well-programmed MH algorithm can sample enough points from the posterior density in a reasonable period of time.  The challenge of relying on MH occurs when dealing with high-dimensional data or complex statistical models.  In these situations, MH is known to be inefficient and can be impractical for such applications.  

A popular, often efficient alternative to MH is Gibbs Sampling [@geman_stochastic_1984].  Gibbs is widely used in many Bayesian software platforms such as WinBUGS [@lunn_winbugs_2000] and JAGS [@plummer_jags:_2003].  When the conditional posterior densities can be explicitly formulated, Gibbs remains a viable choice for the Bayesian practitioner.  Such restrictions limit the application of Gibbs to particular combinations of models and priors.  Gibbs therefore lacks the flexibility of MH, in addition to having certain other efficiency limitations of its own [@robert_bayesian_2001].  

Given the modern computational demands of large dataset and complex models, a more efficient MCMC algorithm is desirable.  Ideally, such an algorithm would retain the theoretical advantages and flexibility of MH, while providing a more computational efficient method of selecting proposals.  Here we transition to the modern HMC algorithm, and why HMC has emerged as a standard inferential tool for many Bayesian practitioners.  

# HMC Background

MH and HMC are equally flexible in their theoretical capabilities to fit a variety of model and prior specifications.  The key advantage of HMC over MH is the use of additional information from the posterior to guide proposals.  HMC uses the gradient of the log posterior to direct the Markov chain to the region the region of highest posterior density, where most of the samples should occur.  In contrast, MH relies entirely on the acceptance ratio to guide the chain.   As a result, a well-tuned HMC chain will accept proposals more frequently than a similarly well-tuned MH algorithm [@roberts_weak_1997][@neal_handbook_2011].  

It should be noted that HMC is a MCMC simulation method, not an optimization method.  While the HMC algorithm guides proposals to regions of high density (sometimes called the typical set [@betancourt_conceptual_2017]), the tails of the density are properly sampled as well.  Both MH and HMC produce ergodic Markov chains, but the mathematics of HMC is substantially more complex than MH.  We provide a brief overview of the theoretical basis of HMC here, and refer to other sources for more detailed expositions (Thomas, Tu 2019)[@neal_handbook_2011] [@betancourt_conceptual_2017].

## Hamiltonian Monte Carlo Concepts and Theory

As with MH, our objective in HMC is to simulate the posterior $p(\Theta | \pmb{X})$.  The mathematical basis of HMC is the Hamiltonian function

$$
\begin{aligned}
H(\pmb{p}, \Theta) &= K(\pmb{p}, \Theta) + U(\Theta) \\
&= -\log p(\pmb{p}) - \log p(\Theta).
\end{aligned}
$$

HMC introduces a latent parameter $\pmb{p} = (p_1, ..., p_k)$ of the same length $k$ as the parameter of interest $\Theta = (\theta_1, ..., \theta_k )$.  The latent parameter $\pmb{p}$ is often called the \textit{momentum} based on its original application to physical laws of motion.  The incorporation of the momentum provides the geometrical structure of the space through which the Markov chain travels.  The purpose of the momentum is to ensure that the MCMC simulation is ergodic, covering the entire space of $\Theta$.  

The distribution of $\pmb{p}$ is most often specified to be multivariate Normal with covariance $M$.  The structure of $M$ is often diagonal, but does not have to be.  

$$
\begin{aligned}
\pmb{p} &\sim N_k(0, M) \\
\log p(\pmb{p}) &\propto \frac{1}{2}\pmb{p}^T M^{-1}\pmb{p}
\end{aligned}
$$

The Hamiltonian function represents the joint distribution of the multivariate Normal $\pmb{p}$ and the log posterior $p(\Theta | \pmb{X})$, whose notation we simplify to $p(\Theta)$.

$$
\begin{aligned}
H(\pmb{p}, \Theta) &= -\frac{1}{2}\pmb{p}^T M^{-1} \pmb{p} - \log p(\Theta)
\end{aligned}
$$
The trajectories over which HMC travels in time are defined by first-order differential equations, 

$$
\begin{aligned}
\frac{d\pmb{p}}{dt} &= -\frac{\partial H(\Theta, \pmb{p})}{\partial\Theta}= -\frac{\partial U(\Theta)}{\partial\Theta} = \nabla_\Theta \log p(\Theta) \\
\frac{d\Theta}{dt} &= \frac{\partial H(\Theta, \pmb{p})}{\partial\pmb{p}} = \frac{\partial K(\pmb{p})}{\partial\pmb{p}} = M^{-1}\pmb{p}
\end{aligned}
$$
Here we see the explicit formulation of the gradient of the log posterior $\nabla_\Theta \log p(\Theta)$.  The combination of these two equations forms deterministic paths for the MCMC chain to travel through the joint distribution $(\pmb{p}, \Theta)$.  Note that while the MCMC produces simulations of both $\pmb{p}$ and $\Theta$, only the values of $\Theta$ are of interest for statistical inference.  

A solution for these differential equations is necessary to produce a practical MCMC simulation.  Since no exact solution exists, a discrete approximation is needed to form the algorithm.  The most commonly used solution is called the leapfrog method [@feynman2017character][@newton1833philosophiae].  The leapfrog defines a discrete step size $\epsilon$ individually for $\pmb{p}$ and $\Theta$.  

$$
\begin{aligned}
\pmb{p}(t + \epsilon/2) &= \pmb{p}(t) + (\epsilon/2)\nabla_\Theta\log\pi(\Theta(t)) \\
\Theta(t + \epsilon) &= \Theta(t) + \epsilon M^{-1}\pmb{p}(t + \epsilon/2) \\
\pmb{p}(t + \epsilon) &= \pmb{p}(t + \epsilon/2) + (\epsilon/2)\nabla_\Theta \log\pi(\Theta(t + \epsilon)).
\end{aligned}
$$

The full step $\epsilon$ in $\Theta$ is sandwiched by half-steps $\epsilon/2$ for $\pmb{p}$. The number of steps $\epsilon$ in an actual HMC algorithm is typically defined by a tuning parameter $L > 1$.    This leapfrog approximation, though slightly more complex than Euler approximations, provides a more accurate approximation of the solution over the many samples that HMC requires.  

The paths defined by the Hamiltonian equations are deterministic once $\pmb{p}$ is defined.  An exact solution would always be accepted in a MCMC algorithm.  However, since the solution to the Hamiltonian equations is an approximation, a Metropolis-Hastings style acceptance ratio is used to formally determine whether a proposal is accepted or rejected.

With the overview of the major concepts in HMC complete, we turn to the formulation of the Hamiltonian Monte Carlo algorithm itself.  

## Hamiltonian Monte Carlo Algorithm

The HMC algorithm is presented in full here.  

\begin{algorithm}
\caption{Euclidean Hamiltonian Monte Carlo}\label{EHMC}
\begin{algorithmic}[1]
\Procedure{EHMC}{$\Theta^{(0)}, \log\pi(\Theta), M, N, \epsilon, L$} 
   \State Calculate $\log\pi(\Theta^{(0)})$ \Comment{Initial value for log posterior}
   \For{$t = 1, ..., N$}\Comment{Repeat simulation $N$ times}
      \State $p^0 \gets N(0, M)$ \Comment{Randomly sample momentum from MVN}
      \State $\Theta^{(t)} \gets \Theta^{(t-1)}, \tilde{\Theta} \gets \Theta^{(t-1)}, \tilde{p} \gets p^{(0)}$ \Comment{Randomly sample from a uniform density, 0 to 1}
      \For{$i = 1, ..., L$}\Comment{Run Leapfrog $L$ times}
        \State $\tilde\Theta, \tilde{p} \gets \text{Leapfrog}(\tilde\Theta, \tilde{p}, \epsilon)$
      \EndFor\label{leapfrogfor}
      \State $\alpha = \min{\left(1, \frac{\exp(\log\tilde\Theta) - \frac{1}{2}\tilde{p}\cdot\tilde{p}}{\exp(\log\tilde\Theta^{(t-1)}) - \frac{1}{2}p^0 \cdot p^0} \right)}$ \Comment{Calculate acceptance proposal probability}
      \State With probability $\alpha$, $\Theta^{(t)}\gets\tilde\Theta$ and $p^{(t)} \gets -\tilde{p}$ 
   \EndFor\label{ehmcendfor}
   \State \textbf{return} $\Theta^{(1)} ... \Theta^{(N)}$ \Comment{Return simulated values of $\Theta$ from the unnormalized posterior}
\Function{Leapfrog}{$\Theta, p, \epsilon$}
\State $\tilde{p} \gets p + (\epsilon/2)\nabla_\Theta\log\pi(\Theta)$
\State $\tilde\Theta \gets \Theta + \epsilon\tilde{p}$
\State $\tilde{p} \gets \tilde{p} + (\epsilon/2)\nabla_\Theta\log\pi(\Theta)$
\State \textbf{return} $\tilde\Theta, \tilde{p}$
\EndFunction
\EndProcedure
\end{algorithmic}
\end{algorithm}

One may notice some similarities with MH:  

\begin{enumerate}
\item An initial set of values $\Theta^{(0)}$ is required for both MH and HMC.
\item The random walk Metropolis proposal is often Multivariate Normal.  HMC simulates from the momentum $\pmb{p}$ from a Multivariate Normal.  
\item Proposals are accepted or rejected based on a ratio of log posteriors in both MH and HMC.  
\end{enumerate}

The key functional difference between the two algorithms is the leapfrog update process.  HMC generates a proposal $(\pmb{p}^t, \Theta^t)$ based on the randomly selected values of $\pmb{p}$ and the number of leapfrog steps $L$.  In contrast, MH selects a proposal from one simple randomly selected value from a proposal distribution.   MH is therefore computationally simpler, but undirected.  HMC's proposal combines the randomization provided by $\pmb{p}$ with trajectories defined the by the gradient of the log posterior $\nabla_\Theta\log\pi(\Theta(t))$. 

From an implementation standpoint, the additional programming required for HMC is fairly minimal.  Manually determiming the gradient may be tedious, but the derivation is typically not difficult.  Once the gradient is provided and coded, programming the leapfrog loop itself is straightforward.  

The challenge of practical HMC implementation is related to implementation of MH - tuning.  While tuning MH involves adjusting a proposal density and little else, HMC requires the selection and adjustment of the mass matrix $M$, number of leapfrog steps $L$, and step size $\epsilon$.  

While $M$ can be fixed as an identity matrix, some modification of the diagonal, at a minimum, can provide notable improvements in computational efficiency.  The selection of $L$ is also an essential step in HMC implementation.  Selecting $L$ too low will produce a Markov chain that is inefficient, and may behave in pattern similar to random walk Metropolis, but with the penalty of additional computation.  Setting $L$ too high can also be inefficient, where the chain may travel through the entire deterministic trajectory multiple times before a proposal is selected.  Finally, $\epsilon$ can be set as a single constant for all $k$ parameters, but varying $\epsilon$ for each individual parameter may improve computational efficiency.  

Developing an intuition and appreciation for how HMC works can be challenging without direct experience working with the algorithm.  The next section provides R code and some simple examples for the interested reader to gain practical, hands-on experience with implementing and tuning HMC.  

# HMC Implementation

We introduce how to use the \textit{hmcR} package in this section.  The software is designed with the flexibility to fit a wide variety of types of statistical models and priors with HMC.  General requirements are described here.  Additional detail for interested readers is provided in the Appendix.  

## Main function:  \textit{hmc}

The function \textit{hmc} runs the main HMC algorithm.  Many of the parameters have defaults to the most frequently used values.  However, any of the defaults can be overriden with a custom specification.  

\begin{itemize}
\item \textit{N}:  number of MCMC samples.  Default is 10,000
\item \textit{theta.init}:  initial values for model parameters
\item \textit{epsilon}:  Default 0.01.  stepsize tuning parameter for HMC
\item \textit{L}:  Default 10.  number of Leapfrog steps tuning parameter for HMC
\item \textit{logPOSTERIOR}:  function to return the log posterior depending on $\Theta$, $p$, and data provided in \textit{...}.
\item \textit{glogPOSTERIOR}:  function to return the gradient of the log posterior depending on $\Theta$, $p$, and data provided in \textit{...}.
\item \textit{varnames}: optional vector of variable names in the model
\item \textit{randlength}:  default FALSE.  Boolean value on whether to apply some randomness to the number of Leapfrog steps \textit{L}
\item \textit{Mdiag}:  optional vector for the diagonal of the Mass matrix $M$
\item \textit{constrain}:  optional vector of which variables are bounded as positive only.  
\item \textit{verbose}:  default FALSE.  Boolean on whether to print status updates of \textit{hmc}
\item \textit{...}:  Parameters, including data, passed to \textit{logPOSTERIOR} and \textit{glogPOSTERIOR}
\end{itemize}

The output of \textit{hmc} is a single list containing the model results.  The most important values provided in the output are the simulated values \textit{thetaDF} and number of accepted proposals \textit{accept}.  

One might notice that none of the above parameters explicitly specify the data or priors.  In this package, the data is passed to the functions indicated with the \textit{logPOSTERIOR} and \textit{glogPOSTERIOR} parameters via the \textit{...} parameter.  Priors should be defined either within the posterior functions or in separate function calls.  The purpose of this design is to maximize flexibility regarding the types of models to fit using this package.  

The default number of simulations is set to 10,000.  However, the user may elect to start with a smaller number during tuning.  Initial values for the parameters must be provided by the user.  Caution should be used to ensure that these initial values are in the support of $\Theta$.  

Defaults are provided for the stepsize $\epsilon$ and leapfrog steps $L$.  These likely will need to be adjusted depending on the particular application.  One simple indicator of whether HMC is sampling correctly is the acceptance rate of the MCMC chain.  In general, acceptance rates between 60\% and 95\% tend to work well for this algorithm [@neal_handbook_2011].  

The acceptance rate can be calculated from the \textit{hmc} function from the number of accepted proposals divided by the number of samples \textit{N}.  Both of these values are included in the output of \textit{hmc}.  

The MCMC simulation values are provided in list and dataframe forms.  In most cases, the dataframe \textit{thetaDF} will be most convenient for exploring posterior simulation values.  The number of columns in the dataframe is equal to the number of parameters defined in the model.  


## Simple example

To illustrate the functionality of the HMC software, we begin with a simple example fitting a Gamma distribution with two parameters $\alpha$ and $\beta$.  The pdf is specified

$$
f(x) = \frac{1}{\beta^\alpha \Gamma(\alpha)} x^{\alpha-1}e^{-x/\beta}I_x(0, \infty)
$$

with likelihood and log likelihood

$$
\begin{aligned}
L(\alpha, \beta; x) &= \prod_{i=1}^n \frac{1}{\beta^\alpha \Gamma(\alpha)} x_i^{\alpha-1}e^{-x_i/\beta} \\
l(\alpha, \beta;x) &= \sum_{i=1}^n -\alpha\log\beta -\log\Gamma(\alpha) + (\alpha-1)\log x_i - x_i/\beta \\
&= -n\alpha\log\beta -n\log\Gamma(\alpha) + (\alpha-1)\sum_{i=1}^n\log x_i - \frac{1}{\beta}\sum_{i=1}^n x_i.
\end{aligned}
$$

Now that the log likelihood has been specified, we can specify our priors.  Both parameters $\alpha$ and $\beta$ are strictly positive.  As such, we can specify half-Normal priors for each of the parameters, with hyperpriors $\eta_1$ and $\eta_2$.

$$
\begin{aligned}
p(\alpha|\eta_1) &= \frac{2\eta_1}{\pi}\exp\left(-\frac{\alpha^2\eta_1^2}{\pi} \right) \\
p(\beta|\eta_2) &= \frac{2\eta_2}{\pi}\exp\left(-\frac{\beta^2\eta_2^2}{\pi} \right)
\end{aligned}
$$

The log posterior is the sum of the log likelihood and the log prior.   

$$
\begin{aligned}
\log p(\alpha, \beta|x) &\propto l(\alpha, \beta; x) + \log p(\alpha,\beta) \\
&\propto  -n\alpha\log\beta -n\log\Gamma(\alpha) + (\alpha-1)\sum_{i=1}^n\log x_i - \frac{1}{\beta}\sum_{i=1}^n x_i - \frac{\alpha^2\eta_1^2}{\pi} - \frac{\beta^2\eta_2^2}{\pi} 
\end{aligned}
$$

Note that we disregard any terms that are not dependent on our parameters of interest $\alpha$ and $\beta$.  These terms are absorbed into the normalizing constant and do not impact the gradient calculation.  

The R functions for the log likelihood and log posterior are provided here.  For our application, our parameter of interest is defined $\Theta := (\alpha, \beta)$.  

```{r, echo=TRUE, eval=TRUE}
# log likelihood of gamma
llgamma <- function(theta, X, y=NULL, Z=NULL) {
  alpha <- theta[1]
  beta <- theta[2]
  n <- length(X)
  -n*alpha*log(beta) - n*log(gamma(alpha)) + (alpha-1)*sum(log(X)) - sum(X)/beta
}

# log posterior of gamma
gamma.lposterior <- function(theta, X, eta1, eta2, y=NULL, Z=NULL) {
  alpha <- theta[1]
  beta <- theta[2]
  llgamma(theta, X) - beta^2 * eta1^2/pi - alpha^2 * eta2^2/pi
}
```


Since our model has two parameters, the partial derivatives must be calculated with respect to $\alpha$ and $\beta$.  

$$
\begin{aligned}
\nabla_\alpha \log p(\alpha, \beta|x) &= -n\log\beta -n\psi(\alpha) + \sum_{i=1}^n\log x_i -2\alpha\eta_1^2/\pi \\
\nabla_\beta \log p(\alpha, \beta|x) &= -\frac{n\alpha}{\beta} + \frac{1}{\beta^2}\sum_{i=1}^n x_i - 2\beta\eta_2^2/\pi
\end{aligned}
$$

The function for the gradient of the log posterior is provided here.

```{r, echo=TRUE, eval=TRUE}
# derivative of posterior
g.gamma.lposterior <- function(theta, X, eta1, eta2) {
  alpha <- theta[1]
  beta <- theta[2]
  n <- length(X)
  dalpha <- -n*log(beta) -n*digamma(alpha) + sum(log(X)) - 2*alpha*eta1^2/pi
  dbeta <- -n*alpha/beta + sum(X)/beta/beta - 2*beta*eta2^2/pi
  c(dalpha, dbeta)
}
```

Now that the log posterior and gradient have been derived and coded in R, we can run the \textit{hmc} function to fit the model. 

First, we simulate data based on a gamma distribution.  Here, we simulate 1000 data points with $\alpha = 2$ and $\beta = 3$. 

```{r, echo=TRUE, eval=TRUE}
# simulate data
set.seed(312)
X <- rgamma(1000, 2, 1/3)
```

Next, we run our $\textit{hmc}$ function on our simulated data.  For this example, we set the initial values $\Theta^{(0)}$ sufficiently away from zero with pre-selected parameters $\epsilon$ and $L$.  This is to prevent the MCMC chain from moving to values less than zero.  

In production software, a transformation is typically performed to the parameters to allow values from the entire real number line (e.g. log transform).  Alternatively, a \textit{constrain} parameter can be set to designate $\alpha$ and $\beta$ as strictly positive.  

```{r, echo=TRUE, eval=TRUE}
N <- 10000
set.seed(143)
ex1 <- hmc(N, theta.init = c(4, 4), epsilon = 2e-2, L = 22, 
                   logPOSTERIOR = gamma.lposterior, 
                   glogPOSTERIOR = g.gamma.lposterior, X=X, 
                   varnames = c("alpha", "beta"),
                   eta1 = 1e-4, eta2 = 1e-4)
```

The \textit{hmclearn} package has a diagnostic plot function to show trace plots and histograms from the simulation.  An optional parameter \textit{actual.mu} can be used to input the true values of the parameters from simulated data.  A default \textit{burnin} period of 100 samples is also selected for these plots.  

Without performing rigorous diagnostics, the trace plots appear stationary with the given burnin period.  The histograms of the simulations also appear well within the range of highest posterior density.  

```{r, echo=TRUE, eval=TRUE}
diagplots(ex1, burnin=100, actual.mu = c(2, 3))

```

A \textit{summary} confirms that the actual parameter values of $\Theta$ fall well within the simulated posterior values.

```{r, echo=TRUE, eval=TRUE}
summary(ex1)
```

In the next section, we will discuss some of the main practical considerations in fitting models using HMC, focusing on parameterization and tuning.  

# HMC Practical Considerations

Parameters for some statistical models are bounded.  When parameters are bounded, the MCMC chain is restricted on simulated values.  Since the HMC algorithm approximates the trajectories through the joint space of the momentum and $\Theta$, it is possible that the chain may move to a location outside of the support of $\Theta$.  

## Constrained Parameters

Parameters for many statistical models are limited to strictly positive numbers.  This was true for the first example using HMC to fit $\alpha$ and $\beta$ from a Gamma distribution.  

We continue our previous example by implementing log transformations for each of these parameters. 

$$
\begin{aligned}
a &= \log\alpha \\
p_a(a) &= p_\alpha(g^{-1}(a))\left|\frac{d\alpha}{da}\right| \\
&= p_\alpha(e^a)e^a \\
\log p_a(a) &= \log p_\alpha(e^a) + a \\
&= \log p_\alpha(\alpha) + \log\alpha
\end{aligned}
$$

The Jacobian term $\log\alpha$ must be included with the prior when employing the log transformation of the original parameter.  A similar transformation is necessary for $\beta$.  

$$
\begin{aligned}
b &= \log\beta \\
\log p_b(b) &= \log p_\beta(\beta) + \log\beta
\end{aligned}
$$

The log posterior can be re-written with the new parameterization.


$$
\begin{aligned}
\log p(a, b|x) &\propto l(\alpha, \beta; x) + \log p(\alpha,\beta) \\
&\propto l(\alpha, \beta; x) + \log p(\alpha) + \log\alpha + \log p(\beta) + \log\beta \\
&\propto  -n\alpha\log\beta -n\log\Gamma(\alpha) + (\alpha-1)\sum_{i=1}^n\log x_i - \frac{1}{\beta}\sum_{i=1}^n x_i - \frac{\alpha^2\eta_1^2}{\pi} - \frac{\beta^2\eta_2^2}{\pi} + \log\alpha + \log\beta
\end{aligned}
$$

The gradient includes the partial derivatives of the Jacobian terms as well.

$$
\begin{aligned}
\nabla_\alpha \log p(a, b|x) &= -n\log\beta -n\psi(\alpha) + \sum_{i=1}^n\log x_i -2\alpha\eta_1^2/\pi + \frac{1}{\alpha}\\
\nabla_\beta \log p(a, b|x) &= -\frac{n\alpha}{\beta} + \frac{1}{\beta^2}\sum_{i=1}^n x_i - 2\beta\eta_2^2/\pi + \frac{1}{\beta}
\end{aligned}
$$

Revised R functions are required for the log posterior and gradient. 

```{r, echo=TRUE, eval=TRUE}

# log likelihood of gamma with log-transformed parameters
llgamma2 <- function(theta, X, y=NULL, Z=NULL) {
  a <- theta[1]
  b <- theta[2]
  alpha <- exp(a)
  beta <- exp(b)
  n <- length(X)
  -n*alpha*log(beta) - n*log(gamma(alpha)) + (alpha-1)*sum(log(X)) - sum(X)/beta
}

# log posterior of gamma with log-transformed parameters
gamma.lposterior2 <- function(theta, X, eta1, eta2, y=NULL, Z=NULL) {
  a <- theta[1]
  b <- theta[2]
  alpha <- exp(a)
  beta <- exp(b)
  llgamma2(theta, X) - beta^2 * eta1^2/pi - alpha^2 * eta2^2/pi + log(alpha) + log(beta)
}

# derivative of posterior with log-transformed parameters
g.gamma.lposterior2 <- function(theta, X, eta1, eta2) {
  a <- theta[1]
  b <- theta[2]
  alpha <- exp(a)
  beta <- exp(b)
  n <- length(X)
  dalpha <- -n*log(beta) -n*digamma(alpha) + sum(log(X)) - 2*alpha*eta1^2/pi + 1/alpha
  dbeta <- -n*alpha/beta + sum(X)/beta/beta - 2*beta*eta2^2/pi + 1/beta
  c(dalpha, dbeta)
}
```

Now, we can safely re-fit the model with parameters that span the full real number line.  Note that we are re-defining $\tilde\Theta := (a, b) = (\log\alpha, \log\beta)$ with initial values at zero.  

```{r, echo=TRUE, eval=TRUE}
N <- 10000
set.seed(143)
ex1b <- hmc(N, theta.init = c(0, 0), epsilon = 1e-3, L = 30, 
                   logPOSTERIOR = gamma.lposterior2, 
                   glogPOSTERIOR = g.gamma.lposterior2, X=X, 
                   varnames = c("a", "b"),
                   eta1 = 1e-4, eta2 = 1e-4)
```

HMC accurately fits the model using the log-transformed parameters, as we can see through the diagnostic plots and summary results.

```{r, echo=TRUE}
diagplots(ex1b, burnin=100, actual.mu = c(log(2), log(3)))

```

```{r, echo=TRUE, eval=TRUE}
summary(ex1b)
```

The exponential of the results confirms that the HMC converges to the correct parameters.  

```{r, echo=TRUE, eval=TRUE}
exp(summary(ex1b))
```

Parameter transformation such as the $log$ transform shown here is the most common method of ensuring that the MCMC simulation is remains in the support of $\Theta$.  An alternate approach to constrained variables was proposed by Neil (cite).  This approach modifies the \textit{leapfrog} function to ensure the proposed $\Theta$ remains within the appropriate support.  A simplifed version of this method is provided in the \textit{hmc} function for strictly positive parameters (i.e. with support $(0, \infty)$).  

\begin{itemize}
\item[] Initial momentum half-step:  $\pmb{p}'(t + \epsilon/2) = \pmb{p}(t) + (\epsilon/2)\nabla_\Theta\log\pi(\Theta(t))$ 
\item[] Initial full-step of $\Theta$:  $\Theta'(t + \epsilon) = \Theta(t) + \epsilon M^{-1}\pmb{p}'(t + \epsilon/2)$
\item[] Check lower constraint of $0$:
\begin{itemize}
\item[] If $\Theta'(t + \epsilon) < 0$ then $p(t + \epsilon/2) = -p'(t + \epsilon/2)$ and  $\Theta(t + \epsilon) = -\Theta'(t + \epsilon)$.  Otherwise, keep the proposed momentum and $\Theta$ as simulated
\end{itemize}
\end{itemize}

## Tuning

Metropolis-Hastings and HMC each utilize parameters that must be tuned to efficiently simulate from the posterior distribution.  This is a matter of more practical importance than theoretical necessity.  In the limit, both MCMC algorithms will converge to the true posterior given infinite samples.  However, the converge rate of these algorithms varies substantially depending on how well the tuning parameters fit the application.

Tuning for random-walk Metropolis (RWM) typically involves adjusting the variance parameterization of the proposal density.  Standardizing the design matrix to a common mean and variance can significantly simplify the tuning exercise.  For example, a multivariate Normal proposal may simply involve adjusting a scaler for the identity covariance matrix.  The analyst typically examines preliminary trace plots along with acceptance rate calculations to assess tuning effectiveness.

HMC can be significantly more complex than RWM due to the number and variety of tuning parameters in the HMC algorithm.  Like RWM with a multivariate Normal proposal, HMC uses a multviariate Normal parameterization for the latent momentum variable $p$.  While HMC may use an identity covariance matrix for $M$, tuning the diagonal and even off-diagonal parameters can improve the efficiency of sampling.  

In addition to the covariance matrix $M$, HMC requires parameter selections for the step size $\epsilon$ and number of leapfrog steps $L$.  Setting $\epsilon$ and $L$ to low values, relative to the particular application, typically result in a high acceptance rate (e.g. greater than 95\%).  On the surface, accepting the vast majority of proposals may appear to be desirable.  However, the combination of a small step size with few leapfrog steps can create a MCMC chain that traverses the support of $\Theta$ very slow.  The evidence of inappropriately small values for these tuning parameters is a trace plot that exhibits a high degree of autocorrelation. 

Generally, the step size should be set as high as possible to balance the acceptance rate and minimize autocorrelation.  An acceptance rate of approximately 60 - 90\% often produces MCMC chains that converge sufficiently quickly.  While $\epsilon$ and $L$ may be tuning jointly, the step size is often selected first, followed by fine-tuning with the number of steps per leapfrog $L$.  

A general approach to tuning HMC is provided here:

\begin{enumerate}
\item Set the stepsize to an initial value such as $\epsilon = 1e-2$ with $L = 10$ and $M$ unit diagonal 
\item Run a preliminary HMC chain and compute the acceptance rate. Adjust $\epsilon$ until the acceptance is between 0.6 and 0.9. 
\item Check the autocorrelation for each parameter either visually or via direct calculation. For parameters with high autocorrelation, reduce the relevant value in the diagonal of $M$. This adjustment may decrease the acceptance rate. 
\item If necessary, increase $L$ to further reduce autocorrelation in the simulation 
\end{enumerate}

Additional adjustments may be made to the tuning parameters beyond these steps.  The value of $\epsilon$ may be constant for each parameter in $(\theta_1, ..., \theta_k)$ in $\Theta$, but does not need to be.  The analyst may instead provide a vector $\epsilon:= \epsilon_1, ..., \epsilon_k$ with a different value for each $\epsilon_i \forall\; i \in 1...k$.  In the \textit{hmclearn} package, the \textit{hmc} function accepts single values and vectors for the parameter \textit{epsilon}.  

The parameter for the number of steps $L$ must be a natural number.  However, the number of steps may be randomized in each simulation $1...N$ to ensure against a periodic Markov chain (which would violate the regularity conditions for ergodicity).  Further, some randomness may be applied to $\epsilon$ in each simulation step $1...N$ for additioanl assurance of an aperiodic chain.  Randomness can be automatically applied for $\epsilon$ and $L$ with logical parameter \textit{randlength} in the \textit{hmc} function.  
In high performance HMC software such as STAN, the popular No-U-Turn Sampler (NUTS) algorithm [@hoffman_no-u-turn_2014] is a popular automated option for selecting $L$.  We avoid using NUTS in this paper as the emphasis here is more pedagogical.  The intent is to provide interested readers with tools to experiment with the core HMC algorithm to gain intuition on the particulars of the algorithm.  

Finally, the gradients of the log posterior must be developed in this software.  However, automated differentiation software such as Autodiff [@carpenter_stan_2015] is available to accurately and efficiently compute gradients for applications in HMC.  Other automated gradient computation software is provided in deep learning platforms such as Tensorflow [@abadi_tensorflow:_2016].  

## QR Decomposition

The efficiency of sampling in the standard HMC algorithm can be improved significantly for multivariate models when the parameters $(\theta_1, ..., \theta_k) \in \Theta$ have an orthogonal basis.  One common method of ensuring an orthogonal basis involves applying QR decomposition [@voss_introduction_2013] to the design matrix prior to applying HMC.  

For example, Stan [@stan_development_team_stan_2017] scales QR decomposition with a factor of $\sqrt{n-1}$ to ensure unit variance for continuous parameters (cite stan manual).  Here $X$ represents the design matrix for the model

$$
\begin{aligned}
X &= QR = Q^*R^* \\
Q^* &= \sqrt{n-1} Q \\
R^* &= \frac{1}{\sqrt{n-1}}R \\
X\Theta &= Q^*R^*\Theta \\
&= Q^* \widetilde{\Theta}
\end{aligned}
$$

The scaled $Q$ matrix is used in the HMC simulation in place of the raw design matrix $X$.  The simulated parameters will then be from $\widetilde{\Theta}$.  To transform back to the original scale, we multiply by the inverse of $R^*$

$$
\begin{aligned}
R^*\Theta &= \widetilde{\Theta}\\
\Theta &= R^{*-1}\widetilde{\Theta}
\end{aligned}
$$

In R software, the base functions \textit{qr} can be used to perform QR decomposition.  The $Q$ and $R$ matrices can be obtained from the R methods \textit{qr.Q} and \textit{qr.R}, respectively, applied to the resulting object. 

# HMC in Statistical Models

With a background of the HMC algorithm and its application to simple examples complete, we turn to more practical, real-world examples.  Generalized Linear Models (GLM) are a broad class of statistical models that use a linear function to relate the mean response to a set of independent variables.  Nonlinear functions of the mean can be accommodated through a specified link function.  While there are the number and types of models to which HMC can be applied are limitless, we will focus on GLM's in this introductory article.  

The major steps required to fit a statistical model are summarized below.  We will elaborate on what is required for each step in this process.  

\begin{enumerate}
\item Specify model
\item Derive log posterior
\item Re-parameterize parameters as needed (i.e. log transforms)
\item Derive gradient of the log posterior
\item Code functions for log posterior and gradient
\item Tune HMC parameters with trial runs
\item Run HMC
\end{enumerate}

## Step 1:  Specify model

The general form of a GLM can be specified (cite Agresti GLM book)

$$
g[E(y)] = \pmb{X}\bm{\beta}
$$

Here, we consider $\bm{\beta}$ our parameter of interest, such that $\Theta := \bm{\beta}$.  The linear predictor of the GLM is the product of the design matrix $(\pmb{x}_1, ..., \pmb{x}_k) \in \pmb{X}$ for $i = 1, .., n$ observations, and the parameter vector $(\beta_1, ..., \beta_k) \in \bm{\beta}$.  A link function $g(\cdot)$ relates the linear predictor to the mean response.  For a linear regression model, $g(\cdot)$ is the identity function, such that $E(y) = \pmb{X}\bm{\beta}$.  

## Step 2:  Derive log posterior

The log posterior is comprised of the sum of the log likelihood of the model and the log prior.    In general, the likelihood can be written based on the inverse of the selected link function.  
$$
f(y; \bm{\beta}) = g^{-1}(\pmb{X}\bm{\beta}) = p(y | \pmb{X}\bm{\beta})
$$

The likelihood functions for standard GLM's are readily available from statistical texts on the subject.  

Once the likelihood is defined, the prior must be selected for the parameter of interest.  Unlike more restrictive algorithms such as Gibbs, the analyst has a substantial degree of flexibility in prior selection for HMC.  Depending on the application, the form of the prior may be defined as restricted within a certain range when known in advance.  

Alternatively, the prior is defined as vague over the support of $\bm{\beta}$.  For example, when $\bm{\beta}$ may be any real number, a vague prior may be a multivariate Normal with high variance.  The resulting prior density would be flat over all practical values of the parameter of interest.

Once the log likelihood is defined and the prior is selected, the log posterior may be written

$$
\log p(\bm\beta | y, \pmb{X}) = \log p(y | \pmb{X}\bm{\beta}) + \log p(\bm{\beta})
$$


## Step 3:  Re-parameterize parameters as needed (i.e. log transforms)

This step should always be considered, but may not be necessary depending on the particular model.  Typically, any standard deviation or variance parameters would be restricted to positive real numbers.  These would need to either be re-parameterized or the constrained HMC leapfrog would need to be used.  

A logistic regression model, for example, would only have the parameter vector for $\bm{\beta}$, for which the support can be the entirely real number line.  A linear regression model has an error term $\sigma^2$ which is strictly positive.  Fitting an HMC without considering the support of $\sigma^2$ could result in the MCMC chain selecting a negative sample.  In such a case, the samples would no longer be valid for the parameter of interest and the algorithm would likely produce an error.  

When re-parameterization is necessary, it is important to include the Jacobian adjustment for new parameter.  Detailed explanation of a change of variables can be found in standard statistical texts.  

## Step 4:  Derive gradient of the log posterior

Since the gradient is a linear operator, the gradient of the log posterior is equivalent to the sum of the gradient of the log likelihood and the gradient of the log prior.  One may derive the gradient of the log likelihood and log prior each individually, in preparation for coding functions for these elements individually.  

Note that if testing multiple prior distributions is desired, the gradient must be derived for each prior distribution to be tested.  

## Step 5:  Code functions for the log posterior and gradient

Once the log posterior and gradient are formed, they must be coded into separate R functions.  This process often requires some iteration and debugging, particularly for complex likelihood functions.  

While the forms of the log likelihood and log prior can often be found from standard sources, the gradient may not be as readily available.  As such, the analyst should test the gradient function at several values to ensure that it is returning the correct result.  

One approach to testing the accuracy of a custom gradient function is to use a package that uses finite differencing or some variant to approximate the gradient given a general function.  The R package \textit{pracma} contains functions that can be useful in this regard.  

While finite differencing is appropriate for checking the accuracy of coding the gradient, such approximations are typically not sufficiently accurate for use in an HMC simulation.  Further, gradient approximation is typically slower than a direct gradient function.  Since the gradient must be calculated for each iteration of the leapfrog algorithm, a slower approximation function can substantially increase the computation time for HMC.  

## Step 6:  Tune HMC parameters with trial runs

Tuning HMC involves adjusting $M$, $\epsilon$, and $L$ for the particular application.  The tuning process detailed in the previous section can be used as a guide to set the parameters.  

Trial runs will have fewer samples than the full simulation, but should have sufficient number of samples to assess the acceptance rate and stationarity of the simulation.  Note that the acceptance rate in trial runs may be higher than the rate during the full simulation.  This can occur when the initial values are distant from the region of high posterior density. 

One common approach in any MCMC is to allow a certain burn-in period before assessing the stationarity of the simulation.  This period can typically be determined with trial trace plots.  The required number of simulations in the burn-in period will typically be inversely related to the combined length defined by the tuning parameters.  Higher values of $\epsilon$ and $L$ will move the chain quickly to the highest posterior region, while lower values will approach more slowly. 

## Step 7:  Run HMC

After the parameters have been tuned, a full HMC simulation can be run.  Analysts often run multiple chains with different initial values.  Most production software packages allow these chains to be run in parallel.   For \textit{hmclearn}, parallelization can be accomplished using contributed R packages for the user's particular platform (e.g. Windows, mac OS, linux).

Once the simulation is complete, readily available diagonstics can be used to assess the convergence of the Markov chain.  In R, the \textit{coda} package provides a number of standard diagnostics, such as the Geweke [@geweke_evaluating_1991] and Gelman and Rubin [@gelman_inference_1992] diagnostics.  The \textit{bayesplot} package also provides graphical functions to assess convergence. 

After the HMC simulation is complete and convergence has been assessed, the simulations can be used for statistical inference.  

# HMC Examples

We demonstrate fitting HMC models in R with three examples of increasing complexity.

## Linear Regression

The linear regression model is this example is specified

$$
y = X\pmb\beta + \sigma
$$

The dependent variable is the vector $(y_1, ..., y_n) \in y$.  The independent variables are contained in the design matrix $\pmb{x}_1, ..., \pmb{x}_p$, with dimensions $N \times p$.  

The model contains two parameters of interest to be fit via HMC, $(\beta_1, ..., \beta_p) \in \pmb{\beta}$ and $\sigma$.  

Next, we specify the likelihood for a linear regression model.  

$$
p(y | X, \beta; \sigma^2) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp{\left(-\frac{1}{2\sigma^2} (y - X\beta)^T(y-X\beta) \right)}
$$

Priors are needed for $\pmb\beta$ and $\sigma$.  We specify a flat uniform prior for $\pmb\beta$ and Inverse Gamma (IG) for $\sigma^2$.  The IG prior has two hyperpriors $a$ and $b$ that will need to be specified.  

$$
\begin{aligned}
p(\beta) &\propto const \\
p(\sigma^2) &\sim IG(a, b) \\
&= \frac{b^a}{\Gamma(a)} (\sigma^2)^{-a-1} \exp\left(-\frac{b}{\sigma^2} \right)
\end{aligned}
$$

The support of $\sigma^2$ is $(0, \infty)$.  We apply a log transformation to expand the support to all real numbers.  The transformed parameter is $\gamma$ which is derived using a single variable transformation.  

$$
\begin{aligned}
\gamma &= \log \sigma^2 \\
\sigma^2 &= g^{-1}(\gamma) = e^\gamma \\
p_\gamma(\gamma) 
&= \frac{b^a}{\Gamma(a)}e^{-a\gamma}\exp\left(-\frac{b}{e^\gamma} \right) \\
\log p(\gamma) &\propto -a\gamma - be^{-\gamma}
\end{aligned}
$$
The log posterior for $\beta$ and $\gamma$ is proportional to the log likelihood plus the log prior.  

$$
\begin{aligned}
\log p(\beta, \gamma | y, X)
&\propto -\left(\frac{n}{2} + a \right)\gamma  -\frac{e^{-\gamma}}{2} (y - X\beta)^T(y-X\beta)- b e^{-\gamma}
\end{aligned}
$$
The gradient of log posterior is needed for the leapfrog function

$$
\begin{aligned}
\nabla_\beta \log p(\beta, \gamma | y, X) &\propto e^{-\gamma}X^T(y - X\beta) \\
\nabla_\gamma \log p(\beta, \gamma | y, X) &\propto -\left(\frac{n}{2} + a \right) 
\end{aligned}
$$
We have now derived everything that is needed to fit a linear regression model.  The dataset \textit{warpbreaks} is available standard with \textit{R}.  
 
```{r, echo=TRUE, eval=TRUE}
head(warpbreaks)
summary(warpbreaks)
```

The dependent variable is stored in $y$.  The design matrix can be constructed using a standard function in \textit{R}.  

```{r, echo=TRUE, eval=TRUE}
y <- warpbreaks$breaks
X <- model.matrix(breaks ~ wool*tension, data=warpbreaks)

```

The vector of parameters of interest is $(\pmb\beta, \gamma) \in \theta$.  The initial values specified in a vector of length 6 for $\beta$ plus 1 for $\gamma$.  The step size is a factor of 10 higher for $\pmb\beta$ than for log transformed variance.  

```{r, echo=TRUE, eval=TRUE}
N <- 1e4
set.seed(143)

eps_vals <- c(rep(2e-1, 6), 2e-2)

t1 <- Sys.time()
  fm1_hmc <- hmc(N, theta.init = c(rep(0, 6), 1), 
                 epsilon = eps_vals, L = 20,
                 logPOSTERIOR = linear_posterior,
                 glogPOSTERIOR = g_linear_posterior,
                 varnames = c(colnames(X), "log_sigma_sq"),
                 param=list(y=y, X=X))
t2 <- Sys.time()
t2 - t1

```

This linear regression model takes approximately 19 seconds to fit on a 2015 Macbook Pro with 2.5GHz i7 processor and 16gb of RAM.  The acceptance rate of this model fit is 96\%, which is appropriate for a relatively simple model such as this one.  


```{r, echo=TRUE, eval=TRUE}
fm1_hmc$accept/N
```

Finally, we can summarize the results and plot the histograms of the simulated posteriors. 

```{r, echo=TRUE, eval=TRUE}
summary(fm1_hmc)
plot(fm1_hmc, burnin=1000)
```


Note that the Inverse Gamma distribution is not always an optimal prior when the support is strictly positive.  This prior can create problematic results when the true value of the parameter is close to zero.  Half-t distributions can be used as a more stable alternative to Inverse Gamma [@gelman2006prior].

## Logistic Regression

Our next example is a slightly more complicated generalized linear model with binary response.  The model is specified

$$
Pr(Y = 1 | X) = [1 + e^{-X\beta}]^{-1}
$$

As in the previous example, the dependent variable is $(y_1, ..., y_n) \in \pmb{y}$ with an $N \times p$ design matrix $(x_1, ..., x_p) \in \pmb{X}$.  

The log-likelihood for a logistic regression model is

$$
\begin{aligned}
l(\beta;X,y) &= \sum_{i=1}^n X_i\beta(y_i - 1) - \log(1 + e^{-X_i\beta})
\end{aligned}
$$

In our prior example, we assigned a uniform prior to $\beta$.  To demonstrate the flexibility of the software, we specify a multivariate Normal prior with hyperparameter $B$.  The hyperprior should be set to a high value such as $1e3$ for a relatively uninformative prior.

$$
\begin{aligned}
\beta &\sim N(0, \Sigma_\beta) \\
&\sim N(0, BI)
\end{aligned}
$$

The log posterior is proportional to the sum of the log likelihood and log prior of $\pmb\beta$.  Note that constants are excluded to simplify the function.  

$$
\begin{aligned}
\log p(\beta | X, y) &\propto \sum_{i=1}^n X_i\beta(y_i - 1) - \log(1 + e^{-X_i\beta}) - \frac{1}{2}\beta^T \Sigma_\beta^{-1} \beta 
\end{aligned}
$$

The gradient of the log posterior is required for the leapfrog function.

$$
\begin{aligned}
\nabla_\beta \log p(\beta | X, y) &\propto (y-1)^TX + \left(\frac{e^{-X\beta}}{1 + e^{-X\beta}}\right)^TX - \Sigma_\beta^{-1}\beta
\end{aligned}
$$

This example is from Hosmer and Lemeshow [@hosmer1989multiple] on a dataset of 189 births at a U.S. hospital.  The dependent variable is an indicator of low birth weight.  Data is available from the MASS package [@venables2013modern].  We prepare the data for analysis as noted in the text. 

```{r, echo=TRUE, eval=TRUE}
birthwt2 <- birthwt

# label race variable
birthwt2$race2 <- factor(birthwt2$race, labels = c("white", "black", "other"))

# reduce to indicator variable for positive number of premature labors
birthwt2$ptd <- ifelse(birthwt2$ptl > 0, 1, 0)

# reduce to three levels
birthwt2$ftv2 <- factor(ifelse(birthwt2$ftv > 2, 2, birthwt2$ftv), 
                        labels = c("0", "1", "2+"))

# create design matrix
X <- model.matrix(low ~ age + lwt + race2 + smoke + ptd + ht + ui + ftv2, 
                  data = birthwt2)
y <- birthwt2$low
```

Variables of interest in this dataset are:

\begin{itemize}
\item low:  birth weight less than 2.5kg (0/1)
\item age:  age of mother (yrs)
\item lwt:  weight of mother (lbs)
\item race2:  factor white/black/other
\item smoke:  smoking indicator (0/1)
\item ptd:  premature labor indicator (0/1)
\item ht:  history of hypertension indicator (0/1)
\item ui:  uterine irritability indicator (0/1)
\item ftv2:  number of physician visits factor (0, 1, 2 or more)
\end{itemize}


Two of the independent variables are continuous with wide ranges of values.  The other nine variables are all dichotomous.  In tuning this model, the step size $\epsilon$ is tuned separately to each of these types of variables.  This example illustrates the need to set the tuning parameters for the specific HMC application.

The log posterior and gradient functions are based on the likelihood and prior choices in this example.  

```{r, echo=TRUE, eval=TRUE}
N <- 1e4

# use different epsilon values for continuous and dichotomous variables
continuous_ind <- c(FALSE, TRUE, TRUE, rep(FALSE, 8))
eps_vals <- ifelse(continuous_ind, 1e-3, 5e-2)

t1 <- Sys.time()
set.seed(143)
fm2_hmc <- hmc(N, theta.init = rep(0, 11), 
               epsilon = eps_vals, L = 10, 
               logPOSTERIOR = logistic_posterior, 
               glogPOSTERIOR = g_logistic_posterior, 
               param=list(y=y, X=X), 
               varnames = colnames(X))
t2 <- Sys.time()
t2 - t1

```

This example is slightly larger than the linear regression, taking approximately 18 seconds to run.  The acceptance rate for this example is 83\%.  


```{r, echo=TRUE, eval=TRUE}
fm2_hmc$accept/N
summary(fm2_hmc, burnin=1000)
plot(fm2_hmc, burnin=1000)

```


## Mixed effects model

This example demonstrates fitting a more complex higher dimension model using HMC.  The distribution is Poisson with the link functon

$$
\begin{aligned}
\mu &:= E(Y | X, Z) = e^{X\beta + Zu} \\
\log \mu &= X\beta + Zu
\end{aligned}
$$
We have $i = 1...N$ observations and $j = 1...M$ groups.  The fixed effects design matrix is $X$ with dimension $N \times p$.  When modeling a random intercept for each group, the random effects design matrix is $Z$ with dimension $N \times M$.  

The likelihood and log likelihood for the Mixed Effects model are based on the Poisson distribution.  

$$
\begin{aligned}
L(\beta; y, X) &= \prod_{i=1}^n \prod_{j=1}^m \frac{e^{-e^{X_i\beta + Z_{ij}u_{ij}}}e^{y_i(X_i\beta + Z_{ij}u_{ij})}}{y_i!} \\
l(\beta; y, X) &=  -e^{\sum_{ij} X\beta + Zu} + y^T(X\beta + Zu)
\end{aligned}
$$

Next we select priors for our parameters of interest $\beta$ and $u$.  

We set a multivariate Normal prior for $\beta$ with diagonal coveriance and hyperparameter $B$.  Let $B = 1e4$ for instance, as a relatively uniformative prior.  The relevant portions of the log prior for $beta$ can be specified.  


$$
\begin{aligned}
\log p(\beta) &\propto  - \frac{1}{2}\beta^T (BI)^{-1} \beta
\end{aligned}
$$


The distribution of the random effects are defined as normal with a mean of zero.  In this example, we assume that the covariance $G$ is diagonal.  


$$
\begin{aligned}
u &\sim N(0, G) \\
&:= D^{1/2} \tau \\
&\sim N(0, D^{1/2} I (D^{1/2})^T) \\
&\sim N(0, D^{1/2} D^{1/2}) \\
&\sim N(0, G)
\end{aligned}
$$

Let $\lambda_k$ where $k = 1, ... p$ denote the diagonal entries of $G^{1/2}$.

$$
G^{1/2} := 
\begin{pmatrix}
\lambda_1 & 0 & ... & 0 \\
0 & \lambda_2 & 0 ... & 0 \\
... & ... & ... & ... \\
0 & 0 & ... & \lambda_M
\end{pmatrix}
$$

We re-parameterize $u$ using a standard normal parameterization we define as $\tau = (\tau_1, ..., \tau_M)$ to improve sampling efficiency [@betancourt_hamiltonian_2013].  Here, $u$ is a deterministic function of $D$ and $\tau$.  The intent of our parameterization is to allow $D$ and $\tau$ to be largely independent in the MCMC sampling.  

$$
\begin{aligned}
\tau &\sim N(0, I_M) \\
u &:= D^{1/2} \tau \\
&\sim N(0, D^{1/2} I (D^{1/2})^T) \\
&\sim N(0, D^{1/2} D^{1/2}) \\
&\sim N(0, G)
\end{aligned}
$$

We assign half-t priors for each of the parameters $\lambda_1, ..., \lambda_M$ with hyperparameters $\nu$ and $A$.  

$$
\begin{aligned}
p(\lambda_k) &\sim \left(1 + \frac{1}{\nu}\left(\frac{\lambda_k}{A} \right)^2 \right)^{-(\nu+1)/2} 
\end{aligned}
$$

We apply a log transformation to $\lambda_k$ where $\xi_k := \log \lambda_k$ where $k = 1...M$.  

$$
\begin{aligned}
p(\lambda_k) &\sim \left(1 + \frac{1}{\nu}\left(\frac{\lambda_k}{A} \right)^2 \right)^{-(\nu+1)/2} \\
p_{\xi_k}(\xi_k) &= p_{\lambda_k}(g^{-1}(\xi_k)) \left\lvert \frac{d\lambda_k}{d\xi_k}  \right\rvert \\
&= p_{\lambda_k} (e^{\xi_k})\lvert e^{\xi_k}\rvert \\
&\propto \left(1 + \frac{1}{\nu}\left(\frac{e^{2\xi_k}}{A^2} \right) \right)^{-(\nu+1)/2} e^{\xi_k}\\
\log p(\xi_k) &\propto -\frac{\nu+1}{2}\log\left(1 + \frac{1}{\nu}\left(\frac{e^{2\xi_k}}{A^2} \right) \right) + \xi_k 
\end{aligned}
$$

With the log likelihood and priors defined, we can specify the log posterior for HMC.

$$
\begin{aligned}
\log p(\beta, \gamma, \tau, \xi | y, X, Z) &\propto \log p(y | \beta, \tau, \gamma, \xi) + \log p(\beta) + \log p(\gamma)+ \log p(\tau) + \log p(\xi) \\
&\propto  -e^{\sum_{ij} X\beta + ZD^{1/2}\tau} + y^T(X\beta + ZD^{1/2}\tau)  - \frac{1}{2}\beta^T(BI)^{-1} \beta \\
&-\sum_{k=1}^M \left(\frac{\nu_{\lambda_k}+1}{2}\log\left(1 + \frac{1}{\nu_{\lambda_k}} 
\frac{e^{2\xi_k}}{A_{\lambda_k}^2}\right)+ \xi_k\right) -\frac{1}{2} \tau^T \tau
\end{aligned}
$$


We need to derive the gradient of the log posterior for the leapfrog function.  Let $J^{kk}$ be the single element version of the diagonal matrix $D^{1/2}$ retaining only a single $e^{\xi_k}$ and the remainder of the diagonal zero.

$$
\begin{aligned}
\frac{\partial l}{\partial \beta} &\propto -\left(e^{\sum_{ij}X\beta + ZLD^{1/2}\tau} \right)^T X + y^T X - \Sigma_\beta^{-1} \beta \\
\frac{\partial l}{\partial \tau} &\propto -\left(e^{\sum_{ij}X\beta + ZLD^{1/2}\tau}  \right) ZLD^{1/2} + y^T ZLD^{1/2} - \tau \\
\frac{\partial l}{\partial \xi_k} &\propto -\left(e^{\sum_{ij}X\beta + ZLD^{1/2}\tau}  \right) ZLJ^{kk}\tau + y^TZLJ^{kk}\tau - \frac{\nu_{\lambda_k} +1}{1 + \nu_{\lambda_k}A_{\lambda_k}^2 e^{-2\xi_k}} + 1 
\end{aligned}
$$

Our example comes from a study on gopher tortoises [@ozgul2009upper][@fox2015ecological][@bolkergithub]

As a comparison, we fit the model using the Frequentist package \textit{lme4} [@bates2007lme4]

```{r, echo=TRUE, eval=TRUE}
library(lme4)

# Gopher data:  Gdat dataframe
data(gopherdat2)

# omit area offset
fm3 <- glmer(shells~prev+factor(year)+(1|Site),
      family=poisson,data=Gdat,
      control=glmerControl(optimizer="bobyqa",
                           check.conv.grad=.makeCC("warning",0.05)))


summary(fm3)

```

Next, we store Frequentist estimates in R variables. 

```{r, echo=TRUE, eval=TRUE}
truevals_fixed <- c(fixef(fm3))
truevals_fixed <- truevals_fixed[c(1, 3, 4, 2)]
truevals_random <- as.numeric(ranef(fm3)$Site[, 1])
truevals <- c(truevals_fixed, truevals_random, 
              log(sqrt(as.numeric(VarCorr(fm3)$Site[, 1]))))
  # sqrt(as.numeric(VarCorr(fm3)$Site[, 1])))
nvar <- length(truevals)

```

The design matrices $X$ and $Z$ must be setup for \textit{hmc}.  

```{r, echo=TRUE, eval=TRUE}
library(Matrix)

##########
# block diagonal
Zi.lst <- split(rep(1, nrow(Gdat)), Gdat$Site)
Zi.lst <- lapply(Zi.lst, as.matrix)
Z <- bdiag(Zi.lst)
Z <- as.matrix(Z)
X <- model.matrix(~ factor(year), data=Gdat)
X <- cbind(X, Gdat$prev)
colnames(X)[ncol(X)] <- "prev"
colnames(X) <- make.names(colnames(X))
colnames(X)[1] <- "intercept"
y <- Gdat$shells
p <- ncol(X)

```

Finally, we run HMC for the Poisson mixed effects model.  Note that we use a slightly informative prior on the random effects variance, a half-t prior with 4 degrees of freedom and scale parameter of 1. This facilitates converge of the HMC while allowing for a range of all reasonable values of the random effects variance.  

Since this is a more complex example, we increase the number of samples to allow the MCMC chain more time to converge.  

```{r, echo=TRUE, eval=TRUE}
N <- 1e5

set.seed(412)
initvals <- c(rep(0, 4), 
                rnorm(10, mean=0, sd=1e-3), 
                0)

M_vals <- c(1e-3, 1e-3, 1e-3, 1, 
            rep(1e-3, 10), 
            1e-3)


eps_vals <- c(rep(1e-3, 3), 1e-4, rep(2e-4, 11))

t1.hmc <- Sys.time()
res <- hmc(N = N, theta.init = initvals, epsilon = eps_vals, L = 10, 
                    logPOSTERIOR  = glmm_poisson_posterior, 
                    Mdiag = M_vals, 
                    varnames=c(colnames(X), paste0("u", 1:ncol(Z)), "lambda"),
                    glogPOSTERIOR  = g_glmm_poisson_posterior, 
                    y = y, X=X, Z=Z, m=10, nulambda=1, Alambda=25) 
t2.hmc <- Sys.time()
t2.hmc - t1.hmc
res$accept/N
```

```{r, echo=TRUE, eval=TRUE}
summary(res)

```


```{r,echo=TRUE, eval=TRUE}
diagplots(res, burnin=1000, actual.mu = truevals_fixed, cols=1:4)
```

# Discussion

Hamiltonian Monte Carlo is a flexible MCMC algorithm that efficiently samples from even complex posterior distributions.  While Gibbs requires at least conditional conjugate priors, HMC may be applied to virtually any model with well-behaved properties such as a twice differentiable continuous distribution.  Analysts are, as a result, able to specify models and priors based entirely on the particular application without artificial mathematical restrictions.  

While there are many algorithms that use the gradient for optimization (e.g. stochastic gradient descent), HMC uniquely applies the gradient to sample from the entire distribution.  The result is an accurate estimate of the full posterior distribution, not merely a single point.  This distribution can be intuitively applied to answer a variety of inference, estimation, and prediction questions.  

This software can be used used to fit a wide variety models using HMC provided functions for the log posterior and its gradient can be developed.  Since the software is open source, analysts with a background in R can easily examine the functions to increase their understanding of the details behind HMC.  This intuition can be beneficial when using STAN and other HMC software with high-performance compiled code that may be more difficult to interpret.  

The HMC algorithm as applied in this software represents only the beginning of its potential application for big data analysis.  This algorithm uses only the first derivative in guiding the Markov chain.  A more advanced version called Riemannian HMC [@girolami_riemann_2011] incorporates the second derivative along with a more sophisticated leapfrog function.  This algorithm can be used to complex, correlated spaces without the need for transformations such as QR decomposition to produce an orthogonal space.  As of this writing, the STAN team has already developed a substantial amount of development for generalized Riemannian HMC software [@gelman_stan:_2015][@betancourt_general_2013].   

Finally, on the computational hardware side parallelization for most available software is currently limited to the embarrassingly parallel application of running multiple HMC chains over the available CPU's.  The incorporation of GPU's in improving the efficiency of HMC represents a new frontier in high performance statistical computation.  One study found that GPU's could be used to reduce computation time when evaluating the posterior as well as its gradient [@beam2016fast].  NVIDIA is sponsoring a grant to support the STAN team in its HMC software development using GPU's [@gelmannvidia].  

# References


