---
title: "HMC Learn"
output: pdf_document
vignette: >
  %\VignetteIndexEntry{HMC_learn}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
header-includes:
  - \usepackage{setspace}
  - \doublespacing
  - \usepackage{amsmath}
  - \usepackage{algorithm}
  - \usepackage{algpseudocode}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
# library(hmclearn)
```

# Introduction

Hamiltonian Monte Carlo (HMC) has emerged as a general purpose tool for Bayesian practitioners.  A key advantage of HMC over more traditional Markov Chain Monte Carlo (MCMC) algorithms is its improved computational efficiency in fitting high-dimensional models.   While the algorithm itself is not difficult to program, the substantial number of tuning parameters can be daunting to those unfamiliar with the theory behind the method.  Until recently, practical access to HMC was limited to individuals with both the mathematical background to understand the algorithm and the programming skill to implement the simulation in a high-performance environment.   

Modern Bayesian software such as Stan has made HMC accessible to practitioners who are comfortable with any one of a variety of well-known programming platforms (e.g. R, Python, Matlab).  The Stan language is similar in style to WinBUGS, which is familiar to many Bayesian statisticians.  The software translates Stan code to a lower-level language to maximize speed and efficiency.  In addition, Stan automates the challenging process of tuning the many parameters in HMC.  As a result, Stan has succeeded in making HMC accessible to many Bayesian practitioners around the world in both academia and industry.  

While Stan and other high-performance software (e.g. PyMC, Edward) provide enormous practical value to analysts, the intuition of how HMC works can be lost in the process of fitting models.  HMC can appear to be an opaque, "black-box" algorithm behind the sophisticated automation.  This is an unfortunate consequence.  While understanding HMC is not necessary to use production software, intuition behind the method can be helpful both in fine-tuning the simulation process and in instilling confidence in the results.  

The purpose of this paper is to introduce HMC to analysts using R software only, an open-source statistical environment that is familiar to many.  While many excellent introductions to HMC are available on a conceptual level, this paper will focus on learning HMC by doing.  Familiarity with popular MCMC algorithms such as Metropolis-Hastings (MH) is helpful, but not required.  A companion R package called hmclearn contains the R code for all of the functions used in this introduction is freely available to download.  

# MCMC Basic Concepts

We consider $n$ observations from a simple random sample $\pmb{X} = (X_1, ..., X_n)$, where each element is independent and identically distributed (iid).  From this sample, we want to fit the distribution of our k-dimensional parameter of interest $\Theta = (\theta_1, ..., \theta_k)$.  The posterior distribution $p(\Theta | \pmb{X})$ can be written as a function of the Likelihood $p(\pmb{X} | \Theta)$ and prior $p(\Theta)$ using Bayes formula.  

$$
\begin{aligned}
p(\Theta | \pmb{X}) &= \frac{p(\pmb{X}|\Theta)p(\Theta)}{\int p(\pmb{X}|\Theta)p(\Theta)d\Theta} \\
&\propto p(\pmb{X}|\Theta) p(\Theta)
\end{aligned}
$$

In many practical data analyses, the integral in the denominator cannot be evaluated directly.  Since the denominator is constant with respect to $\Theta$, only the unnormalized posterior $p(\pmb{X} | \Theta)$ is available.  

## Metropolis-Hastings

The first widely-used MCMC method capable of simulating directly from $p(\pmb{X} | \Theta)$ is called the Metropolis algorithm, originating in the 1950's from an application to statistical physics.  Nearly two decades later, Hastings generalized the algorithm, which is now called Metropolis-Hastings (MH).  We begin with a brief introduction to MH since HMC builds on many similar concepts.  

The objective of MH is to simulate values of $\Theta$ that accurately reflect the posterior density $p(\Theta | \pmb{X})$.  For brevity, we will shorten our notation of the posterior as $p(\Theta)$. The Markov chain simulates values from this density in sequence from $t = 1, .., N$, provided some starting point $\Theta^{(0)}$ which is typically provided by the analyst or the computer program.  
MH defines a transition probability that produces a Markov chain that is ergodic and satisfies detailed balance.  Values of $\Theta^{(t)}$ in the chain are defined in part by a proposal density, which we define as $q(\Theta^{\text{PROP}} | \Theta^{t-1})$.  Here, $\Theta^{\text{PROP}}$ is a proposal for the next value in the chain.  This proposal density is conditioned on the previously stored value $\Theta^{(t-1)}$.  A variety of proposal functions can be used, with random walk proposals being a common choice.  We now outline the MH algorithm in full.  

\begin{algorithm}
\caption{Metropolis-Hastings}\label{Metropolis-Hastings}
\begin{algorithmic}[1]
\Procedure{MH}{$\theta^{(0)}, \pi^{*}(\theta), q(\theta^{(x)}|\theta^{(y)}), N$} 
   \State Calculate $\pi^{*}(\theta^{(0)})$ \Comment{Initial value for posterior}
   \For{$t = 1, ..., N$}\Comment{Repeat simulation $N$ times}
      \State $\theta^{\text{PROP}} \gets q(\theta^{\text{PROP}} | \theta^{(t-1)})$ \Comment{Randomly sample proposal}
      \State $u \gets U(0, 1)$ \Comment{Randomly sample from a uniform density, 0 to 1}
      \State $\alpha = \min\left(1, \frac{\pi(\theta^{\text{PROP}})q(\theta^{\text{PROP}}|\theta^{(t-1)})}{\pi(\theta^{(t-1)})q(\theta^{(t-1)}|\theta^{\text{PROP}})} \right)$ \Comment{Calculate acceptance proposal probability}
      \State If $\alpha < u$, then $\theta^{(t)} \gets \theta^{\text{PROP}}$.  Otherwise, $\theta^{(t)} \gets \theta^{(t-1)}$ \Comment{Select proposal or previous value}
   \EndFor\label{markovendfor}
   \State \textbf{return} $\theta^{(1)} ... \theta^{(N)}$ \Comment{Return simulated values of $\theta$ from the unnormalized posterior}
\EndProcedure
\end{algorithmic}
\end{algorithm}

Each proposal in MH is accepted at probability

$$
\alpha = \min\left(1, \frac{p(\Theta^{\text{PROP}})q(\Theta^{\text{PROP}}|\Theta^{(t-1)})}{p(\Theta^{(t-1)})q(\Theta^{(t-1)}|\Theta^{\text{PROP}})} \right)
$$

which simplifies when $q(\Theta^{PROP} | \Theta^{(t-1)})$ is symmetric (i.e. the Metropolis algorithm)

$$
\alpha = \min\left(1, \frac{p(\Theta^{\text{PROP}})}{p(\Theta^{(t-1)})} \right).
$$

Recall that the denominator of the posterior is constant with respect to $\Theta$.  As such, the ratio of posterior densities at two different points $\Theta^{PROP}$ and $\Theta^{(t-1)}$ can be formulated even when the denominator is unknown (i.e. the constants in the denominator cancel).  

Intuition into why MH works can be obtained by examining the acceptance ratio $\alpha$ closely.  Two different outcomes are possible depending on the value of the posterior at the proposed $\Theta^{PROP}$:

\begin{enumerate}
\item If $p(\Theta^{PROP}) \geq p(\Theta^{(t-1)})$, then the posterior has a higher density at the proposed value of $\Theta$ than at the previous point in the chain $t-1$.  When this occurs, the proposal is always accepted (i.e. at probability 1).  
\item If $p(\Theta^{PROP}) < p(\Theta^{(t-1)})$, then the posterior has a lower density at the proposed value of $\Theta$ than at the previous point in the chain.  When this occurs, we accept the proposal at random based on the ratio $0 < \alpha < 1$.  If the proposal is not accepted, then the proposal is discarded and the Markov chain remains in place $\Theta^{t} := \Theta^{(t-1)}$.  
\end{enumerate}

As such, MH tends to sample more points in the region of higher posterior values.  However, the tails of the posterior are also sampled based on acceptance ratio.  Given enough samples, the MCMC chain samples $\Theta$ at the proportion of the true posterior density.  The resulting simulated values can then be used for statistical inference.  Much more can be said regarding MH.  Interested readers can refer to the following references ... (list references here)

## Limitations of Metropolis-Hastings

The theoretical requirements for fitting models using MH are minimal, making MH an attractive choice for Bayesian inference even today.  The limits of MH are primarily computational in nature.  Since the proposals of $\Theta$ are randomly selected, many simulations are required to accurately describe the true posterior.  Even efficient MH implementations may accept less than 25\% of the proposals (cite Gelman).  

The limited efficiency of MH can be overcome by high computational power for certain applications.  When the dimensionality of the data is small to moderate, a well-programmed MH algorithm can sample enough points from the posterior density in a reasonable period of time.  The challenge of relying on MH occurs when dealing with high-dimensional data or complex statistical models.  In these situations, MH is known to be inefficient (reference here) and can be impractical for such applications.  

A popular, often efficient alternative to MH is Gibbs Sampling (footnote that Gibbs is a particular case of MH?).  Gibbs is widely used in many Bayesian software platforms such as WinBUGS and JAGS.  When the conditional posterior densities can be explicitly formulated, Gibbs remains a viable choice for the Bayesian practitioner.  Such restrictions limit the application of Gibbs to particular combinations of models and priors.  Gibbs therefore lacks the flexibility of MH, in addition to having certain other efficiency limitations of its own (cite Robert).  

Given the modern computational demands of large dataset and complex models, a more efficient MCMC algorithm is desirable.  Ideally, such an algorithm would retain the theoretical advantages and flexibility of MH, while providing a more computational efficient method of selecting proposals.  Here we transition to the modern HMC algorithm, and why HMC has emerged as a standard inferential tool for many Bayesian practitioners.  

# HMC Background

MH and HMC are equally flexible in their theoretical capabilities to fit a variety of model and prior specifications.  The key advantage of HMC over MH is the use of additional information from the posterior to guide proposals.  HMC uses the gradient of the log posterior to direct the Markov chain to the region the region of highest posterior density, where most of the samples should occur.  In contrast, MH relies entirely on the acceptance ratio to guide the chain.   As a result, a well-tuned HMC chain will accept proposals approximately 3 times the frequency of a similarly well-tuned MH algorithm (footnote?:  based on MH theoretical optimal acceptance rate of 0.234 and HMC acceptance of 0.6-0.9).   

It should be noted that HMC is a MCMC simulation method, not an optimization method.  While the HMC algorithm guides proposals to regions of high density (sometimes called the typical set  Betancourt), the tails of the density are properly sampled as well.  Both MH and HMC produce ergodic Markov chains, but the mathematics of HMC is substantially more complex than MH.  We provide a brief overview of the theoretical basis of HMC here, and refer to other sources for more detailed expositions (refer to Wiley paper, Betancourt).

## Hamiltonian Monte Carlo Concepts and Theory

As with MH, our objective in HMC is to simulate the posterior $p(\Theta | \pmb{X})$.  The mathematical basis of HMC is the Hamiltonian function

$$
\begin{aligned}
H(\pmb{p}, \Theta) &= K(\pmb{p}, \Theta) + U(\Theta) \\
&= -\log p(\pmb{p}) - \log p(\Theta).
\end{aligned}
$$

HMC introduces a latent parameter $\pmb{p} = (p_1, ..., p_k)$ of the same length $k$ as the parameter of interest $\Theta = (\theta_1, ..., \theta_k )$.  The latent parameter $\pmb{p}$ is often called the \textit{momentum} based on its original application to physical laws of motion.  The incorporation of the momentum provides the geometrical structure of the space through which the Markov chain travels.  The purpose of the momentum is to ensure that the MCMC simulation is ergodic, covering the entire space of $\Theta$.  

The distribution of $\pmb{p}$ is most often specified to be multivariate Normal with covariance $M$.  The structure of $M$ is often diagonal, but does not have to be.  

$$
\begin{aligned}
\pmb{p} &\sim N_k(0, M) \\
\log p(\pmb{p}) &\propto \frac{1}{2}\pmb{p}^T M^{-1}\pmb{p}
\end{aligned}
$$

The Hamiltonian function represents the joint distribution of the multivariate Normal $\pmb{p}$ and the log posterior $p(\Theta | \pmb{X})$, whose notation we simplify to $p(\Theta)$.

$$
\begin{aligned}
H(\pmb{p}, \Theta) &= -\frac{1}{2}\pmb{p}^T M^{-1} \pmb{p} - \log p(\Theta)
\end{aligned}
$$
The trajectories over which HMC travels in time are defined by first-order differential equations, 

$$
\begin{aligned}
\frac{d\pmb{p}}{dt} &= -\frac{\partial H(\Theta, \pmb{p})}{\partial\Theta}= -\frac{\partial U(\Theta)}{\partial\Theta} = \nabla_\Theta \log p(\Theta) \\
\frac{d\Theta}{dt} &= \frac{\partial H(\Theta, \pmb{p})}{\partial\pmb{p}} = \frac{\partial K(\pmb{p})}{\partial\pmb{p}} = M^{-1}\pmb{p}
\end{aligned}
$$
Here we see the explicit formulation of the gradient of the log posterior $\nabla_\Theta \log p(\Theta)$.  The combination of these two equations forms deterministic paths for the MCMC chain to travel through the joint distribution $(\pmb{p}, \Theta)$.  Note that while the MCMC produces simulations of both $\pmb{p}$ and $\Theta$, only the values of $\Theta$ are of interest for statistical inference.  

A solution for these differential equations is necessary to produce a practical MCMC simulation.  Since no exact solution exists, a discrete approximation is needed to form the algorithm.  The most commonly used solution is called the leapfrog method.  The leapfrog defines a discrete step size $\epsilon$ individually for $\pmb{p}$ and $\Theta$.  

$$
\begin{aligned}
\pmb{p}(t + \epsilon/2) &= \pmb{p}(t) + (\epsilon/2)\nabla_\Theta\log\pi(\Theta(t)) \\
\Theta(t + \epsilon) &= \Theta(t) + \epsilon M^{-1}\pmb{p}(t + \epsilon/2) \\
\pmb{p}(t + \epsilon) &= \pmb{p}(t + \epsilon/2) + (\epsilon/2)\nabla_\Theta \log\pi(\Theta(t + \epsilon)).
\end{aligned}
$$

The full step $\epsilon$ in $\Theta$ is sandwiched by half-steps $\epsilon/2$ for $\pmb{p}$. The number of steps $\epsilon$ in an actual HMC algorithm is typically defined by a tuning parameter $L > 1$.    This leapfrog approximation, though slightly more complex than Euler approximations, provides a more accurate approximation of the solution over the many samples that HMC requires.  

The paths defined by the Hamiltonian equations are deterministic once $\pmb{p}$ is defined.  An exact solution would always be accepted in a MCMC algorithm.  However, since the solution to the Hamiltonian equations is an approximation, a Metropolis-Hastings style acceptance ratio is used to formally determine whether a proposal is accepted or rejected.

With the overview of the major concepts in HMC complete, we turn to the formulation of the Hamiltonian Monte Carlo algorithm itself.  

## Hamiltonian Monte Carlo Algorithm

The HMC algorithm is presented in full here.  

\begin{algorithm}
\caption{Euclidean Hamiltonian Monte Carlo}\label{EHMC}
\begin{algorithmic}[1]
\Procedure{EHMC}{$\Theta^{(0)}, \log\pi(\Theta), M, N, \epsilon, L$} 
   \State Calculate $\log\pi(\Theta^{(0)})$ \Comment{Initial value for log posterior}
   \For{$t = 1, ..., N$}\Comment{Repeat simulation $N$ times}
      \State $p^0 \gets N(0, M)$ \Comment{Randomly sample momentum from MVN}
      \State $\Theta^{(t)} \gets \Theta^{(t-1)}, \tilde{\Theta} \gets \Theta^{(t-1)}, \tilde{p} \gets p^{(0)}$ \Comment{Randomly sample from a uniform density, 0 to 1}
      \For{$i = 1, ..., L$}\Comment{Run Leapfrog $L$ times}
        \State $\tilde\Theta, \tilde{p} \gets \text{Leapfrog}(\tilde\Theta, \tilde{p}, \epsilon)$
      \EndFor\label{leapfrogfor}
      \State $\alpha = \min{\left(1, \frac{\exp(\log\tilde\Theta) - \frac{1}{2}\tilde{p}\cdot\tilde{p}}{\exp(\log\tilde\Theta^{(t-1)}) - \frac{1}{2}p^0 \cdot p^0} \right)}$ \Comment{Calculate acceptance proposal probability}
      \State With probability $\alpha$, $\Theta^{(t)}\gets\tilde\Theta$ and $p^{(t)} \gets -\tilde{p}$ 
   \EndFor\label{ehmcendfor}
   \State \textbf{return} $\Theta^{(1)} ... \Theta^{(N)}$ \Comment{Return simulated values of $\Theta$ from the unnormalized posterior}
\Function{Leapfrog}{$\Theta, p, \epsilon$}
\State $\tilde{p} \gets p + (\epsilon/2)\nabla_\Theta\log\pi(\Theta)$
\State $\tilde\Theta \gets \Theta + \epsilon\tilde{p}$
\State $\tilde{p} \gets \tilde{p} + (\epsilon/2)\nabla_\Theta\log\pi(\Theta)$
\State \textbf{return} $\tilde\Theta, \tilde{p}$
\EndFunction
\EndProcedure
\end{algorithmic}
\end{algorithm}

One may notice some similarities with MH:  

\begin{enumerate}
\item An initial set of values $\Theta^{(0)}$ is required for both MH and HMC.
\item The random walk Metropolis proposal is often Multivariate Normal.  HMC simulates from the momentum $\pmb{p}$ from a Multivariate Normal.  
\item Proposals are accepted or rejected based on a ratio of log posteriors in both MH and HMC.  
\end{enumerate}

The key functional difference between the two algorithms is the leapfrog update process.  HMC generates a proposal $(\pmb{p}^t, \Theta^t)$ based on the randomly selected values of $\pmb{p}$ and the number of leapfrog steps $L$.  In contrast, MH selects a proposal from one simple randomly selected value from a proposal distribution.   MH is therefore computationally simpler, but undirected.  HMC's proposal combines the randomization provided by $\pmb{p}$ with trajectories defined the by the gradient of the log posterior $\nabla_\Theta\log\pi(\Theta(t))$. 

From an implementation standpoint, the additional programming required for HMC is fairly minimal.  Manually determiming the gradient may be tedious, but the derivation is typically not difficult.  Once the gradient is provided and coded, programming the leapfrog loop itself is straightforward.  

The challenge of practical HMC implementation is related to implementation of MH - tuning.  While tuning MH involves adjusting a proposal density and little else, HMC requires the selection and adjustment of the mass matrix $M$, number of leapfrog steps $L$, and step size $\epsilon$.  

While $M$ can be fixed as an identity matrix, some modification of the diagonal, at a minimum, can provide notable improvements in computational efficiency.  The selection of $L$ is also an essential step in HMC implementation.  Selecting $L$ too low will produce a Markov chain that is inefficient, and may behave in pattern similar to random walk Metropolis, but with the penalty of additional computation.  Setting $L$ too high can also be inefficient, where the chain may travel through the entire deterministic trajectory multiple times before a proposal is selected.  Finally, $\epsilon$ can be set as a single constant for all $k$ parameters, but varying $\epsilon$ for each individual parameter may improve computational efficiency.  

Developing an intuition and appreciation for how HMC works can be challenging without direct experience working with the algorithm.  The next section provides R code and some simple examples for the interested reader to gain practical, hands-on experience with implementing and tuning HMC.  

# HMC Implementation



