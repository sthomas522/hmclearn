
@article{bou-rabee_randomized_2015,
	title = {Randomized Hamiltonian Monte Carlo},
	url = {http://arxiv.org/abs/1511.09382},
	abstract = {Tuning the durations of the Hamiltonian flow in Hamiltonian Monte Carlo (also called Hybrid Monte Carlo) ({HMC}) involves a tradeoff between computational cost and sampling quality, which is typically challenging to resolve in a satisfactory way. In this article we present and analyze a randomized {HMC} method ({RHMC}), in which these durations are i.i.d. exponential random variables whose mean is a free parameter. We focus on the small time step size limit, where the algorithm is rejection-free and the computational cost is proportional to the mean duration. In this limit, we prove that {RHMC} is geometrically ergodic under the same conditions that imply geometric ergodicity of the solution to underdamped Langevin equations. Moreover, in the context of a multi-dimensional Gaussian distribution, we prove that the sampling efficiency of {RHMC}, unlike that of constant duration {HMC}, behaves in a regular way. This regularity is also verified numerically in non-Gaussian target distributions. Finally we suggest variants of {RHMC} for which the time step size is not required to be small.},
	journaltitle = {{arXiv}:1511.09382 [math]},
	author = {Bou-Rabee, Nawaf and Sanz-Serna, Jesus Maria},
	urldate = {2017-08-08},
	date = {2015-11-30},
	eprinttype = {arxiv},
	eprint = {1511.09382},
	keywords = {60J25, 62D05, 60H30, 60H35, 37A50, Mathematics - Probability},
	file = {arXiv\:1511.09382 PDF:/home/samuel/Zotero/storage/I35V2HQN/Bou-Rabee and Sanz-Serna - 2015 - Randomized Hamiltonian Monte Carlo.pdf:application/pdf}
}

@article{carpenter_stan_2015,
  title={The {Stan} {Math} {Library}: {Reverse}-{Mode} {Automatic} {Differentiation} in {C}++},
  author={Carpenter, Bob and Hoffman, Matthew D and Brubaker, Marcus and Lee, Daniel and Li, Peter and Betancourt, Michael},
  journal={arXiv preprint arXiv:1509.07164},
  year={2015}
}

@incollection{kucukelbir_automatic_2015,
	title = {Automatic Variational Inference in Stan},
	url = {http://papers.nips.cc/paper/5758-automatic-variational-inference-in-stan.pdf},
	pages = {568--576},
	booktitle = {Advances in Neural Information Processing Systems 28},
	publisher = {Curran Associates, Inc.},
	author = {Kucukelbir, Alp and Ranganath, Rajesh and Gelman, Andrew and Blei, David},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	urldate = {2017-08-08},
	date = {2015},
	file = {NIPS Full Text PDF:/home/samuel/Zotero/storage/WHUZ2JAA/Kucukelbir et al. - 2015 - Automatic Variational Inference in Stan.pdf:application/pdf;NIPS Snapshort:/home/samuel/Zotero/storage/77RALRL9/5758-automatic-variational-inference-in-stan.html:text/html}
}

@article{betancourt_optimizing_2014,
	title = {Optimizing The Integrator Step Size for Hamiltonian Monte Carlo},
	url = {http://arxiv.org/abs/1411.6669},
	abstract = {Hamiltonian Monte Carlo can provide powerful inference in complex statistical problems, but ultimately its performance is sensitive to various tuning parameters. In this paper we use the underlying geometry of Hamiltonian Monte Carlo to construct a universal optimization criteria for tuning the step size of the symplectic integrator crucial to any implementation of the algorithm as well as diagnostics to monitor for any signs of invalidity. An immediate outcome of this result is that the suggested target average acceptance probability of 0.651 can be relaxed to \$0.6 {\textbackslash}lesssim a {\textbackslash}lesssim 0.9\$ with larger values more robust in practice.},
	journaltitle = {{arXiv}:1411.6669 [math, stat]},
	author = {Betancourt, M. J. and Byrne, Simon and Girolami, Mark},
	urldate = {2017-08-08},
	date = {2014-11-24},
	eprinttype = {arxiv},
	eprint = {1411.6669},
	keywords = {Mathematics - Statistics Theory, Statistics - Methodology},
	file = {arXiv\:1411.6669 PDF:/home/samuel/Zotero/storage/QMU4LE34/Betancourt et al. - 2014 - Optimizing The Integrator Step Size for Hamiltonia.pdf:application/pdf}
}

@article{betancourt_geometric_2017,
	title = {The {Geometric} {Foundations} of {Hamiltonian} {Monte} {Carlo}},
	volume = {23},
	issn = {1350-7265},
	url = {http://projecteuclid.org/euclid.bj/1494316818},
	doi = {10.3150/16-BEJ810},
	abstract = {Although Hamiltonian Monte Carlo has proven an empirical success, the lack of a rigorous theoretical understanding of the algorithm has in many ways impeded both principled developments of the method and use of the algorithm in practice. In this paper, we develop the formal foundations of the algorithm through the construction of measures on smooth manifolds, and demonstrate how the theory naturally identifies efficient implementations and motivates promising generalizations.},
	pages = {2257--2298},
	number = {4},
	journaltitle = {Bernoulli},
	shortjournal = {Bernoulli},
	author = {Betancourt, Michael and Byrne, Simon and Livingstone, Sam and Girolami, Mark},
	date = {2017-11-01},
	keywords = {differential geometry, disintegration, fiber bundle, Hamiltonian Monte Carlo, Markov chain Monte Carlo, Riemannian geometry, smooth manifold, symplectic geometry}
}

@inproceedings{sohl-dickstein_hamiltonian_2014,
	title = {Hamiltonian Monte Carlo Without Detailed Balance},
	url = {http://proceedings.mlr.press/v32/sohl-dickstein14.html},
	abstract = {We present a method for performing Hamiltonian Monte Carlo that largely eliminates sample rejection.  In situations that would normally lead to rejection, instead a longer trajectory is computed un...},
	eventtitle = {International Conference on Machine Learning},
	pages = {719--726},
	booktitle = {{PMLR}},
	author = {Sohl-Dickstein, Jascha and Mudigonda, Mayur and {DeWeese}, Michael},
	date = {2014-01-27},
	langid = {english},
	file = {Full Text PDF:/home/samuel/Zotero/storage/6DCF3HIA/Sohl-Dickstein et al. - 2014 - Hamiltonian Monte Carlo Without Detailed Balance.pdf:application/pdf}
}

@article{betancourt_adiabatic_2014,
	title = {Adiabatic Monte Carlo},
	url = {http://arxiv.org/abs/1405.3489},
	abstract = {A common strategy for inference in complex models is the relaxation of a simple model into the more complex target model, for example the prior into the posterior in Bayesian inference. Existing approaches that attempt to generate such transformations, however, are sensitive to the pathologies of complex distributions and can be difficult to implement in practice. Leveraging the geometry of thermodynamic processes I introduce a principled and robust approach to deforming measures that presents a powerful new tool for inference.},
	journaltitle = {{arXiv}:1405.3489 [stat]},
	author = {Betancourt, M. J.},
	urldate = {2017-08-08},
	date = {2014-05-14},
	eprinttype = {arxiv},
	eprint = {1405.3489},
	keywords = {Statistics - Methodology},
	file = {arXiv\:1405.3489 PDF:/home/samuel/Zotero/storage/2AKICRW3/Betancourt - 2014 - Adiabatic Monte Carlo.pdf:application/pdf}
}

@inproceedings{chen_stochastic_2014,
	location = {Beijing, China},
	title = {Stochastic Gradient Hamiltonian Monte Carlo},
	url = {http://dl.acm.org/citation.cfm?id=3044805.3045080},
	series = {{ICML}'14},
	abstract = {Hamiltonian Monte Carlo ({HMC}) sampling methods provide a mechanism for defining distant proposals with high acceptance probabilities in a Metropolis-Hastings framework, enabling more efficient exploration of the state space than standard random-walk proposals. The popularity of such methods has grown significantly in recent years. However, a limitation of {HMC} methods is the required gradient computation for simulation of the Hamiltonian dynamical system--such computation is infeasible in problems involving a large sample size or streaming data. Instead, we must rely on a noisy gradient estimate computed from a subset of the data. In this paper, we explore the properties of such a stochastic gradient {HMC} approach. Surprisingly, the natural implementation of the stochastic approximation can be arbitrarily bad. To address this problem we introduce a variant that uses second-order Langevin dynamics with a friction term that counteracts the effects of the noisy gradient, maintaining the desired target distribution as the invariant distribution. Results on simulated data validate our theory. We also provide an application of our methods to a classification task using neural networks and to online Bayesian matrix factorization.},
	pages = {II--1683--II--1691},
	booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
	publisher = {{JMLR}.org},
	author = {Chen, Tianqi and Fox, Emily B. and Guestrin, Carlos},
	urldate = {2017-08-08},
	date = {2014}
}

@article{betancourt_hamiltonian_2013,
	title = {Hamiltonian Monte Carlo for Hierarchical Models},
	url = {http://arxiv.org/abs/1312.0906},
	abstract = {Hierarchical modeling provides a framework for modeling the complex interactions typical of problems in applied statistics. By capturing these relationships, however, hierarchical models also introduce distinctive pathologies that quickly limit the efficiency of most common methods of in- ference. In this paper we explore the use of Hamiltonian Monte Carlo for hierarchical models and demonstrate how the algorithm can overcome those pathologies in practical applications.},
	journaltitle = {{arXiv}:1312.0906 [stat]},
	author = {Betancourt, M. J. and Girolami, Mark},
	urldate = {2017-08-08},
	date = {2013-12-03},
	eprinttype = {arxiv},
	eprint = {1312.0906},
	keywords = {Statistics - Methodology},
	file = {arXiv\:1312.0906 PDF:/home/samuel/Zotero/storage/AJJR7QCZ/Betancourt and Girolami - 2013 - Hamiltonian Monte Carlo for Hierarchical Models.pdf:application/pdf}
}

@article{betancourt_generalizing_2013,
	title = {Generalizing the No-U-Turn Sampler to Riemannian Manifolds},
	url = {http://arxiv.org/abs/1304.1920},
	abstract = {Hamiltonian Monte Carlo provides efficient Markov transitions at the expense of introducing two free parameters: a step size and total integration time. Because the step size controls discretization error it can be readily tuned to achieve certain accuracy criteria, but the total integration time is left unconstrained. Recently Hoffman and Gelman proposed a criterion for tuning the integration time in certain systems with their No U-Turn Sampler, or {NUTS}. In this paper I investigate the dynamical basis for the success of {NUTS} and generalize it to Riemannian Manifold Hamiltonian Monte Carlo.},
	journaltitle = {{arXiv}:1304.1920 [stat]},
	author = {Betancourt, M. J.},
	urldate = {2017-08-08},
	date = {2013-04-06},
	eprinttype = {arxiv},
	eprint = {1304.1920},
	keywords = {Statistics - Methodology},
	file = {arXiv\:1304.1920 PDF:/home/samuel/Zotero/storage/M3SMMKQ6/Betancourt - 2013 - Generalizing the No-U-Turn Sampler to Riemannian M.pdf:application/pdf}
}

@incollection{betancourt_general_2013,
	title = {A General Metric for Riemannian Manifold Hamiltonian Monte Carlo},
	isbn = {978-3-642-40019-3 978-3-642-40020-9},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-40020-9_35},
	series = {Lecture Notes in Computer Science},
	abstract = {Markov Chain Monte Carlo ({MCMC}) is an invaluable means of inference with complicated models, and Hamiltonian Monte Carlo, in particular Riemannian Manifold Hamiltonian Monte Carlo ({RMHMC}), has demonstrated success in many challenging problems. Current {RMHMC} implementations, however, rely on a Riemannian metric that limits their application. In this paper I propose a new metric for {RMHMC} without these limitations and verify its success on a distribution that emulates many hierarchical and latent models.},
	pages = {327--334},
	booktitle = {Geometric Science of Information},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Betancourt, Michael},
	date = {2013},
	langid = {english},
	note = {{DOI}: 10.1007/978-3-642-40020-9\_35}
}

@book{neal_handbook_2011,
	title = {Handbook of {Markov} {Chain} {Monte} {Carlo}},
	isbn = {978-1-4200-7942-5},
	year = 2011,
	abstract = {Since their popularization in the 1990s, Markov chain Monte Carlo ({MCMC}) methods have revolutionized statistical computing and have had an especially profound impact on the practice of Bayesian statistics. Furthermore, {MCMC} methods have enabled the development and use of intricate models in an astonishing array of disciplines as diverse as fisheries science and economics. The wide-ranging practical importance of {MCMC} has sparked an expansive and deep investigation into fundamental Markov chain theory.  The Handbook of Markov Chain Monte Carlo provides a reference for the broad audience of developers and users of {MCMC} methodology interested in keeping up with cutting-edge theory and applications. The first half of the book covers {MCMC} foundations, methodology, and algorithms. The second half considers the use of {MCMC} in a variety of practical applications including in educational research, astrophysics, brain imaging, ecology, and sociology.  The in-depth introductory section of the book allows graduate students and practicing scientists new to {MCMC} to become thoroughly acquainted with the basic theory, algorithms, and applications. The book supplies detailed examples and case studies of realistic scientific problems presenting the diversity of methods used by the wide-ranging {MCMC} community. Those familiar with {MCMC} methods will find this book a useful refresher of current theory and recent developments.},
	pagetotal = {620},
	publisher = {{CRC} Press},
	author = {Neal, Radford M. and Brooks, Steve and Gelman, Andrew and Jones, Galin},
	date = {2011-05-10},
	langid = {english},
	keywords = {Mathematics / Probability \& Statistics / General, Science / Life Sciences / Biology}
}

@article{chib_understanding_1995,
  title={Understanding the {Metropolis}-{Hastings} {Algorithm}},
  author={Chib, Siddhartha and Greenberg, Edward},
  journal={The American Statistician},
  volume={49},
  number={4},
  pages={327--335},
  year={1995},
  publisher={Taylor \& Francis Group}
}

@article{girolami_riemann_2011,
  title={Riemann {Manifold} {Langevin} and {Hamiltonian} {Monte} {Carlo} {Methods}},
  author={Girolami, Mark and Calderhead, Ben},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={73},
  number={2},
  pages={123--214},
  year={2011},
  publisher={Wiley Online Library}
}

@article{roberts_simple_1994,
	title = {Simple conditions for the convergence of the Gibbs sampler and Metropolis-Hastings algorithms},
	volume = {49},
	issn = {0304-4149},
	url = {http://www.sciencedirect.com/science/article/pii/0304414994901341},
	doi = {10.1016/0304-4149(94)90134-1},
	abstract = {Markov chain Monte Carlo ({MCMC}) simulation methods are being used increasingly in statistical computation to explore and estimate features of likelihood surfaces and Bayesian posterior distributions. This paper presents simple conditions which ensure the convergence of two widely used versions of {MCMC}, the Gibbs sampler and Metropolis-Hastings algorithms.},
	pages = {207--216},
	number = {2},
	journaltitle = {Stochastic Processes and their Applications},
	shortjournal = {Stochastic Processes and their Applications},
	author = {Roberts, G. O. and Smith, A. F. M.},
	urldate = {2017-08-08},
	date = {1994-02-01},
	keywords = {Markov chain Monte Carlo, ergodicity, Gibbs sampler, lower semicontinuity, Metropolis-Hastings algorithm, statistical computation}
}

@book{newman_monte_1999,
	title = {Monte Carlo Methods in Statistical Physics},
	isbn = {978-0-19-851797-9},
	abstract = {This book provides an introduction to Monte Carlo simulations in classical statistical physics and is aimed both at students beginning work in the field and at more experienced researchers who wish to learn more about Monte Carlo methods. The material covered includes methods for both equilibrium and out of equilibrium systems, and common algorithms like the Metropolis and heat-bath algorithms are discussed in detail, as well as more sophisticated ones such as continuous time Monte Carlo, cluster algorithms, multigrid methods, entropic sampling and simulated tempering. Data analysis techniques are also explained starting with straightforward measurement and error-estimation techniques and progressing to topics such as the single and multiple histogram methods and finite size scaling. The last few chapters of the book are devoted to implementation issues, including discussions of such topics as lattice representations, efficient implementation of data structures, multispin coding, parallelization of Monte Carlo algorithms, and random number generation. At the end of the book the authors give a number of example programmes demonstrating the applications of these techniques to a variety of well-known models.},
	pagetotal = {500},
	publisher = {Clarendon Press},
	author = {Newman, M. E. J. and Barkema, G. T.},
	date = {1999-02-11},
	langid = {english},
	note = {Google-Books-{ID}: J5aLdDN4uFwC},
	keywords = {Mathematics / Probability \& Statistics / General, Computers / Desktop Applications / Design \& Graphics, Science / Mechanics / General, Science / Physics / General}
}

@article{neal_improved_1994,
	title = {An Improved Acceptance Procedure for the Hybrid {Monte} {Carlo} Algorithm},
	volume = {111},
	issn = {0021-9991},
	url = {http://www.sciencedirect.com/science/article/pii/S0021999184710540},
	doi = {10.1006/jcph.1994.1054},
	abstract = {The probability of accepting a candidate move in the hybrid Monte Carlo algorithm can be increased by considering a transition to be between windows of several states at the beginning and end of the trajectory, with a particular state within the selected window then being chosen according to the Boltzmann probabilities. The detailed balance condition used to justify the algorithm still holds with this procedure, provided the start state is randomly positioned within its window. The new procedure is shown empirically to significantly improve the acceptance rate for a test system of uncoupled oscillators. It also allows expectations to be estimated using data from all states in the windows, rather than just states that are accepted.},
	pages = {194--203},
	number = {1},
	journaltitle = {Journal of Computational Physics},
	shortjournal = {Journal of Computational Physics},
	author = {Neal, Radford M.},
	urldate = {2017-08-09},
	date = {1994-03-01},
	file = {ScienceDirect Snapshot:/home/samuel/Zotero/storage/2WUFYD7D/S0021999184710540.html:text/html}
}

@book{evans_approximating_2000,
	title = {Approximating Integrals via Monte Carlo and Deterministic Methods},
	isbn = {978-0-19-158987-4},
	abstract = {This book is designed to introduce graduate students and researchers to the primary methods useful for approximating integrals. The emphasis is on those methods that have been found to be of practical use, and although the focus is on approximating higher- dimensional integrals the lower-dimensional case is also covered. Included in the book are asymptotic techniques, multiple quadrature and quasi-random techniques as well as a complete development of Monte Carlo algorithms. For the Monte Carlo section importance sampling methods, variance reduction techniques and the primary Markov Chain Monte Carlo algorithms are covered. This book brings these various techniques together for the first time, and hence provides an accessible textbook and reference for researchers in a wide variety of disciplines.},
	pagetotal = {302},
	publisher = {{OUP} Oxford},
	author = {Evans, Michael and Swartz, Timothy},
	date = {2000-03-23},
	langid = {english},
	note = {Google-Books-{ID}: {GMHKfx}84L4MC},
	keywords = {Mathematics / Probability \& Statistics / General, Business \& Economics / Investments \& Securities / General, Business \& Economics / Statistics, Computers / Computer Science, Mathematics / Applied, Technology \& Engineering / Quality Control}
}

@book{whittaker_calculus_1967,
	edition = {4th edition},
	title = {The Calculus of Observations: An Introduction to Numerical Analysis},
	shorttitle = {The Calculus of Observations},
	abstract = {Published 1967, Paperback.},
	pagetotal = {397},
	publisher = {Dover Publications},
	author = {Whittaker, E. T. and Robinson, G.},
	date = {1967}
}

@article{bayes_essay_1763,
	title = {An Essay towards Solving a Problem in the Doctrine of Chances. By the Late Rev. Mr. Bayes, F. R. S. Communicated by Mr. Price, in a Letter to John Canton, A. M. F. R. S.},
	volume = {53},
	issn = {0261-0523},
	url = {http://rstl.royalsocietypublishing.org/cgi/doi/10.1098/rstl.1763.0053},
	doi = {10.1098/rstl.1763.0053},
	pages = {370--418},
	number = {0},
	journaltitle = {Philosophical Transactions of the Royal Society of London},
	author = {Bayes, Mr. and Price, Mr.},
	urldate = {2017-08-09},
	date = {1763-01-01},
	langid = {english}
}

@book{carlin_bayesian_2008,
	title = {Bayesian {Methods} for {Data} {Analysis}},
	isbn = {978-1-58488-698-3},
	abstract = {Broadening its scope to nonstatisticians, Bayesian Methods for Data Analysis, Third Edition provides an accessible introduction to the foundations and applications of Bayesian analysis. Along with a complete reorganization of the material, this edition concentrates more on hierarchical Bayesian modeling as implemented via Markov chain Monte Carlo ({MCMC}) methods and related data analytic techniques. New to the Third Edition New data examples, corresponding R and {WinBUGS} code, and homework problems  Explicit descriptions and illustrations of hierarchical modeling—now commonplace in Bayesian data analysis A new chapter on Bayesian design that emphasizes Bayesian clinical trials A completely revised and expanded section on ranking and histogram estimation A new case study on infectious disease modeling and the 1918 flu epidemic A solutions manual for qualifying instructors that contains solutions, computer code, and associated output for every homework problem—available both electronically and in print Ideal for Anyone Performing Statistical Analyses Focusing on applications from biostatistics, epidemiology, and medicine, this text builds on the popularity of its predecessors by making it suitable for even more practitioners and students.},
	pagetotal = {538},
	publisher = {{CRC} Press},
	author = {Carlin, Bradley P. and Louis, Thomas A.},
	date = {2008-06-30},
	langid = {english},
	note = {Google-Books-{ID}: {GTJUt}8fcFx8C},
	keywords = {Mathematics / Probability \& Statistics / General, Science / Life Sciences / Biology}
}

@book{gelman_bayesian_2013,
	title = {{Bayesian} {Data} {Analysis}},
	year = {2013},
	isbn = {978-1-4398-4095-5},
	abstract = {Now in its third edition, this classic book is widely considered the leading text on Bayesian methods, lauded for its accessible, practical approach to analyzing data and solving research problems. Bayesian Data Analysis, Third Edition continues to take an applied approach to analysis using up-to-date Bayesian methods. The authors—all leaders in the statistics community—introduce basic concepts from a data-analytic perspective before presenting advanced methods. Throughout the text, numerous worked examples drawn from real applications and research emphasize the use of Bayesian inference in practice. New to the Third Edition   Four new chapters on nonparametric modeling Coverage of weakly informative priors and boundary-avoiding priors Updated discussion of cross-validation and predictive information criteria Improved convergence monitoring and effective sample size calculations for iterative simulation Presentations of Hamiltonian Monte Carlo, variational Bayes, and expectation propagation New and revised software code   The book can be used in three different ways. For undergraduate students, it introduces Bayesian inference starting from first principles. For graduate students, the text presents effective current approaches to Bayesian modeling and computation in statistics and related fields. For researchers, it provides an assortment of Bayesian methods in applied statistics. Additional materials, including data sets used in the examples, solutions to selected exercises, and software instructions, are available on the book’s web page.},
	pagetotal = {677},
	publisher = {{CRC} Press},
	author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
	date = {2013-11-01},
	langid = {english},
	keywords = {Mathematics / Probability \& Statistics / General, Computers / Mathematical \& Statistical Software, Psychology / Research \& Methodology}
}

@book{oneill_elementary_1997,
	title = {Elementary Differential Geometry},
	isbn = {978-0-12-526745-8},
	abstract = {Written primarily for readers who have completed the standard first courses in calculus and linear algebra, Elementary Differential Geometry, Second Edition provides an introduction to the geometry of curves and surfaces. Although the popular First Edition has been extensively modified, this Second Edition maintains the elementary character of that volume, while providing an introduction to the use of computers and expanding discussion on certain topics. Further emphasis has been placed on topological properties, properties of geodesics, singularities of vector fields, and the theorems of Bonnet and Hadamard. For readers with access to the symbolic computation programs, Mathematica or Maple, the book includes approximately 30 optional computer exercises. These are not intended as an essential part of the book, but rather an extension. No computer skill is necessary to take full advantage of this comprehensive text.* Gives detailed examples for all essential ideas* Provides more than 300 exercises* Features more than 200 illustrations* Includes an introduction to using computers, and supplies answers to computer exercises given for both Mathematica and Maple systems},
	pagetotal = {500},
	publisher = {Academic Press},
	author = {O'Neill, Barrett},
	date = {1997},
	langid = {english},
	note = {Google-Books-{ID}: 4uMAw3NwnmgC},
	keywords = {Mathematics / Geometry / Differential}
}

@book{meyn_markov_2009,
	title = {Markov Chains and Stochastic Stability},
	isbn = {978-0-521-73182-9},
	abstract = {Meyn and Tweedie is back! The bible on Markov chains in general state spaces has been brought up to date to reflect developments in the field since 1996 - many of them sparked by publication of the first edition. The pursuit of more efficient simulation algorithms for complex Markovian models, or algorithms for computation of optimal policies for controlled Markov models, has opened new directions for research on Markov chains. As a result, new applications have emerged across a wide range of topics including optimisation, statistics, and economics. New commentary and an epilogue by Sean Meyn summarise recent developments and references have been fully updated. This second edition reflects the same discipline and style that marked out the original and helped it to become a classic: proofs are rigorous and concise, the range of applications is broad and knowledgeable, and key ideas are accessible to practitioners with limited mathematical background.},
	pagetotal = {623},
	publisher = {Cambridge University Press},
	author = {Meyn, Sean and Tweedie, Richard L.},
	date = {2009-04-02},
	langid = {english},
	note = {Google-Books-{ID}: Md7RnYEPkJwC},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Optimization, Mathematics / Probability \& Statistics / Stochastic Processes, Technology \& Engineering / Automation, Technology \& Engineering / General}
}

@book{mackay_information_2003,
	title = {Information {Theory}, {Inference} and {Learning} {Algorithms}},
	isbn = {978-0-521-64298-9},
	year = 2003,
	abstract = {Information theory and inference, often taught separately, are here united in one entertaining textbook. These topics lie at the heart of many exciting areas of contemporary science and engineering - communication, signal processing, data mining, machine learning, pattern recognition, computational neuroscience, bioinformatics, and cryptography. This textbook introduces theory in tandem with applications. Information theory is taught alongside practical communication systems, such as arithmetic coding for data compression and sparse-graph codes for error-correction. A toolbox of inference techniques, including message-passing algorithms, Monte Carlo methods, and variational approximations, are developed alongside applications of these tools to clustering, convolutional codes, independent component analysis, and neural networks. The final part of the book describes the state of the art in error-correcting codes, including low-density parity-check codes, turbo codes, and digital fountain codes -- the twenty-first century standards for satellite communications, disk drives, and data broadcast. Richly illustrated, filled with worked examples and over 400 exercises, some with detailed solutions, David {MacKay}'s groundbreaking book is ideal for self-learning and for undergraduate or graduate courses. Interludes on crosswords, evolution, and sex provide entertainment along the way. In sum, this is a textbook on information, communication, and coding for a new generation of students, and an unparalleled entry point into these subjects for professionals in areas as diverse as computational biology, financial engineering, and machine learning.},
	pagetotal = {628},
	publisher = {Cambridge University Press},
	author = {{MacKay}, David J. C.},
	date = {2003-09-25},
	langid = {english},
	keywords = {Science / Physics / General, Computers / Computer Science, Computers / Computer Vision \& Pattern Recognition, Computers / Data Modeling \& Design, Computers / Information Theory, Computers / Neural Networks, Computers / Programming / General, Technology \& Engineering / Electronics / General}
}

@book{alston_case_2012,
	title = {Case Studies in Bayesian Statistical Modelling and Analysis},
	isbn = {978-1-118-39432-8},
	abstract = {Provides an accessible foundation to Bayesian analysis using real world models This book aims to present an introduction to Bayesian modelling and computation, by considering real case studies drawn from diverse fields spanning ecology, health, genetics and finance. Each chapter comprises a description of the problem, the corresponding model, the computational method, results and inferences as well as the issues that arise in the implementation of these approaches. Case Studies in Bayesian Statistical Modelling and Analysis:  Illustrates how to do Bayesian analysis in a clear and concise manner using real-world problems. Each chapter focuses on a real-world problem and describes the way in which the problem may be analysed using Bayesian methods. Features approaches that can be used in a wide area of application, such as, health, the environment, genetics, information science, medicine, biology, industry and remote sensing.  Case Studies in Bayesian Statistical Modelling and Analysis is aimed at statisticians, researchers and practitioners who have some expertise in statistical modelling and analysis, and some understanding of the basics of Bayesian statistics, but little experience in its application. Graduate students of statistics and biostatistics will also find this book beneficial.},
	pagetotal = {383},
	publisher = {John Wiley \& Sons},
	author = {Alston, Clair L. and Mengersen, Kerrie L. and Pettitt, Anthony N.},
	date = {2012-10-10},
	langid = {english},
	note = {Google-Books-{ID}: m57pB4xw9CwC},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes}
}

@book{amari_information_????,
	edition = {1},
	title = {Information Geometry and Its Applications {\textbar} Shun-ichi Amari {\textbar} Springer},
	isbn = {978-4-431-55977-1},
	url = {http://www.springer.com/us/book/9784431559771},
	series = {Applied Mathematical Sciences},
	abstract = {This is the first comprehensive book on information geometry, written by the founder of the field. It begins with an elementary introduction to dualistic...},
	pagetotal = {373},
	number = {194},
	publisher = {Springer Japan},
	author = {Amari, Shun-ichi}
}

@book{baez_gauge_1994,
	title = {Gauge Fields, Knots, and Gravity},
	isbn = {978-981-02-1729-7},
	abstract = {This is an introduction to the basic tools of mathematics needed to understand the relation between knot theory and quantum gravity. The book begins with a rapid course on manifolds and differential forms, emphasizing how these provide a proper language for formulating Maxwell's equations on arbitrary spacetimes. The authors then introduce vector bundles, connections and curvature in order to generalize Maxwell theory to the Yang-Mills equations. The relation of gauge theory to the newly discovered knot invariants such as the Jones polynomial is sketched. Riemannian geometry is then introduced in order to describe Einstein's equations of general relativity and show how an attempt to quantize gravity leads to interesting applications of knot theory.},
	pagetotal = {465},
	publisher = {World Scientific},
	author = {Baez, John C. and Muniain, Javier P.},
	date = {1994},
	langid = {english},
	note = {Google-Books-{ID}: {YiGoQgAACAAJ}},
	keywords = {Science / Physics / General}
}

@article{salvatier_probabilistic_2016,
  title={Probabilistic {Programming} in {Python} {Using} {PyMC3}},
  author={Salvatier, John and Wiecki, Thomas V and Fonnesbeck, Christopher},
  journal={PeerJ Computer Science},
  volume={2},
  pages={e55},
  year={2016},
  publisher={PeerJ Inc.}
}

@online{_bambi:_????,
	title = {Bambi: A simple interface for fitting Bayesian mixed effects models},
	url = {https://osf.io/szrru/},
	shorttitle = {Bambi},
	abstract = {The popularity of Bayesian statistical methods has increased dramatically among psychologists in recent years; however, many common types of mixed effects models remain difficult to fit in a Bayesian setting. Here we introduce an open­source Python package named Bambi ({BAyesian} Model Building Interface) that is built on top of the powerful {PyMC}3 probabilistic programming framework and makes it easy to specify complex generalized linear mixed effects models using a formula notation similar to those found in packages like lme4 and nlme. We demonstrate Bambi’s versatility and ease-­of-­use by applying it to three social and personality psychology datasets that span a range of common statistical models­­including multiple regression, logistic regression, and mixed­effects modeling with crossed random effects. We conclude with a discussion of the strengths and weaknesses of a Bayesian approach in the context of common statistical practice in psychology. {\textbar}
    Hosted on the Open Science Framework}
}

@inproceedings{wang_adaptive_2013,
	location = {Atlanta, {GA}, {USA}},
	title = {Adaptive Hamiltonian and Riemann Manifold Monte Carlo Samplers},
	url = {http://dl.acm.org/citation.cfm?id=3042817.3043100},
	series = {{ICML}'13},
	abstract = {In this paper we address the widely-experienced difficulty in tuning Monte Carlo sampler based on simulating Hamiltonian dynamics. We develop an algorithm that allows for the adaptation of Hamiltonian and Riemann manifold Hamiltonian Monte Carlo samplers using Bayesian optimization that allows for infinite adaptation of the parameters of these samplers. We show that the resulting samplers are ergodic, and that the use of our adaptive algorithms makes it easy to obtain more efficient samplers, in some cases precluding the need for more complex solutions. Hamiltonian-based Monte Carlo samplers are widely known to be an excellent choice of {MCMC} method, and we aim with this paper to remove a key obstacle towards the more widespread use of these samplers in practice.},
	pages = {III--1462--III--1470},
	booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
	publisher = {{JMLR}.org},
	author = {Wang, Ziyu and Mohamed, Shakir and De Freitas, Nando},
	urldate = {2017-08-08},
	date = {2013}
}

@article{tierney_markov_1994,
author = "Tierney, Luke",
doi = "10.1214/aos/1176325750",
fjournal = "The Annals of Statistics",
journal = "Ann. Statist.",
month = "12",
number = "4",
pages = "1701--1728",
publisher = "The Institute of Mathematical Statistics",
title = "Markov {Chains} for {Exploring} {Posterior} {Distributions}",
url = "https://doi.org/10.1214/aos/1176325750",
volume = "22",
year = "1994"
}


@article{gelman_stan:_2015,
	title = {Stan:  a {Probabilistic} {Programming} {Language} for {Bayesian} {Inference} and {Optimization}},
	volume = {40},
	issn = {1076-9986},
	url = {http://journals.sagepub.com/doi/abs/10.3102/1076998615606113},
	doi = {10.3102/1076998615606113},
	shorttitle = {Stan},
	abstract = {Stan is a free and open-source C++ program that performs Bayesian inference or optimization for arbitrary user-specified models and can be called from the command line, R, Python, Matlab, or Julia and has great promise for fitting large and complex statistical models in many areas of application. We discuss Stan from users’ and developers’ perspectives and illustrate with a simple but nontrivial nonlinear regression example.},
	pages = {530--543},
	number = {5},
	journaltitle = {Journal of Educational and Behavioral Statistics},
	shortjournal = {Journal of Educational and Behavioral Statistics},
	author = {Gelman, Andrew and Lee, Daniel and Guo, Jiqiang},
	urldate = {2017-08-08},
	date = {2015-10-01},
	langid = {english}
}

@article{smith_bayesian_1993,
	title = {Bayesian Computation Via the Gibbs Sampler and Related Markov Chain Monte Carlo Methods},
	volume = {55},
	issn = {0035-9246},
	url = {http://www.jstor.org/stable/2346063},
	doi = {10.2307/2346063},
	abstract = {The use of the Gibbs sampler for Bayesian computation is reviewed and illustrated in the context of some canonical examples. Other Markov chain Monte Carlo simulation methods are also briefly described, and comments are made on the advantages of sample-based approaches for Bayesian inference summaries.},
	pages = {3--23},
	number = {1},
	journaltitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Smith, A. F. M. and Roberts, G. O.},
	urldate = {2017-08-08},
	date = {1993}
}

@article{girolami_riemannian_2009,
	title = {{Riemannian} {Manifold} {Hamiltonian} {Monte} {Carlo}},
	url = {http://arxiv.org/abs/0907.1100},
	abstract = {The paper proposes a Riemannian Manifold Hamiltonian Monte Carlo sampler to resolve the shortcomings of existing Monte Carlo algorithms when sampling from target densities that may be high dimensional and exhibit strong correlations. The method provides a fully automated adaptation mechanism that circumvents the costly pilot runs required to tune proposal densities for Metropolis-Hastings or indeed Hybrid Monte Carlo and Metropolis Adjusted Langevin Algorithms. This allows for highly efficient sampling even in very high dimensions where different scalings may be required for the transient and stationary phases of the Markov chain. The proposed method exploits the Riemannian structure of the parameter space of statistical models and thus automatically adapts to the local manifold structure at each step based on the metric tensor. A semi-explicit second order symplectic integrator for non-separable Hamiltonians is derived for simulating paths across this manifold which provides highly efficient convergence and exploration of the target density. The performance of the Riemannian Manifold Hamiltonian Monte Carlo method is assessed by performing posterior inference on logistic regression models, log-Gaussian Cox point processes, stochastic volatility models, and Bayesian estimation of parameter posteriors of dynamical systems described by nonlinear differential equations. Substantial improvements in the time normalised Effective Sample Size are reported when compared to alternative sampling approaches. Matlab code at {\textbackslash}url\{http://www.dcs.gla.ac.uk/inference/rmhmc\} allows replication of all results.},
	journaltitle = {{arXiv}:0907.1100 [math, stat]},
	author = {Girolami, Mark and Calderhead, Ben and Chin, Siu A.},
	urldate = {2017-08-08},
	date = {2009-07-06},
	eprinttype = {arxiv},
	eprint = {0907.1100},
	keywords = {Mathematics - Statistics Theory, Mathematics - Numerical Analysis, Statistics - Computation}
}

@article{hoffman_no-u-turn_2014,
  title={The {No}-{U}-{Turn} {Sampler}: {Adaptively} {Setting} {Path} {Lengths} in {Hamiltonian} {Monte} {Carlo}.},
  author={Hoffman, Matthew D and Gelman, Andrew},
  journal={Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={1593--1623},
  year={2014}
}

@article{metropolis_equation_1953,
  title={Equation of {State} {Calculations} by {Fast} {Computing} {Machines}},
  author={Metropolis, Nicholas and Rosenbluth, Arianna W and Rosenbluth, Marshall N and Teller, Augusta H and Teller, Edward},
  journal={The Journal of Chemical Physics},
  volume={21},
  number={6},
  pages={1087--1092},
  year={1953},
  publisher={AIP}
}

@article{andrieu_introduction_2003,
	title = {An {Introduction} to {MCMC} for {Machine} {Learning}},
	volume = {50},
	issn = {0885-6125, 1573-0565},
	url = {https://link.springer.com/article/10.1023/A:1020281327116},
	doi = {10.1023/A:1020281327116},
	abstract = {This purpose of this introductory paper is threefold. First, it introduces the Monte Carlo method with emphasis on probabilistic machine learning. Second, it reviews the main building blocks of modern Markov chain Monte Carlo simulation, thereby providing and introduction to the remaining papers of this special issue. Lastly, it discusses new interesting research horizons.},
	pages = {5--43},
	number = {1},
	journaltitle = {Machine Learning},
	shortjournal = {Machine Learning},
	author = {Andrieu, Christophe and Freitas, Nando de and Doucet, Arnaud and Jordan, Michael I.},
	date = {2003-01-01},
	langid = {english},
	file = {Full Text PDF:/home/samuel/Zotero/storage/3SIJI47R/Andrieu et al. - 2003 - An Introduction to MCMC for Machine Learning.pdf:application/pdf}
}

@article{roberts_langevin_2002,
	title = {Langevin {Diffusions} and {Metropolis}-{Hastings} {Algorithms}},
	volume = {4},
	issn = {1387-5841, 1573-7713},
	url = {https://link.springer.com/article/10.1023/A:1023562417138},
	doi = {10.1023/A:1023562417138},
	abstract = {We consider a class of Langevin diffusions with state-dependent volatility. The volatility of the diffusion is chosen so as to make the stationary distribution of the diffusion with respect to its natural clock, a heated version of the stationary density of interest. The motivation behind this construction is the desire to construct uniformly ergodic diffusions with required stationary densities. Discrete time algorithms constructed by Hastings accept reject mechanisms are constructed from discretisations of the algorithms, and the properties of these algorithms are investigated.},
	pages = {337--357},
	number = {4},
	journaltitle = {Methodology And Computing In Applied Probability},
	shortjournal = {Methodology and Computing in Applied Probability},
	author = {Roberts, G. O. and Stramer, O.},
	date = {2002-12-01},
	langid = {english}
}

@report{resnik_gibbs_2010,
	title = {Gibbs Sampling for the Uninitiated},
	url = {http://www.dtic.mil/docs/citations/ADA523027},
	abstract = {This document is intended for computer scientists who would like to try out a Markov Chain Monte Carlo ({MCMC}) technique, particularly to do inference with Bayesian models on problems related to text processing. We try to keep theory to the absolute minimum needed, though we work through the details much more explicitly than you usually see even in "introductory" explanations. That means we've attempted to be ridiculously explicit in our exposition and notation. After providing the reasons and reasoning behind Gibbs sampling (and at least nodding our heads in the direction of theory), we work through an example application in detail -- the derivation of a Gibbs sampler for a Naive Bayes model. Along with the example, we discuss some practical implementation issues, including the integrating out of continuous parameters when possible. We conclude with some pointers to literature that we've found to be somewhat more friendly to uninitiated readers.},
	number = {{CS}-{TR}-4956},
	institution = {{MARYLAND} {UNIV} {COLLEGE} {PARK} {INST} {FOR} {ADVANCED} {COMPUTER} {STUDIES}, {MARYLAND} {UNIV} {COLLEGE} {PARK} {INST} {FOR} {ADVANCED} {COMPUTER} {STUDIES}},
	author = {Resnik, Philip and Hardisty, Eric},
	date = {2010-04}
}

@article{yang_automatically_2017,
	title = {Automatically tuned general-purpose {MCMC} via new adaptive diagnostics},
	volume = {32},
	issn = {0943-4062, 1613-9658},
	url = {https://link.springer.com/article/10.1007/s00180-016-0682-2},
	doi = {10.1007/s00180-016-0682-2},
	abstract = {Adaptive Markov Chain Monte Carlo ({MCMC}) algorithms attempt to ‘learn’ from the results of past iterations so the Markov chain can converge quicker. Unfortunately, adaptive {MCMC} algorithms are no longer Markovian, so their convergence is difficult to guarantee. In this paper, we develop new diagnostics to determine whether the adaption is still improving the convergence. We present an algorithm which automatically stops adapting once it determines further adaption will not increase the convergence speed. Our algorithm allows the computer to tune a ‘good’ Markov chain through multiple phases of adaption, and then run conventional non-adaptive {MCMC}. In this way, the efficiency gains of adaptive {MCMC} can be obtained while still ensuring convergence to the target distribution.},
	pages = {315--348},
	number = {1},
	journaltitle = {Computational Statistics},
	shortjournal = {Comput Stat},
	author = {Yang, Jinyoung and Rosenthal, Jeffrey S.},
	date = {2017-03-01},
	langid = {english}
}

@article{graham_continuously_2017,
	title = {Continuously tempered Hamiltonian Monte Carlo},
	url = {http://arxiv.org/abs/1704.03338},
	abstract = {Hamiltonian Monte Carlo ({HMC}) is a powerful Markov chain Monte Carlo ({MCMC}) method for performing approximate inference in complex probabilistic models of continuous variables. In common with many {MCMC} methods, however, the standard {HMC} approach performs poorly in distributions with multiple isolated modes. We present a method for augmenting the Hamiltonian system with an extra continuous temperature control variable which allows the dynamic to bridge between sampling a complex target distribution and a simpler unimodal base distribution. This augmentation both helps improve mixing in multimodal targets and allows the normalisation constant of the target distribution to be estimated. The method is simple to implement within existing {HMC} code, requiring only a standard leapfrog integrator. We demonstrate experimentally that the method is competitive with annealed importance sampling and simulating tempering methods at sampling from challenging multimodal distributions and estimating their normalising constants.},
	journaltitle = {{arXiv}:1704.03338 [stat]},
	author = {Graham, Matthew M. and Storkey, Amos J.},
	urldate = {2017-08-08},
	date = {2017-04-11},
	eprinttype = {arxiv},
	eprint = {1704.03338},
	keywords = {Statistics - Computation},
	file = {arXiv\:1704.03338 PDF:/home/samuel/Zotero/storage/UFCUZ4DQ/Graham and Storkey - 2017 - Continuously tempered Hamiltonian Monte Carlo.pdf:application/pdf}
}

@incollection{geman_stochastic_1984,
  title={Stochastic {Relaxation}, {Gibbs} {Distributions}, and the {Bayesian} {Restoration} of {Images}},
  author={Geman, Stuart and Geman, Donald},
  booktitle={Readings in Computer Vision},
  pages={564--584},
  year={1987},
  publisher={Elsevier}
}

@article{casella_explaining_1992,
	title = {Explaining the {Gibbs} {Sampler}},
	volume = {46},
	issn = {0003-1305},
	url = {http://www.jstor.org/stable/2685208},
	doi = {10.2307/2685208},
	abstract = {Computer-intensive algorithms, such as the Gibbs sampler, have become increasingly popular statistical tools, both in applied and theoretical work. The properties of such algorithms, however, may sometimes not be obvious. Here we give a simple explanation of how and why the Gibbs sampler works. We analytically establish its properties in a simple case and provide insight for more complicated cases. There are also a number of examples.},
	pages = {167--174},
	number = {3},
	journaltitle = {The American Statistician},
	author = {Casella, George and George, Edward I.},
	urldate = {2017-08-08},
	date = {1992}
}

@article{byrne_geodesic_2013,
	title = {Geodesic Monte Carlo on Embedded Manifolds},
	volume = {40},
	issn = {1467-9469},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/sjos.12036/abstract},
	doi = {10.1111/sjos.12036},
	abstract = {Markov chain Monte Carlo methods explicitly defined on the manifold of probability distributions have recently been established. These methods are constructed from diffusions across the manifold and the solution of the equations describing geodesic flows in the Hamilton–Jacobi representation. This paper takes the differential geometric basis of Markov chain Monte Carlo further by considering methods to simulate from probability distributions that themselves are defined on a manifold, with common examples being classes of distributions describing directional statistics. Proposal mechanisms are developed based on the geodesic flows over the manifolds of support for the distributions, and illustrative examples are provided for the hypersphere and Stiefel manifold of orthonormal matrices.},
	pages = {825--845},
	number = {4},
	journaltitle = {Scandinavian Journal of Statistics},
	shortjournal = {Scand J Statist},
	author = {Byrne, Simon and Girolami, Mark},
	urldate = {2017-08-08},
	date = {2013-12-01},
	langid = {english},
	keywords = {directional statistics, Riemannian manifold, Stiefel manifold, geodesic, Hamiltonian Monte Carlo},
	file = {Full Text PDF:/home/samuel/Zotero/storage/AAHK3TDU/Byrne and Girolami - 2013 - Geodesic Monte Carlo on Embedded Manifolds.pdf:application/pdf}
}

@article{bui-thanh_solving_2014,
	title = {Solving large-scale {PDE}-constrained Bayesian inverse problems with Riemann manifold Hamiltonian Monte Carlo},
	volume = {30},
	issn = {0266-5611},
	url = {http://stacks.iop.org/0266-5611/30/i=11/a=114014},
	doi = {10.1088/0266-5611/30/11/114014},
	abstract = {We consider the Riemann manifold Hamiltonian Monte Carlo ({RMHMC}) method for solving statistical inverse problems governed by partial differential equations ({PDEs}). The Bayesian framework is employed to cast the inverse problem into the task of statistical inference whose solution is the posterior distribution in infinite dimensional parameter space conditional upon observation data and Gaussian prior measure. We discretize both the likelihood and the prior using the H 1 -conforming finite element method together with a matrix transfer technique. The power of the {RMHMC} method is that it exploits the geometric structure induced by the {PDE} constraints of the underlying inverse problem. Consequently, each {RMHMC} posterior sample is almost uncorrelated/independent from the others providing statistically efficient Markov chain simulation. However this statistical efficiency comes at a computational cost. This motivates us to consider computationally more efficient strategies for {RMHMC}. At the heart of our construction is the fact that for Gaussian error structures the Fisher information matrix coincides with the Gauss–Newton Hessian. We exploit this fact in considering a computationally simplified {RMHMC} method combining state-of-the-art adjoint techniques and the superiority of the {RMHMC} method. Specifically, we first form the Gauss–Newton Hessian at the maximum a posteriori point and then use it as a fixed constant metric tensor throughout {RMHMC} simulation. This eliminates the need for the computationally costly differential geometric Christoffel symbols, which in turn greatly reduces computational effort at a corresponding loss of sampling efficiency. We further reduce the cost of forming the Fisher information matrix by using a low rank approximation via a randomized singular value decomposition technique. This is efficient since a small number of Hessian-vector products are required. The Hessian-vector product in turn requires only two extra {PDE} solves using the adjoint technique. Various numerical results up to 1025 parameters are presented to demonstrate the ability of the {RMHMC} method in exploring the geometric structure of the problem to propose (almost) uncorrelated/independent samples that are far away from each other, and yet the acceptance rate is almost unity. The results also suggest that for the {PDE} models considered the proposed fixed metric {RMHMC} can attain almost as high a quality performance as the original {RMHMC}, i.e. generating (almost) uncorrelated/independent samples, while being two orders of magnitude less computationally expensive.},
	pages = {114014},
	number = {11},
	journaltitle = {Inverse Problems},
	shortjournal = {Inverse Problems},
	author = {Bui-Thanh, T. and Girolami, M.},
	urldate = {2017-08-08},
	date = {2014},
	langid = {english}
}

@inproceedings{betancourt_fundamental_2015,
	title = {The Fundamental Incompatibility of Scalable Hamiltonian Monte Carlo and Naive Data Subsampling},
	url = {http://proceedings.mlr.press/v37/betancourt15.html},
	abstract = {Leveraging the coherent exploration of Hamiltonian flow, Hamiltonian Monte Carlo produces computationally efficient Monte Carlo estimators, even with respect to complex and high-dimensional target ...},
	eventtitle = {International Conference on Machine Learning},
	pages = {533--540},
	booktitle = {{PMLR}},
	author = {Betancourt, Michael},
	date = {2015-06-01},
	langid = {english},
	file = {Full Text PDF:/home/samuel/Zotero/storage/WU5JANT8/Betancourt - 2015 - The Fundamental Incompatibility of Scalable Hamilt.pdf:application/pdf}
}

@inproceedings{ma_complete_2015,
	location = {Cambridge, {MA}, {USA}},
	title = {A Complete Recipe for Stochastic Gradient {MCMC}},
	url = {http://dl.acm.org/citation.cfm?id=2969442.2969566},
	series = {{NIPS}'15},
	abstract = {Many recent Markov chain Monte Carlo ({MCMC}) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution. In tandem, a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations. However, such stochastic gradient {MCMC} samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is non-trivial. Even with simple dynamics, significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise. In this paper, we provide a general recipe for constructing {MCMC} samplers—including stochastic gradient versions—based on continuous Markov processes specified via two matrices. We constructively prove that the framework is complete. That is, any continuous Markov process that provides samples from the target distribution can be written in our framework. We show how previous continuous-dynamic samplers can be trivially "reinvented" in our framework, avoiding the complicated sampler-specific proofs. We likewise use our recipe to straightforwardly propose a new state-adaptive sampler: stochastic gradient Riemann Hamiltonian Monte Carlo ({SGRHMC}). Our experiments on simulated data and a streaming Wikipedia analysis demonstrate that the proposed {SGRHMC} sampler inherits the benefits of Riemann {HMC}, with the scalability of stochastic gradient methods.},
	pages = {2917--2925},
	booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems},
	publisher = {{MIT} Press},
	author = {Ma, Yi-An and Chen, Tianqi and Fox, Emily B.},
	urldate = {2017-08-08},
	date = {2015}
}

@article{hastings_monte_1970,
    author = {Hastings, W. K.},
    title = "{Monte {Carlo} {Sampling} {Methods} {Using} {Markov} {Chains} and {Their} {Applications}}",
    journal = {Biometrika},
    volume = {57},
    number = {1},
    pages = {97-109},
    year = {1970},
    month = {04},
    abstract = "{A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/57.1.97},
    url = {https://doi.org/10.1093/biomet/57.1.97},
    eprint = {http://oup.prod.sis.lan/biomet/article-pdf/57/1/97/23940249/57-1-97.pdf},
}


@article{betancourt_conceptual_2017,
	title = {A {Conceptual} {Introduction} to {Hamiltonian} {Monte} {Carlo}},
	year = 2017,
	journal={arXiv preprint arXiv:1701.02434},
	url = {http://arxiv.org/abs/1701.02434},
	pages = {1--60},
	abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous under- standing of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is con- fined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any ex- haustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
	journaltitle = {{arXiv}:1701.02434 [stat]},
	author = {Betancourt, Michael},
	urldate = {2017-08-08},
	date = {2017-01-09},
	eprinttype = {arxiv},
	eprint = {1701.02434},
	keywords = {Statistics - Methodology},
	file = {arXiv\:1701.02434 PDF:/home/samuel/Zotero/storage/XTN73I7Y/Betancourt - 2017 - A Conceptual Introduction to Hamiltonian Monte Car.pdf:application/pdf}
}

@article{nishimura_geometrically_2016,
	title = {Geometrically Tempered Hamiltonian Monte Carlo},
	url = {http://arxiv.org/abs/1604.00872},
	abstract = {Hamiltonian Monte Carlo ({HMC}) has become routinely used for sampling from posterior distributions. Its extension Riemann manifold {HMC} ({RMHMC}) modifies the proposal kernel through distortion of local distances by a Riemannian metric. The performance depends critically on the choice of metric, with the Fisher information providing the standard choice. In this article, we propose a new class of metrics aimed at improving {HMC}'s performance on multi-modal target distributions. We refer to the proposed approach as geometrically tempered {HMC} ({GTHMC}) due to its connection to other tempering methods. We establish a geometric theory behind {RMHMC} to motivate {GTHMC} and characterize its theoretical properties. Moreover, we develop a novel variable step size integrator for simulating Hamiltonian dynamics to improve on the usual St{\textbackslash}"\{o\}rmer-Verlet integrator which suffers from numerical instability in {GTHMC} settings. We illustrate {GTHMC} through simulations, demonstrating generality and substantial gains over standard {HMC} implementations in terms of effective sample sizes.},
	journaltitle = {{arXiv}:1604.00872 [stat]},
	author = {Nishimura, Akihiko and Dunson, David},
	urldate = {2017-08-08},
	date = {2016-04-04},
	eprinttype = {arxiv},
	eprint = {1604.00872},
	keywords = {Statistics - Computation},
	file = {arXiv\:1604.00872 PDF:/home/samuel/Zotero/storage/WN62TYIU/Nishimura and Dunson - 2016 - Geometrically Tempered Hamiltonian Monte Carlo.pdf:application/pdf}
}

@article{livingstone_geometric_2016,
	title = {On the {Geometric} {Ergodicity} of {Hamiltonian} {Monte} {Carlo}},
	url = {http://arxiv.org/abs/1601.08057},
	abstract = {We establish general conditions under which Markov chains produced by the Hamiltonian Monte Carlo method will and will not be geometrically ergodic. We consider implementations with both position-independent and position-dependent integration times. In the former case we find that the conditions for geometric ergodicity are essentially a non-vanishing gradient of the log-density which asymptotically points towards the centre of the space and does not grow faster than linearly. In an idealised scenario in which the integration time is allowed to change in different regions of the space, we show that geometric ergodicity can be recovered for a much broader class of tail behaviours, leading to some guidelines for the choice of this free parameter in practice.},
	journaltitle = {{arXiv}:1601.08057 [stat]},
	journal={arXiv preprint arXiv:1601.08057},
	year = {2016},
	pages = {1--29},
	author = {Livingstone, Samuel and Betancourt, Michael and Byrne, Simon and Girolami, Mark},
	urldate = {2017-08-08},
	date = {2016-01-29},
	eprinttype = {arxiv},
	eprint = {1601.08057},
	keywords = {Statistics - Methodology, Statistics - Computation, Statistics - Machine Learning},
	file = {arXiv\:1601.08057 PDF:/home/samuel/Zotero/storage/WZ2K4FW4/Livingstone et al. - 2016 - On the Geometric Ergodicity of Hamiltonian Monte C.pdf:application/pdf}
}

@article{blei_variational_2017,
	title = {Variational Inference: A Review for Statisticians},
	volume = {112},
	issn = {0162-1459},
	url = {http://dx.doi.org/10.1080/01621459.2017.1285773},
	doi = {10.1080/01621459.2017.1285773},
	shorttitle = {Variational Inference},
	abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this article, we review variational inference ({VI}), a method from machine learning that approximates probability densities through optimization. {VI} has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind {VI} is to first posit a family of densities and then to find a member of that family which is close to the target density. Closeness is measured by Kullback–Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of {VI} applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in {VI} and highlight important open problems. {VI} is powerful, but it is not yet well understood. Our hope in writing this article is to catalyze statistical research on this class of algorithms. Supplementary materials for this article are available online.},
	pages = {859--877},
	number = {518},
	journaltitle = {Journal of the American Statistical Association},
	author = {Blei, David M. and Kucukelbir, Alp and {McAuliffe}, Jon D.},
	urldate = {2017-08-08},
	date = {2017-04-03},
	keywords = {Algorithms, Computationally intensive methods, Statistical computing}
}

@article{betancourt_identifying_2016,
	title = {Identifying the Optimal Integration Time in Hamiltonian Monte Carlo},
	url = {http://arxiv.org/abs/1601.00225},
	abstract = {By leveraging the natural geometry of a smooth probabilistic system, Hamiltonian Monte Carlo yields computationally efficient Markov Chain Monte Carlo estimation. At least provided that the algorithm is sufficiently well-tuned. In this paper I show how the geometric foundations of Hamiltonian Monte Carlo implicitly identify the optimal choice of these parameters, especially the integration time. I then consider the practical consequences of these principles in both existing algorithms and a new implementation called {\textbackslash}emph\{Exhaustive Hamiltonian Monte Carlo\} before demonstrating the utility of these ideas in some illustrative examples.},
	journaltitle = {{arXiv}:1601.00225 [stat]},
	author = {Betancourt, Michael},
	urldate = {2017-08-08},
	date = {2016-01-02},
	eprinttype = {arxiv},
	eprint = {1601.00225},
	keywords = {Statistics - Methodology, Statistics - Computation},
	file = {arXiv\:1601.00225 PDF:/home/samuel/Zotero/storage/Y3DFYDWF/Betancourt - 2016 - Identifying the Optimal Integration Time in Hamilt.pdf:application/pdf}
}

@article{ernst_yules_2017,
	title = {Yule’s “nonsense correlation” solved!},
	volume = {45},
	issn = {0090-5364, 2168-8966},
	url = {http://projecteuclid.org/euclid.aos/1498636874},
	doi = {10.1214/16-AOS1509},
	abstract = {In this paper, we resolve a longstanding open statistical problem. The problem is to mathematically prove Yule’s 1926 empirical finding of “nonsense correlation” [J. Roy. Statist. Soc. 89 (1926) 1–63], which we do by analytically determining the second moment of the empirical correlation coefficient θ:=∫10W1(t)W2(t)dt−∫10W1(t)dt∫10W2(t)dt∫10W21(t)dt−(∫10W1(t)dt)2−−−−−−−−−−−−−−−−−−−−−−−√∫10W22(t)dt−(∫10W2(t)dt)2−−−−−−−−−−−−−−−−−−−−−−−√,θ:=∫01W1(t)W2(t)dt−∫01W1(t)dt∫01W2(t)dt∫01W12(t)dt−(∫01W1(t)dt)2∫01W22(t)dt−(∫01W2(t)dt)2,{\textbackslash}begin\{eqnarray*\}\&\&{\textbackslash}theta:={\textbackslash}frac\{{\textbackslash}int\_\{0\}{\textasciicircum}\{1\}W\_\{1\}(t)W\_\{2\}(t){\textbackslash},dt-{\textbackslash}int\_\{0\}{\textasciicircum}\{1\}W\_\{1\}(t){\textbackslash},dt{\textbackslash}int\_\{0\}{\textasciicircum}\{1\}W\_\{2\}(t){\textbackslash},dt\}\{{\textbackslash}sqrt\{{\textbackslash}int\_\{0\}{\textasciicircum}\{1\}W{\textasciicircum}\{2\}\_\{1\}(t){\textbackslash},dt-({\textbackslash}int\_\{0\}{\textasciicircum}\{1\}W\_\{1\}(t){\textbackslash},dt){\textasciicircum}\{2\}\}{\textbackslash}sqrt\{{\textbackslash}int\_\{0\}{\textasciicircum}\{1\}W{\textasciicircum}\{2\}\_\{2\}(t){\textbackslash},dt-({\textbackslash}int\_\{0\}{\textasciicircum}\{1\}W\_\{2\}(t){\textbackslash},dt){\textasciicircum}\{2\}\}\},{\textbackslash}end\{eqnarray*\} of two independent Wiener processes, W1,W2W1,W2W\_\{1\},W\_\{2\}. Using tools from Fredholm integral equation theory, we successfully calculate the second moment of θθ{\textbackslash}theta to obtain a value for the standard deviation of θθ{\textbackslash}theta of nearly 0.5. The “nonsense” correlation, which we call “volatile” correlation, is volatile in the sense that its distribution is heavily dispersed and is frequently large in absolute value. It is induced because each Wiener process is “self-correlated” in time. This is because a Wiener process is an integral of pure noise, and thus its values at different time points are correlated. In addition to providing an explicit formula for the second moment of θθ{\textbackslash}theta, we offer implicit formulas for higher moments of θθ{\textbackslash}theta.},
	pages = {1789--1809},
	number = {4},
	journaltitle = {The Annals of Statistics},
	shortjournal = {Ann. Statist.},
	author = {Ernst, Philip A. and Shepp, Larry A. and Wyner, Abraham J.},
	date = {2017-08},
	keywords = {integral equations, Volatile correlation, Wiener processes},
	file = {Snapshot:/home/samuel/Zotero/storage/YQXDNLU9/1498636874.html:text/html}
}

@article{phillips_understanding_1986,
	title = {Understanding spurious regressions in econometrics},
	volume = {33},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/pii/0304407686900011},
	doi = {10.1016/0304-4076(86)90001-1},
	abstract = {This paper provides an analytical study of linear regressions involving the levels of economic time series. An asymptotic theory is developed for regressions that relate quite general integrated random processes. This includes the spurious regressions of Granger and Newbold (1974) and the recent cointegrating regressions of Granger and Engle (1985). An asymptotic theory is developed for the regression coefficients and for conventional significance tests. It is shown that the usual t- and F-ratio test statistics do not possess limiting distributions in this context but actually diverge as the sample size T ↑ ∞. The limiting behavior of regression diagnostics such as the Durbin–Watson statistic, the coefficient of determination and the Box–Pierce statistic is also analyzed. The theoretical results that we present explain many of the earlier simulation findings of Granger and Newbold (1974, 1977).},
	pages = {311--340},
	number = {3},
	journaltitle = {Journal of Econometrics},
	shortjournal = {Journal of Econometrics},
	author = {Phillips, P. C. B.},
	urldate = {2017-08-10},
	date = {1986-12-01},
	file = {ScienceDirect Snapshot:/home/samuel/Zotero/storage/HE83W28A/0304407686900011.html:text/html}
}

@article{granger_spurious_1974,
	title = {Spurious regressions in econometrics},
	volume = {2},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/pii/0304407674900347},
	doi = {10.1016/0304-4076(74)90034-7},
	pages = {111--120},
	number = {2},
	journaltitle = {Journal of Econometrics},
	shortjournal = {Journal of Econometrics},
	author = {Granger, C. W. J. and Newbold, P.},
	urldate = {2017-08-10},
	date = {1974-07-01},
	file = {ScienceDirect Snapshot:/home/samuel/Zotero/storage/ASS7BZKP/0304407674900347.html:text/html}
}

@article{yule_why_1926,
	title = {Why do we Sometimes get Nonsense-Correlations between Time-Series?--A Study in Sampling and the Nature of Time-Series},
	volume = {89},
	issn = {0952-8385},
	url = {http://www.jstor.org/stable/2341482},
	doi = {10.2307/2341482},
	shorttitle = {Why do we Sometimes get Nonsense-Correlations between Time-Series?},
	pages = {1--63},
	number = {1},
	journaltitle = {Journal of the Royal Statistical Society},
	author = {Yule, G. Udny},
	urldate = {2017-08-10},
	date = {1926}
}

@article{alyass_big_2015,
	title = {From big data analysis to personalized medicine for all: challenges and opportunities},
	volume = {8},
	issn = {1755-8794},
	url = {https://doi.org/10.1186/s12920-015-0108-y},
	doi = {10.1186/s12920-015-0108-y},
	shorttitle = {From big data analysis to personalized medicine for all},
	abstract = {Recent advances in high-throughput technologies have led to the emergence of systems biology as a holistic science to achieve more precise modeling of complex diseases. Many predict the emergence of personalized medicine in the near future. We are, however, moving from two-tiered health systems to a two-tiered personalized medicine. Omics facilities are restricted to affluent regions, and personalized medicine is likely to widen the growing gap in health systems between high and low-income countries. This is mirrored by an increasing lag between our ability to generate and analyze big data. Several bottlenecks slow-down the transition from conventional to personalized medicine: generation of cost-effective high-throughput data; hybrid education and multidisciplinary teams; data storage and processing; data integration and interpretation; and individual and global economic relevance. This review provides an update of important developments in the analysis of big data and forward strategies to accelerate the global transition to personalized medicine.},
	pages = {33},
	journaltitle = {{BMC} Medical Genomics},
	shortjournal = {{BMC} Medical Genomics},
	author = {Alyass, Akram and Turcotte, Michelle and Meyre, David},
	urldate = {2017-08-10},
	date = {2015-06-27},
	keywords = {Big data, Cloud computing, High-dimensionality, High-throughput technologies, Integrative methods, Omics, Personalized medicine},
	file = {Full Text PDF:/home/samuel/Zotero/storage/Y8BWF5C6/Alyass et al. - 2015 - From big data analysis to personalized medicine fo.pdf:application/pdf;Snapshot:/home/samuel/Zotero/storage/5IB9S9NE/s12920-015-0108-y.html:text/html}
}

@article{murdoch_inevitable_2013,
	title = {The Inevitable Application of Big Data to Health Care},
	volume = {309},
	issn = {0098-7484},
	url = {http://jamanetwork.com/journals/jama/fullarticle/1674245},
	doi = {10.1001/jama.2013.393},
	abstract = {The amount of data being digitally collected and stored is vast and expanding rapidly. As a result, the science of data management and analysis is also advancing to enable organizations to convert this vast resource into information and knowledge that helps them achieve their objectives. Computer...},
	pages = {1351--1352},
	number = {13},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Murdoch, Travis B. and Detsky, Allan S.},
	date = {2013-04-03},
	file = {Snapshot:/home/samuel/Zotero/storage/726EZWP5/1674245.html:text/html}
}

@book{hastie_elements_2009,
	title = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition},
	isbn = {978-0-387-84858-7},
	shorttitle = {The Elements of Statistical Learning},
	abstract = {During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression \& path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-{PLUS} and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including {CART}, {MARS}, projection pursuit and gradient boosting.},
	pagetotal = {757},
	publisher = {Springer Science \& Business Media},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	date = {2009-08-26},
	langid = {english},
	note = {Google-Books-{ID}: {tVIjmNS}3Ob8C},
	keywords = {Mathematics / Probability \& Statistics / General, Science / Life Sciences / Biology, Computers / Computer Science, Computers / Databases / Data Mining, Computers / Intelligence ({AI}) \& Semantics, Computers / Software Development \& Engineering / General, Science / Life Sciences / General}
}

@book{bellman_dynamic_1957,
	title = {Dynamic Programming},
	abstract = {This is a brief description of decision processes in dynamic programming.},
	pagetotal = {339},
	publisher = {Princeton University Press},
	author = {Bellman, Richard Ernest},
	date = {1957},
	langid = {english},
	note = {Google-Books-{ID}: {rZW}4ugAACAAJ}
}

@article{fan_challenges_2014,
	title = {Challenges of Big Data analysis},
	volume = {1},
	issn = {2095-5138},
	url = {https://academic.oup.com/nsr/article/1/2/293/1397586/Challenges-of-Big-Data-analysis},
	doi = {10.1093/nsr/nwt032},
	abstract = {Big Data bring new opportunities to modern society and challenges to data scientists. On the one hand, Big Data hold great promises for discovering subtle population patterns and heterogeneities that are not possible with small-scale data. On the other hand, the massive sample size and high dimensionality of Big Data introduce unique computational and statistical challenges, including scalability and storage bottleneck, noise accumulation, spurious correlation, incidental endogeneity and measurement errors. These challenges are distinguished and require new computational and statistical paradigm. This paper gives overviews on the salient features of Big Data and how these features impact on paradigm change on statistical and computational methods as well as computing architectures. We also provide various new perspectives on the Big Data analysis and computation. In particular, we emphasize on the viability of the sparsest solution in high-confidence set and point out that exogenous assumptions in most statistical methods for Big Data cannot be validated due to incidental endogeneity. They can lead to wrong statistical inferences and consequently wrong scientific conclusions.},
	pages = {293--314},
	number = {2},
	journaltitle = {National Science Review},
	shortjournal = {Natl Sci Rev},
	author = {Fan, Jianqing and Han, Fang and Liu, Han},
	date = {2014-06-01},
	file = {Full Text PDF:/home/samuel/Zotero/storage/8TVB7WN7/Fan et al. - 2014 - Challenges of Big Data analysis.pdf:application/pdf;Snapshot:/home/samuel/Zotero/storage/MYL8L8UR/Challenges-of-Big-Data-analysis.html:text/html}
}

@online{mcafee_big_2012,
	title = {Big Data: The Management Revolution},
	url = {https://hbr.org/2012/10/big-data-the-management-revolution},
	shorttitle = {Big Data},
	abstract = {Exploiting vast new flows of information can radically improve your company’s performance. But first you’ll have to change your decision-making culture.},
	titleaddon = {Harvard Business Review},
	author = {{McAfee}, Andrew and Brynjolfsson, Erik},
	date = {2012-10-01},
	file = {Snapshot:/home/samuel/Zotero/storage/2VMBYQSX/big-data-the-management-revolution.html:text/html}
}

@book{vaart_asymptotic_2000,
	title = {Asymptotic Statistics},
	isbn = {978-0-521-78450-4},
	abstract = {Here is a practical and mathematically rigorous introduction to the field of asymptotic statistics. In addition to most of the standard topics of an asymptotics course--likelihood inference, M-estimation, the theory of asymptotic efficiency, U-statistics, and rank procedures--the book also presents recent research topics such as semiparametric models, the bootstrap, and empirical processes and their applications. The topics are organized from the central idea of approximation by limit experiments, one of the book's unifying themes that mainly entails the local approximation of the classical i.i.d. set up with smooth parameters by location experiments involving a single, normally distributed observation.},
	pagetotal = {462},
	publisher = {Cambridge University Press},
	author = {Vaart, A. W. van der},
	date = {2000-06-19},
	langid = {english},
	note = {Google-Books-{ID}: {UEuQEM}5RjWgC},
	keywords = {Mathematics / Probability \& Statistics / General}
}

@book{lehmann_theory_2003,
	title = {Theory of Point Estimation},
	isbn = {978-0-387-98502-2},
	abstract = {Since the publication in 1983 of Theory of Point Estimation, much new work has made it desirable to bring out a second edition. The inclusion of the new material has increased the length of the book from 500 to 600 pages; of the approximately 1000 references about 25\% have appeared since 1983. The greatest change has been the addition to the sparse treatment of Bayesian inference in the first edition. This includes the addition of new sections on Equivariant, Hierarchical, and Empirical Bayes, and on their comparisons. Other major additions deal with new developments concerning the information in equality and simultaneous and shrinkage estimation. The Notes at the end of each chapter now provide not only bibliographic and historical material but also introductions to recent development in point estimation and other related topics which, for space reasons, it was not possible to include in the main text. The problem sections also have been greatly expanded. On the other hand, to save space most of the discussion in the first edition on robust estimation (in particu lar L, M, and R estimators) has been deleted. This topic is the subject of two excellent books by Hampel et al (1986) and Staudte and Sheather (1990). Other than subject matter changes, there have been some minor modifications in the presentation.},
	pagetotal = {624},
	publisher = {Springer New York},
	author = {Lehmann, E. L. and Casella, George},
	date = {2003-09-09},
	langid = {english},
	note = {Google-Books-{ID}: 9St7DCbu9AUC},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Applied, Mathematics / Probability \& Statistics / Stochastic Processes}
}

@book{cover_elements_2012,
	title = {Elements of {Information} {Theory}},
	isbn = {978-1-118-58577-1},
	abstract = {The latest edition of this classic is updated with new problem sets and material  The Second Edition of this fundamental textbook maintains the book's tradition of clear, thought-provoking instruction. Readers are provided once again with an instructive mix of mathematics, physics, statistics, and information theory.  All the essential topics in information theory are covered in detail, including entropy, data compression, channel capacity, rate distortion, network information theory, and hypothesis testing. The authors provide readers with a solid understanding of the underlying theory and applications. Problem sets and a telegraphic summary at the end of each chapter further assist readers. The historical notes that follow each chapter recap the main points.  The Second Edition features: * Chapters reorganized to improve teaching * 200 new problems * New material on source coding, portfolio theory, and feedback capacity * Updated references  Now current and enhanced, the Second Edition of Elements of Information Theory remains the ideal textbook for upper-level undergraduate and graduate courses in electrical engineering, statistics, and telecommunications. An Instructor's Manual presenting detailed solutions to all the problems in the book is available from the Wiley editorial department.},
	pagetotal = {688},
	publisher = {John Wiley \& Sons},
	author = {Cover, Thomas M. and Thomas, Joy A.},
	date = {2012-11-28},
	langid = {english},
	note = {Google-Books-{ID}: {VWq}5GG6ycxMC},
	keywords = {Computers / General, Computers / Information Technology}
}

@article{vasicek_test_1976,
	title = {A Test for Normality Based on Sample Entropy},
	volume = {38},
	issn = {0035-9246},
	url = {http://www.jstor.org/stable/2984828},
	doi = {10.2307/2984828},
	abstract = {A test of the composite hypothesis of normality is introduced. The test is based on the property of the normal distribution that its entropy exceeds that of any other distribution with a density that has the same variance. The test statistic is based on a class of estimators of entropy constructed here. The test is shown to be a consistent test of the null hypothesis for all alternatives without a singular continuous part. The power of the test is estimated against several alternatives. It is observed that the test compares favourably with other tests for normality.},
	pages = {54--59},
	number = {1},
	journaltitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Vasicek, Oldrich},
	urldate = {2017-08-11},
	date = {1976}
}

@online{carpenter_typical_????,
	title = {Typical Sets and the Curse of Dimensionality},
	url = {http://mc-stan.org/users/documentation/case-studies/curse-dims.html},
	titleaddon = {{STAN} Case Studies},
	author = {Carpenter, Bob},
	urldate = {2017-08-11},
	file = {Typical Sets and the Curse of Dimensionality:/home/samuel/Zotero/storage/X425N68I/curse-dims.html:text/html}
}

@book{voss_introduction_2013,
	title = {An {Introduction} to {Statistical} {Computing}: A {Simulation}-{Based} {Approach}},
	isbn = {978-1-118-72802-4},
	shorttitle = {An Introduction to Statistical Computing},
	abstract = {A comprehensive introduction to sampling-based methods in statistical computing The use of computers in mathematics and statistics has opened up a wide range of techniques for studying otherwise intractable problems. Sampling-based simulation techniques are now an invaluable tool for exploring statistical models. This book gives a comprehensive introduction to the exciting area of sampling-based methods. An Introduction to Statistical Computing introduces the classical topics of random number generation and Monte Carlo methods. It also includes some advanced methods such as the reversible jump Markov chain Monte Carlo algorithm and modern methods such as approximate Bayesian computation and multilevel Monte Carlo techniques An Introduction to Statistical Computing:  Fully covers the traditional topics of statistical computing. Discusses both practical aspects and the theoretical background. Includes a chapter about continuous-time models. Illustrates all methods using examples and exercises. Provides answers to the exercises (using the statistical computing environment R); the corresponding source code is available online. Includes an introduction to programming in R.  This book is mostly self-contained; the only prerequisites are basic knowledge of probability up to the law of large numbers. Careful presentation and examples make this book accessible to a wide range of students and suitable for self-study or as the basis of a taught course},
	pagetotal = {305},
	publisher = {John Wiley \& Sons},
	author = {Voss, Jochen},
	date = {2013-08-28},
	langid = {english},
	note = {Google-Books-{ID}: 1UqaAAAAQBAJ},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes}
}

@book{agresti_foundations_2015,
	title = {Foundations of Linear and Generalized Linear Models},
	isbn = {978-1-118-73003-4},
	abstract = {A valuable overview of the most important ideas and results in statistical analysis Written by a highly-experienced author, Foundations of Linear and Generalized Linear Models is a clear and comprehensive guide to the key concepts and results of linear statistical models. The book presents a broad, in-depth overview of the most commonly used statistical models by discussing the theory underlying the models, R software applications, and examples with crafted models to elucidate key ideas and promote practical model building.The book begins by illustrating the fundamentals of linear models, such as how the model-fitting projects the data onto a model vector subspace and how orthogonal decompositions of the data yield information about effects of explanatory variables. Subsequently, the book covers the most popular generalized linear models, which include binomial and multinomial logistic regression for categorical data, and Poisson and negative binomial loglinear models for count data. Focusing on the theoretical underpinnings of these models, Foundations of Linear and Generalized Linear Models also features:An introduction to quasi-likelihood methods that require weaker distributional assumptions, such as generalized estimating equation {methodsAn} overview of linear mixed models and generalized linear mixed models with random effects for clustered correlated data, Bayesian modeling, and extensions to handle problematic cases such as high-dimensional {problemsNumerous} examples that use R software for all text data {analysesMore} than 400 exercises for readers to practice and extend the theory, methods, and data {analysisA} supplementary website with datasets for the examples and exercises An invaluable textbook for upper-undergraduate and graduate-level students in statistics and biostatistics courses, Foundations of Linear and Generalized Linear Models is also an excellent reference for practicing statisticians and biostatisticians, as well as anyone who is interested in learning about the most important statistical models for analyzing data.},
	pagetotal = {472},
	publisher = {Wiley},
	author = {Agresti, Alan},
	date = {2015-02-09},
	langid = {english},
	note = {Google-Books-{ID}: {bWiAoAEACAAJ}},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes}
}

@inproceedings{donoho_high-dimensional_2000,
	title = {High-dimensional data analysis: The curses and blessings of dimensionality},
	shorttitle = {High-dimensional data analysis},
	abstract = {The coming century is surely the century of data. A combination of blind faith and serious purpose makes our society invest massively in the collection and processing of data of all kinds, on scales unimaginable until recently. Hyperspectral Imagery, Internet Portals, Financial tick-by-tick data, and {DNA} Microarrays are just a few of the betterknown sources, feeding data in torrential streams into scientific and business databases worldwide. In traditional statistical data analysis, we think of observations of instances of particular phenomena (e.g. instance ↔ human being), these observations being a vector of values we measured on several variables (e.g. blood pressure, weight, height,...). In traditional statistical methodology, we assumed many observations and a few, wellchosen variables. The trend today is towards more observations but even more so, to radically larger numbers of variables – voracious, automatic, systematic collection of hyper-informative detail about each observed instance. We are seeing examples where the observations gathered on individual instances are curves, or spectra, or images, or},
	booktitle = {Ams Conference on Math Challenges of the 21st Century},
	author = {Donoho, David L. and Tukey, John},
	date = {2000}
}

@article{liang_statistical_2008,
	title = {Statistical advances and challenges for analyzing correlated high dimensional {SNP} data in genomic study for complex diseases},
	volume = {2},
	issn = {1935-7516},
	url = {http://projecteuclid.org/euclid.ssu/1206734290},
	doi = {10.1214/07-SS026},
	abstract = {Recent advances of information technology in biomedical sciences and other applied areas have created numerous large diverse data sets with a high dimensional feature space, which provide us a tremendous amount of information and new opportunities for improving the quality of human life. Meanwhile, great challenges are also created driven by the continuous arrival of new data that requires researchers to convert these raw data into scientific knowledge in order to benefit from it. Association studies of complex diseases using {SNP} data have become more and more popular in biomedical research in recent years. In this paper, we present a review of recent statistical advances and challenges for analyzing correlated high dimensional {SNP} data in genomic association studies for complex diseases. The review includes both general feature reduction approaches for high dimensional correlated data and more specific approaches for {SNPs} data, which include unsupervised haplotype mapping, tag {SNP} selection, and supervised {SNPs} selection using statistical testing/scoring, statistical modeling and machine learning methods with an emphasis on how to identify interacting loci.},
	pages = {43--60},
	journaltitle = {Statistics Surveys},
	shortjournal = {Statist. Surv.},
	author = {Liang, Yulan and Kelemen, Arpad},
	date = {2008},
	mrnumber = {MR2520980},
	zmnumber = {1196.62144},
	keywords = {Complex disease, High dimensional data, Single Nucleotide Polymorphism, Statistical methods},
	file = {Snapshot:/home/samuel/Zotero/storage/HLTHKEQP/1206734290.html:text/html}
}

@article{clarke_properties_2008,
	title = {The properties of high-dimensional data spaces: implications for exploring gene and protein expression data},
	volume = {8},
	issn = {1474-175X},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238676/},
	doi = {10.1038/nrc2294},
	shorttitle = {The properties of high-dimensional data spaces},
	abstract = {High-throughput genomic and proteomic technologies are widely used in cancer research to build better predictive models of diagnosis, prognosis and therapy, to identify and characterize key signalling networks and to find new targets for drug development. These technologies present investigators with the task of extracting meaningful statistical and biological information from high-dimensional data spaces, wherein each sample is defined by hundreds or thousands of measurements, usually concurrently obtained. The properties of high dimensionality are often poorly understood or overlooked in data modelling and analysis. From the perspective of translational science, this Review discusses the properties of high-dimensional data spaces that arise in genomic and proteomic studies and the challenges they can pose for data analysis and interpretation.},
	pages = {37--49},
	number = {1},
	journaltitle = {Nature reviews. Cancer},
	shortjournal = {Nat Rev Cancer},
	author = {Clarke, Robert and Ressom, Habtom W. and Wang, Antai and Xuan, Jianhua and Liu, Minetta C. and Gehan, Edmund A. and Wang, Yue},
	urldate = {2017-08-10},
	date = {2008-01},
	pmid = {18097463},
	pmcid = {PMC2238676},
	file = {PubMed Central Full Text PDF:/home/samuel/Zotero/storage/LMFWIULU/Clarke et al. - 2008 - The properties of high-dimensional data spaces im.pdf:application/pdf}
}

@book{buhlmann_statistics_2011,
	title = {Statistics for High-Dimensional Data: Methods, Theory and Applications},
	isbn = {978-3-642-20192-9},
	shorttitle = {Statistics for High-Dimensional Data},
	abstract = {Modern statistics deals with large and complex data sets, and consequently with models containing a large number of parameters. This book presents a detailed account of recently developed approaches, including the Lasso and versions of it for various models, boosting methods, undirected graphical modeling, and procedures controlling false positive selections.A special characteristic of the book is that it contains comprehensive mathematical theory on high-dimensional statistics combined with methodology, algorithms and illustrations with real data examples. This in-depth approach highlights the methods’ great potential and practical applicability in a variety of settings. As such, it is a valuable resource for researchers, graduate students and experts in statistics, applied mathematics and computer science.},
	pagetotal = {568},
	publisher = {Springer Science \& Business Media},
	author = {Bühlmann, Peter and Geer, Sara van de},
	date = {2011-06-08},
	langid = {english},
	note = {Google-Books-{ID}: S6jYXmh988UC},
	keywords = {Mathematics / Probability \& Statistics / General, Computers / Mathematical \& Statistical Software, Mathematics / Probability \& Statistics / Stochastic Processes, Mathematics / Discrete Mathematics}
}

@article{johnstone_statistical_2009,
	title = {Statistical challenges of high-dimensional data},
	volume = {367},
	rights = {© 2009 The Royal Society},
	issn = {1364-503X, 1471-2962},
	url = {http://rsta.royalsocietypublishing.org/content/367/1906/4237},
	doi = {10.1098/rsta.2009.0159},
	abstract = {Modern applications of statistical theory and methods can involve extremely large datasets, often with huge numbers of measurements on each of a comparatively small number of experimental units. New methodology and accompanying theory have emerged in response: the goal of this Theme Issue is to illustrate a number of these recent developments. This overview article introduces the difficulties that arise with high-dimensional data in the context of the very familiar linear statistical model: we give a taste of what can nevertheless be achieved when the parameter vector of interest is sparse, that is, contains many zero elements. We describe other ways of identifying low-dimensional subspaces of the data space that contain all useful information. The topic of classification is then reviewed along with the problem of identifying, from within a very large set, the variables that help to classify observations. Brief mention is made of the visualization of high-dimensional data and ways to handle computational problems in Bayesian analysis are described. At appropriate points, reference is made to the other papers in the issue.},
	pages = {4237--4253},
	number = {1906},
	journaltitle = {Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences},
	author = {Johnstone, Iain M. and Titterington, D. Michael},
	date = {2009-11-13},
	langid = {english},
	pmid = {19805443},
	file = {Full Text PDF:/home/samuel/Zotero/storage/ZQWZFQIF/Johnstone and Titterington - 2009 - Statistical challenges of high-dimensional data.pdf:application/pdf;Snapshot:/home/samuel/Zotero/storage/LDHW38KL/4237.html:text/html}
}

@book{ramsay_functional_2005,
	title = {Functional Data Analysis},
	isbn = {978-0-387-40080-8},
	abstract = {Scientists and others today often collect samples of curves and other functional observations. This monograph presents many ideas and techniques for such data.  Included are expressions in the functional domain of such classics as linear regression, principal components analysis, linear modeling, and canonical correlation analysis, as well as specifically functional techniques such as curve registration and principal differential analysis. Data arising in real applications are used throughout for both motivation and illustration, showing how functional approaches allow us to see new things, especially by exploiting the smoothness of the processes generating the data. The data sets exemplify the wide scope of functional data analysis; they are drawn from growth analysis, meteorology, biomechanics, equine science, economics, and medicine. The book presents novel statistical technology, much of it based on the authors’ own research work, while keeping the mathematical level widely accessible. It is designed to appeal to students, to applied data analysts, and to experienced researchers; it will have value both within statistics and across a broad spectrum of other fields.   This second edition is aimed at a wider range of readers, and especially those who would like to apply these techniques to their research problems. It complements the authors' other recent volume Applied Functional Data Analysis: Methods and Case Studies. In particular, there is an extended coverage of data smoothing and other matters arising in the preliminaries to a functional data analysis. The chapters on the functional linear model and modeling of the dynamics of systems through the use of differential equations and principal differential analysis have been completely rewritten and extended to include new developments. Other chapters have been revised substantially, often to give more weight to examples and practical considerations.  Jim Ramsay is Professor of Psychology at {McGill} University and is an international authority on many aspects of multivariate analysis. He was President of the Statistical Society of Canada in 2002-3 and holds the Society’s Gold Medal for his work in functional data analysis. Bernard Silverman is Master of St Peter’s College and Professor of Statistics at Oxford University. He was President of the Institute of Mathematical Statistics in 2000–1. He is a Fellow of the Royal Society. His main specialty is in computational statistics, and he is the author or editor of several highly regarded books in this area.},
	pagetotal = {464},
	publisher = {Springer Science \& Business Media},
	author = {Ramsay, James and Silverman, B. W.},
	date = {2005-06-08},
	langid = {english},
	note = {Google-Books-{ID}: {mU}3dop5wY\_4C},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes}
}

@book{carlin_bayes_2010,
	title = {Bayes and Empirical Bayes Methods for Data Analysis, Second Edition},
	isbn = {978-1-4200-5766-9},
	abstract = {In recent years, Bayes and empirical Bayes ({EB}) methods have continued to increase in popularity and impact. Building on the first edition of their popular text, Carlin and Louis introduce these methods, demonstrate their usefulness in challenging applied settings, and show how they can be implemented using modern Markov chain Monte Carlo ({MCMC}) methods. Their presentation is accessible to those new to Bayes and empirical Bayes methods, while providing in-depth coverage valuable to seasoned practitioners.With its broad appeal as a text for those in biomedical science, education, social science, agriculture, and engineering, this second edition offers a relatively gentle and comprehensive introduction for students and practitioners already familiar with more traditional frequentist statistical methods. Focusing on practical tools for data analysis, the book shows how properly structured Bayes and {EB} procedures typically have good frequentist and Bayesian performance, both in theory and in practice.},
	pagetotal = {448},
	publisher = {Taylor \& Francis},
	author = {Carlin, Bradley P. and Louis, Thomas A.},
	date = {2010-12-12},
	langid = {english},
	note = {Google-Books-{ID}: 484r1P5o0hQC},
	keywords = {Mathematics / Probability \& Statistics / General, Science / Life Sciences / Biology, Psychology / Research \& Methodology, Mathematics / General}
}

@article{fisher_mathematical_1922,
	title = {On the Mathematical Foundations of Theoretical Statistics},
	volume = {222},
	issn = {0264-3952},
	url = {http://www.jstor.org/stable/91208},
	doi = {10.2307/91208},
	pages = {309--368},
	journaltitle = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
	author = {Fisher, R. A.},
	urldate = {2017-08-19},
	date = {1922}
}

@book{lindley_introduction_1965,
	title = {Introduction to Probability and Statistics from a Bayesian Viewpoint, Part 2, Inference},
	isbn = {978-0-521-05563-5},
	abstract = {The two parts of this book treat probability and statistics as mathematical disciplines and with the same degree of rigour as is adopted for other branches of applied mathematics at the level of a British honours degree. They contain the minimum information about these subjects that any honours graduate in mathematics ought to know. They are written primarily for general mathematicians, rather than for statistical specialists or for natural scientists who need to use statistics in their work. No previous knowledge of probability or statistics is assumed, though familiarity with calculus and linear algebra is required. The first volume takes the theory of probability sufficiently far to be able to discuss the simpler random processes, for example, queueing theory and random walks. The second volume deals with statistics, the theory of making valid inferences from experimental data, and includes an account of the methods of least squares and maximum likelihood; it uses the results of the first volume.},
	pagetotal = {312},
	publisher = {{CUP} Archive},
	author = {Lindley, D. V.},
	date = {1965},
	langid = {english},
	note = {Google-Books-{ID}: 4Y84AAAAIAAJ},
	keywords = {Mathematics / Probability \& Statistics / General}
}

@article{neyman_outline_1937,
	title = {Outline of a Theory of Statistical Estimation Based on the Classical Theory of Probability},
	volume = {236},
	issn = {0080-4614},
	url = {http://www.jstor.org/stable/91337},
	doi = {10.2307/91337},
	pages = {333--380},
	number = {767},
	journaltitle = {Philosophical Transactions of the Royal Society of London. Series A, Mathematical and Physical Sciences},
	author = {Neyman, J.},
	urldate = {2017-08-19},
	date = {1937}
}

@article{tierney_accurate_1986,
	title = {Accurate Approximations for Posterior Moments and Marginal Densities},
	volume = {81},
	issn = {0162-1459},
	url = {http://amstat.tandfonline.com/doi/abs/10.1080/01621459.1986.10478240},
	doi = {10.1080/01621459.1986.10478240},
	abstract = {This article describes approximations to the posterior means and variances of positive functions of a real or vector-valued parameter, and to the marginal posterior densities of arbitrary (i.e., not necessarily positive) parameters. These approximations can also be used to compute approximate predictive densities. To apply the proposed method, one only needs to be able to maximize slightly modified likelihood functions and to evaluate the observed information at the maxima. Nevertheless, the resulting approximations are generally as accurate and in some cases more accurate than approximations based on third-order expansions of the likelihood and requiring the evaluation of third derivatives. The approximate marginal posterior densities behave very much like saddle-point approximations for sampling distributions. The principal regularity condition required is that the likelihood times prior be unimodal.},
	pages = {82--86},
	number = {393},
	journaltitle = {Journal of the American Statistical Association},
	shortjournal = {Journal of the American Statistical Association},
	author = {Tierney, Luke and Kadane, Joseph B.},
	urldate = {2017-08-19},
	date = {1986-03-01},
	file = {Snapshot:/home/samuel/Zotero/storage/H3N53BB9/01621459.1986.html:text/html}
}

@article{geyer_practical_1992,
	title = {Practical Markov Chain Monte Carlo},
	volume = {7},
	issn = {0883-4237},
	url = {http://www.jstor.org/stable/2246094},
	abstract = {Markov chain Monte Carlo using the Metropolis-Hastings algorithm is a general method for the simulation of stochastic processes having probability densities known up to a constant of proportionality. Despite recent advances in its theory, the practice has remained controversial. This article makes the case for basing all inference on one long run of the Markov chain and estimating the Monte Carlo error by standard nonparametric methods well-known in the time-series and operations research literature. In passing it touches on the Kipnis-Varadhan central limit theorem for reversible Markov chains, on some new variance estimators, on judging the relative efficiency of competing Monte Carlo schemes, on methods for constructing more rapidly mixing Markov chains and on diagnostics for Markov chain Monte Carlo.},
	pages = {473--483},
	number = {4},
	journaltitle = {Statistical Science},
	author = {Geyer, Charles J.},
	urldate = {2017-08-19},
	date = {1992},
	file = {JSTOR Full Text PDF:/home/samuel/Zotero/storage/YA8WHM43/Geyer - 1992 - Practical Markov Chain Monte Carlo.pdf:application/pdf}
}

@book{georgii_gibbs_2011,
	title = {Gibbs Measures and Phase Transitions},
	isbn = {978-3-11-025032-9},
	abstract = {"This book is much more than an introduction to the subject of its title. It covers in depth a broad range of topics in the mathematical theory of phase transition in statistical mechanics and as an up to date reference in its chosen topics it is a work of outstanding scholarship. It is in fact one of the author's stated aims that this comprehensive monograph should serve both as an introductory text and as a reference for the expert. In its latter function it informs the reader about the state of the art in several directions. It is introductory in the sense that it does not assume any prior knowledge of statistical mechanics and is accessible to a general readership of mathematicians with a basic knowledge of measure theory and probability. As such it should contribute considerably to the further growth of the already lively interest in statistical mechanics on the part of probabilists and other mathematicians."Fredos Papangelou, Zentralblatt {MATH} The second edition has been extended by a new section on large deviations and some comments on the more recent developments in the area.},
	pagetotal = {561},
	publisher = {Walter de Gruyter},
	author = {Georgii, Hans-Otto},
	date = {2011-05-31},
	langid = {english},
	note = {Google-Books-{ID}: Xl\_NocttwvAC},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes, Mathematics / General, Science / Physics / Mathematical \& Computational}
}

@article{jackman_estimation_2000,
	title = {Estimation and Inference via Bayesian Simulation: An Introduction to Markov Chain Monte Carlo},
	volume = {44},
	issn = {0092-5853},
	url = {http://www.jstor.org/stable/2669318},
	shorttitle = {Estimation and Inference via Bayesian Simulation},
	abstract = {Bayesian statistics have made great strides in recent years, developing a class of methods for estimation and inference via stochastic simulation known as Markov Chain Monte Carlo ({MCMC}) methods. {MCMC} constitutes a revolution in statistical practice with effects beginning to be felt in the social sciences: models long consigned to the "too hard" basket are now within reach of quantitative researchers. I review the statistical pedigree of {MCMC} and the underlying statistical concepts. I demonstrate some of the strengths and weaknesses of {MCMC} and offer practical suggestions for using {MCMC} in social-science settings. Simple, illustrative examples include a probit model of voter turnout and a linear regression for time-series data with autoregressive disturbances. I conclude with a more challenging application, a multinomial probit model, to showcase the power of {MCMC} methods.},
	pages = {375--404},
	number = {2},
	journaltitle = {American Journal of Political Science},
	author = {Jackman, Simon},
	urldate = {2017-08-19},
	date = {2000},
	file = {JSTOR Full Text PDF:/home/samuel/Zotero/storage/238GNM22/Jackman - 2000 - Estimation and Inference via Bayesian Simulation .pdf:application/pdf}
}

@article{lunn_winbugs_2000,
  title={{WinBUGS}-a {Bayesian} {Modelling} {Framework}: {Concepts}, {Structure}, and {Extensibility}},
  author={Lunn, David J and Thomas, Andrew and Best, Nicky and Spiegelhalter, David},
  journal={Statistics and Computing},
  volume={10},
  number={4},
  pages={325--337},
  year={2000},
  publisher={Springer}
}

@misc{spiegelhalter_bugs:_1999,
	title = {{BUGS}: Bayesian Inference Using Gibbs Sampling, Version 0.5 (version ii)},
	shorttitle = {{BUGS}},
	abstract = {On Jul 21, 1999 David Spiegelhalter (and others) published: {BUGS}: Bayesian Inference Using Gibbs Sampling, Version 0.5 (version ii)},
	publisher = {Medical Research Council Biostatistics Unit},
	author = {Spiegelhalter, David and Thomas, Andrew and Best, Nicky and Gilks, Wally},
	date = {1999},
	file = {Snapshot:/home/samuel/Zotero/storage/GQIE3V5Z/2418327_BUGS_Bayesian_Inference_Using_Gibbs_Sampling_Version_05_version_ii.html:text/html}
}

@article{lunn_bugs_2009,
	title = {The {BUGS} project: Evolution, critique and future directions},
	volume = {28},
	issn = {1097-0258},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/sim.3680/abstract},
	doi = {10.1002/sim.3680},
	shorttitle = {The {BUGS} project},
	abstract = {{BUGS} is a software package for Bayesian inference using Gibbs sampling. The software has been instrumental in raising awareness of Bayesian modelling among both academic and commercial communities internationally, and has enjoyed considerable success over its 20-year life span. Despite this, the software has a number of shortcomings and a principal aim of this paper is to provide a balanced critical appraisal, in particular highlighting how various ideas have led to unprecedented flexibility while at the same time producing negative side effects. We also present a historical overview of the {BUGS} project and some future perspectives. Copyright © 2009 John Wiley \& Sons, Ltd.},
	pages = {3049--3067},
	number = {25},
	journaltitle = {Statistics in Medicine},
	shortjournal = {Statist. Med.},
	author = {Lunn, David and Spiegelhalter, David and Thomas, Andrew and Best, Nicky},
	urldate = {2017-08-19},
	date = {2009-11-10},
	langid = {english},
	keywords = {Bayesian modelling, {BUGS}, graphical models, {OpenBUGS}, {WinBUGS}},
	file = {Full Text PDF:/home/samuel/Zotero/storage/L6P9VAZQ/Lunn et al. - 2009 - The BUGS project Evolution, critique and future d.pdf:application/pdf;Snapshot:/home/samuel/Zotero/storage/843WF3ZF/abstract.html:text/html}
}

@misc{r_core_team_r:_2017,
	title = {R:  A {Language} and {Environment} for {Statistical} {Computing}},
	url = {https://www.R-project.org},
	publisher = {R Foundation for Statistical Computing; Vienna, Austria},
	author = {R Core Team},
	date = {2017}
}

@misc{nimble_development_team_nimble:_2017,
	title = {{NIMBLE}:  An R Package for Programming with {BUGS} models, version 0.6-6},
	url = {http://r-nimble.org},
	author = {{NIMBLE} Development Team},
	date = {2017}
}

@inproceedings{plummer_jags:_2003,
	title = {\{{JAGS}\}: A program for analysis of \{Bayesian\} graphical models using \{Gibbs\} sampling},
	shorttitle = {\{{JAGS}\}},
	eventtitle = {Proceedings of the 3rd International Workshop on Distributed Statistical Computing},
	author = {Plummer, Martyn},
	date = {2003},
	keywords = {jags, software, statistics}
}

@article{carpenter_stan:_2017,
  title={Stan: {A} {Probabilistic} {Programming} {Language}},
  author={Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
  journal={Journal of Statistical Software},
  volume={76},
  number={1},
  year={2017},
  publisher={Columbia Univ., New York, NY (United States); Harvard Univ., Cambridge, MA~…}
}

@article{metropolis_monte_1949,
	title = {The {Monte} {Carlo} {Method}},
	volume = {44},
	issn = {0162-1459},
	url = {http://amstat.tandfonline.com/doi/abs/10.1080/01621459.1949.10483310},
	doi = {10.1080/01621459.1949.10483310},
	abstract = {We shall present here the motivation and a general description of a method dealing with a class of problems in mathematical physics. The method is, essentially, a statistical approach to the study of differential equations, or more generally, of integro-differential equations that occur in various branches of the natural sciences.},
	pages = {335--341},
	number = {247},
	journaltitle = {Journal of the American Statistical Association},
	shortjournal = {Journal of the American Statistical Association},
	author = {Metropolis, Nicholas and Ulam, S.},
	urldate = {2017-08-19},
	date = {1949-09-01},
	file = {Snapshot:/home/samuel/Zotero/storage/Z74XN4TM/01621459.1949.html:text/html}
}

@article{al-rfou_theano:_2016,
  title={Theano: {A} {Python} {Framework} for {Fast} {Computation} of {Mathematical} {Expressions}},
  author={Team, The Theano Development and Al-Rfou, Rami and Alain, Guillaume and Almahairi, Amjad and Angermueller, Christof and Bahdanau, Dzmitry and Ballas, Nicolas and Bastien, Fr{\'e}d{\'e}ric and Bayer, Justin and Belikov, Anatoly and others},
  journal={arXiv preprint arXiv:1605.02688},
  pages={1--19},
  year={2016}
}

@article{betancourt_convergence_2017,
	title = {The {Convergence} of {Markov} {Chain} {Monte} {Carlo} {Methods}: {From} the {Metropolis} {Method} to {Hamiltonian} {Monte} {Carlo}},
	url = {http://arxiv.org/abs/1706.01520},
	shorttitle = {The Convergence of Markov chain Monte Carlo Methods},
	abstract = {From its inception in the 1950s to the modern frontiers of applied statistics, Markov chain Monte Carlo has been one of the most ubiquitous and successful methods in statistical computing. In that time its development has been fueled by increasingly difficult problems and novel techniques from physics. In this article I will review the history of Markov chain Monte Carlo from its inception with the Metropolis method to today's state-of-the-art in Hamiltonian Monte Carlo. Along the way I will focus on the evolving interplay between the statistical and physical perspectives of the method.},
	journaltitle = {{arXiv}:1706.01520 [physics, stat]},
	author = {Betancourt, Michael},
	urldate = {2017-08-19},
	date = {2017-06-05},
	eprinttype = {arxiv},
	eprint = {1706.01520},
	keywords = {Statistics - Methodology, Physics - History and Philosophy of Physics},
	file = {arXiv\:1706.01520 PDF:/home/samuel/Zotero/storage/MFDZ2YWD/Betancourt - 2017 - The Convergence of Markov chain Monte Carlo Method.pdf:application/pdf;arXiv.org Snapshot:/home/samuel/Zotero/storage/K84TKVHZ/1706.html:text/html}
}

@article{robert_short_2011,
	title = {A Short History of Markov Chain Monte Carlo: Subjective Recollections from Incomplete Data},
	volume = {26},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/euclid.ss/1307626568},
	doi = {10.1214/10-STS351},
	shorttitle = {A Short History of Markov Chain Monte Carlo},
	abstract = {We attempt to trace the history and development of Markov chain Monte Carlo ({MCMC}) from its early inception in the late 1940s through its use today. We see how the earlier stages of Monte Carlo ({MC}, not {MCMC}) research have led to the algorithms currently in use. More importantly, we see how the development of this methodology has not only changed our solutions to problems, but has changed the way we think about problems.},
	pages = {102--115},
	number = {1},
	journaltitle = {Statistical Science},
	shortjournal = {Statist. Sci.},
	author = {Robert, Christian and Casella, George},
	date = {2011-02},
	mrnumber = {MR2849912},
	zmnumber = {1222.65006},
	keywords = {Bayesian methods, Gibbs sampling, hierarchical models, Metropolis–Hasting algorithm},
	file = {Snapshot:/home/samuel/Zotero/storage/UI2QQNEF/1307626568.html:text/html}
}

@article{cowles_markov_1996,
	title = {Markov Chain Monte Carlo Convergence Diagnostics: A Comparative Review},
	volume = {91},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/2291683},
	shorttitle = {Markov Chain Monte Carlo Convergence Diagnostics},
	abstract = {A critical issue for users of Markov chain Monte Carlo ({MCMC}) methods in applications is how to determine when it is safe to stop sampling and use the samples to estimate characteristics of the distribution of interest. Research into methods of computing theoretical convergence bounds holds promise for the future but to date has yielded relatively little of practical use in applied work. Consequently, most {MCMC} users address the convergence problem by applying diagnostic tools to the output produced by running their samplers. After giving a brief overview of the area, we provide an expository review of 13 convergence diagnostics, describing the theoretical basis and practical implementation of each. We then compare their performance in two simple models and conclude that all of the methods can fail to detect the sorts of convergence failure that they were designed to identify. We thus recommend a combination of strategies aimed at evaluating and accelerating {MCMC} sampler convergence, including applying diagnostic procedures to a small number of parallel chains, monitoring autocorrelations and cross-correlations, and modifying parametrizations or sampling algorithms appropriately. We emphasize, however, that it is not possible to say with certainty that a finite sample from an {MCMC} algorithm is representative of an underlying stationary distribution.},
	pages = {883--904},
	number = {434},
	journaltitle = {Journal of the American Statistical Association},
	author = {Cowles, Mary Kathryn and Carlin, Bradley P.},
	urldate = {2017-08-19},
	date = {1996},
	file = {JSTOR Full Text PDF:/home/samuel/Zotero/storage/AVDAA6N2/Cowles and Carlin - 1996 - Markov Chain Monte Carlo Convergence Diagnostics .pdf:application/pdf}
}

@article{carter_gibbs_1994,
	title = {On Gibbs sampling for state space models},
	volume = {81},
	issn = {0006-3444},
	url = {https://academic.oup.com/biomet/article/81/3/541/257082/On-Gibbs-sampling-for-state-space-models},
	doi = {10.1093/biomet/81.3.541},
	abstract = {{SUMMARYWe} show how to use the Gibbs sampler to carry out Bayesian inference on a linear state space model with errors that are a mixture of normals and coefficients that can switch over time. Our approach simultaneously generates the whole of the state vector given the mixture and coefficient indicator variables and simultaneously generates all the indicator variables conditional on the state vectors. The states are generated efficiently using the Kalman filter. We illustrate our approach by several examples and empirically compare its performance to another Gibbs sampler where the states are generated one at a time. The empirical results suggest that our approach is both practical to implement and dominates the Gibbs sampler that generates the states one at a time.},
	pages = {541--553},
	number = {3},
	journaltitle = {Biometrika},
	shortjournal = {Biometrika},
	author = {Carter, C. K. and Kohn, R.},
	date = {1994-09-01},
	file = {Snapshot:/home/samuel/Zotero/storage/BLXF8G8H/257082.html:text/html}
}

@book{gilks_markov_1995,
	title = {{Markov} {Chain} {Monte} {Carlo} in {Practice}},
	isbn = {978-0-412-05551-5},
	abstract = {In a family study of breast cancer, epidemiologists in Southern California increase the power for detecting a gene-environment interaction. In Gambia, a study helps a vaccination program reduce the incidence of Hepatitis B carriage. Archaeologists in Austria place a Bronze Age site in its true temporal location on the calendar scale. And in France, researchers map a rare disease with relatively little variation.Each of these studies applied Markov chain Monte Carlo methods to produce more accurate and inclusive results. General state-space Markov chain theory has seen several developments that have made it both more accessible and more powerful to the general statistician. Markov Chain Monte Carlo in Practice introduces {MCMC} methods and their applications, providing some theoretical background as well. The authors are researchers who have made key contributions in the recent development of {MCMC} methodology and its application. Considering the broad audience, the editors emphasize practice rather than theory, keeping the technical content to a minimum. The examples range from the simplest application, Gibbs sampling, to more complex applications. The first chapter contains enough information to allow the reader to start applying {MCMC} in a basic way. The following chapters cover main issues, important concepts and results, techniques for implementing {MCMC}, improving its performance, assessing model adequacy, choosing between models, and applications and their domains.Markov Chain Monte Carlo in Practice is a thorough, clear introduction to the methodology and applications of this simple idea with enormous potential. It shows the importance of {MCMC} in real applications, such as archaeology, astronomy, biostatistics, genetics, epidemiology, and image analysis, and provides an excellent base for {MCMC} to be applied to other fields as well.},
	pagetotal = {522},
	publisher = {{CRC} Press},
	author = {Gilks, W. R. and Richardson, S. and Spiegelhalter, David},
	date = {1995-12-01},
	langid = {english},
	note = {Google-Books-{ID}: {TRXrMWY}\_i2IC},
	keywords = {Mathematics / Probability \& Statistics / General, Science / Life Sciences / Biology}
}

@misc{stan_development_team_stan_2017,
	title = {Stan {Modeling} {Language} {Users} {Guide} and {Reference} {Manual}},
	url = {http://mc-stan.org},
	author = {{STAN} Development Team},
	date = {2017}
}

@online{_data_science1,
	title = {In data science, the R language is swallowing Python {\textbar} {InfoWorld}},
	url = {http://www.infoworld.com/article/2951779/application-development/in-data-science-the-r-language-is-swallowing-python.html},
	urldate = {2017-08-19},
	file = {In data science, the R language is swallowing Python | InfoWorld:/home/samuel/Zotero/storage/XGFS2YA2/in-data-science-the-r-language-is-swallowing-python.html:text/html}
}

@article{crabill_sufficient_1968,
	title = {Sufficient Conditions for Positive Recurrence and Recurrence of Specially Structured Markov Chains},
	volume = {16},
	issn = {0030-364X},
	url = {http://www.jstor.org/stable/168304},
	abstract = {This paper considers a specially structured Markov chain that arises naturally in certain queuing problems. Sufficient conditions for positive recurrence and recurrence are obtained using the concept of conditional expected one-step displacement of the chain. The proof makes use of the fundamental dual theorem of linear programming. A numerical example is given showing that the sufficient conditions are not necessary. The results of this paper apply to certain aspects of previous work by Harris on "Queues with State Dependent Stochastic Service Rates," (1967) in this Journal. Specifically, the results of this paper generalize the sufficient conditions for recurrence given by Theorems 1 and 2 of that paper.},
	pages = {858--867},
	number = {4},
	journaltitle = {Operations Research},
	author = {Crabill, Thomas B.},
	urldate = {2017-08-22},
	date = {1968},
	file = {JSTOR Full Text PDF:/home/samuel/Zotero/storage/TBVTQ3HJ/Crabill - 1968 - Sufficient Conditions for Positive Recurrence and .pdf:application/pdf}
}

@article{pakes_conditions_1969,
	title = {Some Conditions for Ergodicity and Recurrence of Markov Chains},
	volume = {17},
	issn = {0030-364X},
	url = {http://www.jstor.org/stable/168325},
	abstract = {This paper finds sufficient conditions for the ergodicity and recurrence of irreducible and aperiodic Markov chains. They extend some of the ones commonly used. The paper also indicates their use in discussing a certain class of queuing problem with state dependent service times.},
	pages = {1058--1061},
	number = {6},
	journaltitle = {Operations Research},
	author = {Pakes, A. G.},
	urldate = {2017-08-22},
	date = {1969},
	file = {JSTOR Full Text PDF:/home/samuel/Zotero/storage/UNSR3GXI/Pakes - 1969 - Some Conditions for Ergodicity and Recurrence of M.pdf:application/pdf}
}

@book{grinstead_introduction_1997,
	title = {Introduction to Probability},
	isbn = {978-0-8218-0749-1},
	abstract = {This text is designed for an introductory probability course at the university level for undergraduates in mathematics, the physical and social sciences, engineering, and computer science. It presents a thorough treatment of probability ideas and techniques necessary for a firm understanding of the subject.},
	pagetotal = {536},
	publisher = {American Mathematical Soc.},
	author = {Grinstead, Charles Miller and Snell, James Laurie},
	date = {1997},
	langid = {english},
	note = {Google-Books-{ID}: 14oq4uWGCkwC},
	keywords = {Mathematics / Probability \& Statistics / General}
}

@book{bertsekas_introduction_2002,
	title = {Introduction to Probability},
	isbn = {978-1-886529-40-3},
	pagetotal = {440},
	publisher = {Athena Scientific},
	author = {Bertsekas, Dimitri P. and Tsitsiklis, John N.},
	date = {2002},
	langid = {english},
	note = {Google-Books-{ID}: {bcHaAAAAMAAJ}},
	keywords = {Mathematics / Probability \& Statistics / General}
}

@incollection{gallager_finite_1996,
	title = {Finite State Markov Chains},
	isbn = {978-1-4613-5986-9 978-1-4615-2329-1},
	url = {https://link.springer.com/chapter/10.1007/978-1-4615-2329-1_4},
	series = {The Springer International Series in Engineering and Computer Science},
	abstract = {The counting processes \{N(t),t≥0\} of Chapters 2 and 3 have the property that N(t)changesat discrete instants of time, butis definedfor all real t ≥ 0. Such stochastic processes are generally called continuous time processes. The Markov chains to be discussed in this and the next chapter are stochastic processesdefinedonly at integer values of time, n = 0, 1,….At each integer time n ≥ 0, there is a random variable Xncalled thestateat time n, and the process is then the family of random variables \{Xn,n≥0\}. These processes are often called discrete time processes, but we prefer the more specific term integer time processes. An integer time process \{Xn;n≥0\} can also be viewed as a continuous time process \{X(t);t≥0\} by taking X(t)={XnX}(t)={XnX}(t) = X\_n for n≤t{\textless}n+l, but since changes only occur at integer times, it is usually simpler to view the process only at integer times.},
	pages = {103--147},
	booktitle = {Discrete Stochastic Processes},
	publisher = {Springer, Boston, {MA}},
	author = {Gallager, Robert G.},
	date = {1996},
	langid = {english},
	note = {{DOI}: 10.1007/978-1-4615-2329-1\_4},
	file = {Snapshot:/home/samuel/Zotero/storage/YB2U36BG/978-1-4615-2329-1_4.html:text/html}
}

@article{athreya_convergence_1996,
	title = {On the convergence of the {Markov} chain simulation method},
	volume = {24},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1033066200},
	doi = {10.1214/aos/1033066200},
	abstract = {The Markov chain simulation method has been successfully used in many problems, including some that arise in Bayesian statistics. We give a self-contained proof of the convergence of this method in general state spaces under conditions that are easy to verify.},
	pages = {69--100},
	number = {1},
	journaltitle = {The Annals of Statistics},
	shortjournal = {Ann. Statist.},
	author = {Athreya, Krishna B. and Doss, Hani and Sethuraman, Jayaram},
	date = {1996-02},
	mrnumber = {MR1389881},
	zmnumber = {0860.60057},
	keywords = {Calculation of posterior distributions, ergodic theorem, successive substitution sampling},
	file = {Snapshot:/home/samuel/Zotero/storage/E7NMCMMM/1033066200.html:text/html}
}

@article{parry_intrinsic_1964,
	title = {Intrinsic Markov Chains},
	volume = {112},
	issn = {0002-9947},
	url = {http://www.jstor.org/stable/1994009},
	pages = {55--66},
	number = {1},
	journaltitle = {Transactions of the American Mathematical Society},
	author = {Parry, William},
	urldate = {2017-08-23},
	date = {1964},
	file = {JSTOR Full Text PDF:/home/samuel/Zotero/storage/JVZHRRIG/Parry - 1964 - Intrinsic Markov Chains.pdf:application/pdf}
}

@online{weisstein_left_2017,
	title = {Left Eigenvector},
	rights = {Copyright 1999-2017 Wolfram Research, Inc.  See http://mathworld.wolfram.com/about/terms.html for a full terms of use statement.},
	url = {http://mathworld.wolfram.com/LeftEigenvector.html},
	abstract = {A left eigenvector is defined as a row vector X\_L satisfying  X\_LA=lambda\_LX\_L.   In many common applications, only right eigenvectors (and not left eigenvectors) need be considered. Hence the unqualified term "eigenvector" can be understood to refer to a right eigenvector.},
	titleaddon = {Mathworld - A Wolfram Web Resource},
	type = {Text},
	author = {Weisstein, Eric W.},
	date = {2017-08-22},
	langid = {english},
	file = {Snapshot:/home/samuel/Zotero/storage/LPUSVRXR/LeftEigenvector.html:text/html}
}

@book{ching_markov_2006,
	title = {Markov Chains: Models, Algorithms and Applications},
	isbn = {978-0-387-29335-6},
	shorttitle = {Markov Chains},
	abstract = {{MARKOV} {CHAINS}: Models, Algorithms and Applications outlines recent developments of Markov chain models for modeling queueing sequences, Internet, re-manufacturing systems, reverse logistics, inventory systems, bio-informatics, {DNA} sequences, genetic networks, data mining, and many other practical systems. The book consists of eight chapters. Chapter 1 is a brief introduction to the classical theory on both discrete and continuous time Markov chains. The relationship between Markov chains of finite states and matrix theory is also discussed. Chapter 2 discusses the applications of continuous time Markov chains to model queueing systems and discrete time Markov chains for computing. Chapter 3 studies re-manufacturing systems and presents Markovian models for reverse manufacturing applications. In Chapter 4, Hidden Markov models are applied to classify customers. Chapter 5 discusses the Markov decision process for customer lifetime values. Customer Lifetime Values ({CLV}) is an important concept and quantity in marketing management. Chapter 6 covers higher-order Markov chain models. Multivariate Markov models are discussed in Chapter 7. It presents a class of multivariate Markov chain models with a lower order of model parameters. Chapter 8 studies higher-order hidden Markov models. It proposes a class of higher-order hidden Markov models with an efficient algorithm for solving the model parameters. This book is aimed at students, professionals, practitioners, and researchers in applied mathematics, scientific computing, and operational research, who are interested in the formulation and computation of queueing and manufacturing systems.},
	pagetotal = {228},
	publisher = {Springer},
	author = {Ching, Wai-Ki and Ng, Michael K.},
	date = {2006},
	langid = {english},
	note = {Google-Books-{ID}: dr3fm2SAvr4C},
	keywords = {Mathematics / Probability \& Statistics / General, Computers / Mathematical \& Statistical Software, Computers / Computer Science, Mathematics / Applied, Business \& Economics / Operations Research, Business \& Economics / Production \& Operations Management, Computers / Data Processing}
}

@book{robert_monte_2005,
	title = {Monte {Carlo} {Statistical} {Methods}},
	isbn = {978-0-387-21239-5},
	abstract = {Monte Carlo statistical methods, particularly those based on Markov chains, are now an essential component of the standard set of techniques used by statisticians. This new edition has been revised towards a coherent and flowing coverage of these simulation techniques, with incorporation of the most recent developments in the field. In particular, the introductory coverage of random variable generation has been totally revised, with many concepts being unified through a fundamental theorem of simulation  There are five completely new chapters that cover Monte Carlo control, reversible jump, slice sampling, sequential Monte Carlo, and perfect sampling. There is a more in-depth coverage of Gibbs sampling, which is now contained in three consecutive chapters. The development of Gibbs sampling starts with slice sampling and its connection with the fundamental theorem of simulation, and builds up to two-stage Gibbs sampling and its theoretical properties. A third chapter covers the multi-stage Gibbs sampler and its variety of applications. Lastly, chapters from the previous edition have been revised towards easier access, with the examples getting more detailed coverage.  This textbook is intended for a second year graduate course, but will also be useful to someone who either wants to apply simulation techniques for the resolution of practical problems or wishes to grasp the fundamental principles behind those methods. The authors do not assume familiarity with Monte Carlo techniques (such as random variable generation), with computer programming, or with any Markov chain theory (the necessary concepts are developed in Chapter 6). A solutions manual, which covers approximately 40\% of the problems, is available for instructors who require the book for a course.  Christian P. Robert is Professor of Statistics in the Applied Mathematics Department at Université Paris Dauphine, France. He is also Head of the Statistics Laboratory at the Center for Research in Economics and Statistics ({CREST}) of the National Institute for Statistics and Economic Studies ({INSEE}) in Paris, and Adjunct Professor at Ecole Polytechnique. He has written three other books, including The Bayesian Choice, Second Edition, Springer 2001. He also edited Discretization and {MCMC} Convergence Assessment, Springer 1998. He has served as associate editor for the Annals of Statistics and the Journal of the American Statistical Association. He is a fellow of the Institute of Mathematical Statistics, and a winner of the Young Statistician Award of the Societié de Statistique de Paris in 1995.  George Casella is Distinguished Professor and Chair, Department of Statistics, University of Florida. He has served as the Theory and Methods Editor of the Journal of the American Statistical Association and Executive Editor of Statistical Science. He has authored three other textbooks: Statistical Inference, Second Edition, 2001, with Roger L. Berger; Theory of Point Estimation, 1998, with Erich Lehmann; and Variance Components, 1992, with Shayle R. Searle and Charles E. {McCulloch}. He is a fellow of the Institute of Mathematical Statistics and the American Statistical Association, and an elected fellow of the International Statistical Institute.},
	pagetotal = {684},
	publisher = {Springer New York},
	author = {Robert, Christian and Casella, George},
	date = {2005-08-24},
	langid = {english},
	note = {Google-Books-{ID}: {HfhGAxn}5GugC},
	keywords = {Mathematics / Probability \& Statistics / General, Computers / Mathematical \& Statistical Software, Mathematics / Probability \& Statistics / Stochastic Processes, Mathematics / Discrete Mathematics}
}

@article{roberts_weak_1997,
	title = {Weak Convergence and Optimal Scaling of Random Walk Metropolis Algorithms},
	volume = {7},
	issn = {1050-5164},
	url = {http://www.jstor.org/stable/2245134},
	abstract = {This paper considers the problem of scaling the proposal distribution of a multidimensional random walk Metropolis algorithm in order to maximize the efficiency of the algorithm. The main result is a weak convergence result as the dimension of a sequence of target densities, n, converges to ∞. When the proposal variance is appropriately scaled according to n, the sequence of stochastic processes formed by the first component of each Markov chain converges to the appropriate limiting Langevin diffusion process. The limiting diffusion approximation admits a straightforward efficiency maximization problem, and the resulting asymptotically optimal policy is related to the asymptotic acceptance rate of proposed moves for the algorithm. The asymptotically optimal acceptance rate is 0.234 under quite general conditions. The main result is proved in the case where the target density has a symmetric product form. Extensions of the result are discussed.},
	pages = {110--120},
	number = {1},
	journaltitle = {The Annals of Applied Probability},
	author = {Roberts, G. O. and Gelman, A. and Gilks, W. R.},
	urldate = {2017-08-25},
	date = {1997},
	file = {JSTOR Full Text PDF:/home/samuel/Zotero/storage/F55IKLG4/Roberts et al. - 1997 - Weak Convergence and Optimal Scaling of Random Wal.pdf:application/pdf}
}

@book{kelly_reversibility_2011,
	location = {New York, {NY}, {USA}},
	title = {Reversibility and Stochastic Networks},
	isbn = {978-1-107-40115-0},
	abstract = {This classic in stochastic network modelling broke new ground when it was published in 1979, and it remains a superb introduction to reversibility and its applications. The book concerns behaviour in equilibrium of vector stochastic processes or stochastic networks. When a stochastic network is reversible its analysis is greatly simplified, and the first chapter is devoted to a discussion of the concept of reversibility. The rest of the book focuses on the various applications of reversibility and the extent to which the assumption of reversibility can be relaxed without destroying the associated tractability. Now back in print for a new generation, this book makes enjoyable reading for anyone interested in stochastic processes thanks to the author's clear and easy-to-read style. Elementary probability is the only prerequisite and exercises are interspersed throughout.},
	publisher = {Cambridge University Press},
	author = {Kelly, F. P.},
	date = {2011}
}

@article{brooks_markov_1998,
	title = {Markov Chain Monte Carlo Method and Its Application},
	volume = {47},
	issn = {0039-0526},
	url = {http://www.jstor.org/stable/2988428},
	abstract = {The Markov chain Monte Carlo ({MCMC}) method, as a computer-intensive statistical tool, has enjoyed an enormous upsurge in interest over the last few years. This paper provides a simple, comprehensive and tutorial review of some of the most common areas of research in this field. We begin by discussing how {MCMC} algorithms can be constructed from standard building-blocks to produce Markov chains with the desired stationary distribution. We also motivate and discuss more complex ideas that have been proposed in the literature, such as continuous time and dimension jumping methods. We discuss some implementational issues associated with {MCMC} methods. We take a look at the arguments for and against multiple replications, consider how long chains should be run for and how to determine suitable starting points. We also take a look at graphical models and how graphical approaches can be used to simplify {MCMC} implementation. Finally, we present a couple of examples, which we use as case-studies to highlight some of the points made earlier in the text. In particular, we use a simple changepoint model to illustrate how to tackle a typical Bayesian modelling problem via the {MCMC} method, before using mixture model problems to provide illustrations of good sampler output and of the implementation of a reversible jump {MCMC} algorithm.},
	pages = {69--100},
	number = {1},
	journaltitle = {Journal of the Royal Statistical Society. Series D (The Statistician)},
	author = {Brooks, Stephen P.},
	urldate = {2017-08-24},
	date = {1998}
}

@book{wickham_advanced_2014,
	title = {Advanced R},
	isbn = {978-1-4665-8696-3},
	abstract = {An Essential Reference for Intermediate and Advanced R Programmers Advanced R presents useful tools and techniques for attacking many types of R programming problems, helping you avoid mistakes and dead ends. With more than ten years of experience programming in R, the author illustrates the elegance, beauty, and flexibility at the heart of R.  The book develops the necessary skills to produce quality code that can be used in a variety of circumstances. You will learn:  The fundamentals of R, including standard data types and functions Functional programming as a useful framework for solving wide classes of problems The positives and negatives of metaprogramming  How to write fast, memory-efficient code   This book not only helps current R users become R programmers but also shows existing programmers what’s special about R. Intermediate R programmers can dive deeper into R and learn new strategies for solving diverse problems while programmers from other languages can learn the details of R and understand why R works the way it does.},
	pagetotal = {476},
	publisher = {Taylor \& Francis},
	author = {Wickham, Hadley},
	date = {2014-09-25},
	langid = {english},
	note = {Google-Books-{ID}: {PFHFNAEACAAJ}},
	keywords = {Mathematics / Probability \& Statistics / General, Science / Life Sciences / Biology}
}

@article{athreya_proof_1992,
	title = {A Proof of Convergence of the Markov Chain Simulation Method},
	series = {Technical Report 868},
	author = {Athreya, Krishna B. and Doss, Hani and Sethuraman, Jayaram},
	date = {1992}
}

@book{gelman_bayesian_1996,
	title = {Bayesian statistics 5: proceedings of the fifth Valencia international meeting, June 5-9, 1994},
	isbn = {978-0-19-852356-7},
	shorttitle = {Bayesian statistics 5},
	abstract = {The proceedings of the Valencia International Meeting on Bayesian Statistics (held every four years) provide an overview of this important and highly topical area in theoretical and applied statistics. This fifth Proceedings reflects a growing emphasis on computational issues concerned with making Bayesian methods routinely available to applied practitioners both statisticians and specialists in other subject-matter, whose work depends on careful quantification of uncertainties. This book contains several invited papers by leading authorities.},
	pagetotal = {840},
	publisher = {Clarendon Press},
	author = {Gelman, A. and Roberts, G. O. and Gilks, W. R.},
	date = {1996-08-01},
	langid = {english},
	keywords = {Mathematics / Probability \& Statistics / General, Bayesian statistical decision theory, Bayesian statistical decision theory - Congresses, Bayesian statistical decision theory/ Congresses, Mathematics / Probability \& Statistics / Bayesian Analysis, Medical / Epidemiology, Science / Life Sciences / Botany}
}

@article{abadi_tensorflow:_2016,
	title = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Distributed} {Systems}},
	url = {http://arxiv.org/abs/1603.04467},
	shorttitle = {{TensorFlow}},
	abstract = {{TensorFlow} is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using {TensorFlow} can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as {GPU} cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the {TensorFlow} interface and an implementation of that interface that we have built at Google. The {TensorFlow} {API} and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
	journaltitle = {{arXiv}:1603.04467 [cs]},
	author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	urldate = {2017-09-29},
	date = {2016-03-14},
	eprinttype = {arxiv},
	eprint = {1603.04467},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Learning},
	file = {arXiv\:1603.04467 PDF:/home/samuel/Zotero/storage/TM95HGVQ/Abadi et al. - 2016 - TensorFlow Large-Scale Machine Learning on Hetero.pdf:application/pdf;arXiv.org Snapshot:/home/samuel/Zotero/storage/7EGU5BRA/1603.html:text/html}
}

@article{tran_edward:_2016,
	title = {Edward: {A} {Library} for {Probabilistic} {Modeling}, {Inference}, and {Criticism}},
	url = {http://arxiv.org/abs/1610.09787},
	shorttitle = {Edward},
	abstract = {Probabilistic modeling is a powerful approach for analyzing empirical information. We describe Edward, a library for probabilistic modeling. Edward's design reflects an iterative process pioneered by George Box: build a model of a phenomenon, make inferences about the model given data, and criticize the model's fit to the data. Edward supports a broad class of probabilistic models, efficient algorithms for inference, and many techniques for model criticism. The library builds on top of {TensorFlow} to support distributed training and hardware such as {GPUs}. Edward enables the development of complex probabilistic models and their algorithms at a massive scale.},
	journaltitle = {{arXiv}:1610.09787 [cs, stat]},
	author = {Tran, Dustin and Kucukelbir, Alp and Dieng, Adji B. and Rudolph, Maja and Liang, Dawen and Blei, David M.},
	urldate = {2017-09-29},
	date = {2016-10-31},
	eprinttype = {arxiv},
	eprint = {1610.09787},
	keywords = {Statistics - Computation, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Programming Languages, Statistics - Applications},
	file = {arXiv\:1610.09787 PDF:/home/samuel/Zotero/storage/65Q2I8BE/Tran et al. - 2016 - Edward A library for probabilistic modeling, infe.pdf:application/pdf;arXiv.org Snapshot:/home/samuel/Zotero/storage/3SWI9R87/1610.html:text/html}
}

@article{neal_sampling_1996,
	title = {Sampling from multimodal distributions using tempered transitions},
	volume = {6},
	issn = {0960-3174, 1573-1375},
	url = {https://link.springer.com/article/10.1007/BF00143556},
	doi = {10.1007/BF00143556},
	abstract = {I present a new Markov chain sampling method appropriate for distributions with isolated modes. Like the recently developed method of ‘simulated tempering’, the ‘tempered transition’ method uses a series of distributions that interpolate between the distribution of interest and a distribution for which sampling is easier. The new method has the advantage that it does not require approximate values for the normalizing constants of these distributions, which are needed for simulated tempering, and can be tedious to estimate. Simulated tempering performs a random walk along the series of distributions used. In contrast, the tempered transitions of the new method move systematically from the desired distribution, to the easily-sampled distribution, and back to the desired distribution. This systematic movement avoids the inefficiency of a random walk, an advantage that is unfortunately cancelled by an increase in the number of interpolating distributions required. Because of this, the sampling efficiency of the tempered transition method in simple problems is similar to that of simulated tempering. On more complex distributions, however, simulated tempering and tempered transitions may perform differently. Which is better depends on the ways in which the interpolating distributions are ‘deceptive’.},
	pages = {353--366},
	number = {4},
	journaltitle = {Statistics and Computing},
	shortjournal = {Stat Comput},
	author = {Neal, Radford M.},
	urldate = {2017-09-28},
	date = {1996-12-01},
	langid = {english},
	file = {Snapshot:/home/samuel/Zotero/storage/43PY8XLG/BF00143556.html:text/html}
}

@article{wang_theory_1945,
	title = {On the Theory of the Brownian Motion {II}},
	volume = {17},
	url = {https://link.aps.org/doi/10.1103/RevModPhys.17.323},
	doi = {10.1103/RevModPhys.17.323},
	abstract = {{DOI}:https://doi.org/10.1103/{RevModPhys}.17.323},
	pages = {323--342},
	number = {2},
	journaltitle = {Reviews of Modern Physics},
	shortjournal = {Rev. Mod. Phys.},
	author = {Wang, Ming Chen and Uhlenbeck, G. E.},
	urldate = {2017-09-26},
	date = {1945-04-01},
	file = {APS Snapshot:/home/samuel/Zotero/storage/62U4VSM6/RevModPhys.17.html:text/html}
}

@article{robbins_stochastic_1951,
	title = {A Stochastic Approximation Method},
	volume = {22},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/euclid.aoms/1177729586},
	doi = {10.1214/aoms/1177729586},
	abstract = {Let M(x)M(x)M(x) denote the expected value at level xxx of the response to a certain experiment. M(x)M(x)M(x) is assumed to be a monotone function of xxx but is unknown to the experimenter, and it is desired to find the solution x=θx=θx = {\textbackslash}theta of the equation M(x)=αM(x)=αM(x) = {\textbackslash}alpha, where αα{\textbackslash}alpha is a given constant. We give a method for making successive experiments at levels x1,x2,⋯x1,x2,⋯x\_1,x\_2,{\textbackslash}cdots in such a way that xnxnx\_n will tend to θθ{\textbackslash}theta in probability.},
	pages = {400--407},
	number = {3},
	journaltitle = {The Annals of Mathematical Statistics},
	shortjournal = {Ann. Math. Statist.},
	author = {Robbins, Herbert and Monro, Sutton},
	urldate = {2017-09-26},
	date = {1951-09},
	mrnumber = {MR42668},
	zmnumber = {0054.05901},
	file = {Snapshot:/home/samuel/Zotero/storage/QZ543B2B/1177729586.html:text/html}
}

@book{carmo_differential_2016,
	title = {Differential Geometry of Curves and Surfaces: Revised and Updated Second Edition},
	isbn = {978-0-486-80699-0},
	shorttitle = {Differential Geometry of Curves and Surfaces},
	abstract = {One of the most widely used texts in its field, this volume introduces  the differential geometry of curves and surfaces in both local and  global aspects. The presentation departs from the traditional approach  with its more extensive use of elementary linear algebra and its  emphasis on basic geometrical facts rather than machinery or random  details. Many examples and exercises enhance the clear, well-written  exposition, along with hints and answers to some of the problems.The  treatment begins with a chapter on curves, followed by explorations of  regular surfaces, the geometry of the Gauss map, the intrinsic geometry  of surfaces, and global differential geometry. Suitable for advanced  undergraduates and graduate students of mathematics, this text\&\#39;s  prerequisites include an undergraduate course in linear algebra and some  familiarity with the calculus of several variables. For this second  edition, the author has corrected, revised, and updated the entire  volume.},
	pagetotal = {529},
	publisher = {Courier Dover Publications},
	author = {Carmo, Manfredo P. do},
	date = {2016-12-14},
	langid = {english},
	note = {Google-Books-{ID}: {uXF}6DQAAQBAJ},
	keywords = {Mathematics / Geometry / Differential}
}

@article{wand_fully_2014,
	title = {Fully Simplified Multivariate Normal Updates in Non-Conjugate Variational Message Passing},
	volume = {15},
	url = {http://jmlr.org/papers/v15/wand14a.html},
	pages = {1351--1369},
	journaltitle = {Journal of Machine Learning Research},
	author = {Wand, Matt P.},
	urldate = {2017-09-26},
	date = {2014},
	file = {Full Text PDF:/home/samuel/Zotero/storage/DBL3J3EW/Wand - 2014 - Fully Simplified Multivariate Normal Updates in No.pdf:application/pdf;Snapshot:/home/samuel/Zotero/storage/5EYILYNU/wand14a.html:text/html}
}

@article{kramer_hamiltonian_2014,
	title = {Hamiltonian Monte Carlo methods for efficient parameter estimation in steady state dynamical systems},
	volume = {15},
	issn = {1471-2105},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4262080/},
	doi = {10.1186/1471-2105-15-253},
	abstract = {Background
Parameter estimation for differential equation models of intracellular processes is a highly relevant bu challenging task. The available experimental data do not usually contain enough information to identify all parameters uniquely, resulting in ill-posed estimation problems with often highly correlated parameters. Sampling-based Bayesian statistical approaches are appropriate for tackling this problem. The samples are typically generated via Markov chain Monte Carlo, however such methods are computationally expensive and their convergence may be slow, especially if there are strong correlations between parameters. Monte Carlo methods based on Euclidean or Riemannian Hamiltonian dynamics have been shown to outperform other samplers by making proposal moves that take the local sensitivities of the system’s states into account and accepting these moves with high probability. However, the high computational cost involved with calculating the Hamiltonian trajectories prevents their widespread use for all but the smallest differential equation models. The further development of efficient sampling algorithms is therefore an important step towards improving the statistical analysis of predictive models of intracellular processes.

Results
We show how state of the art Hamiltonian Monte Carlo methods may be significantly improved for steady state dynamical models. We present a novel approach for efficiently calculating the required geometric quantities by tracking steady states across the Hamiltonian trajectories using a Newton-Raphson method and employing local sensitivity information. Using our approach, we compare both Euclidean and Riemannian versions of Hamiltonian Monte Carlo on three models for intracellular processes with real data and demonstrate at least an order of magnitude improvement in the effective sampling speed. We further demonstrate the wider applicability of our approach to other gradient based {MCMC} methods, such as those based on Langevin diffusions.

Conclusion
Our approach is strictly benefitial in all test cases. The Matlab sources implementing our {MCMC} methodology is available from https://github.com/a-kramer/ode\_rmhmc.

Electronic supplementary material
The online version of this article (doi:10.1186/1471-2105-15-253) contains supplementary material, which is available to authorized users.},
	number = {1},
	journaltitle = {{BMC} Bioinformatics},
	shortjournal = {{BMC} Bioinformatics},
	author = {Kramer, Andrei and Calderhead, Ben and Radde, Nicole},
	urldate = {2017-09-23},
	date = {2014-07-28},
	pmid = {25066046},
	pmcid = {PMC4262080},
	file = {PubMed Central Full Text PDF:/home/samuel/Zotero/storage/BBU3XEMW/Kramer et al. - 2014 - Hamiltonian Monte Carlo methods for efficient para.pdf:application/pdf}
}

@book{hairer_geometric_2002,
	title = {Geometric Numerical Integration: Structure-Preserving Algorithms for Ordinary Differential Equations},
	isbn = {978-3-540-43003-2},
	shorttitle = {Geometric Numerical Integration},
	abstract = {Numerical methods that preserve properties of Hamiltonian systems, reversible systems, differential equations on manifolds and problems with highly oscillatory solutions are the subject of this book. A complete self-contained theory of symplectic and symmetric methods, which include Runge-Kutta, composition, splitting, multistep and various specially designed integrators, is presented and their construction and practical merits are discussed. The long-time behaviour of the numerical solutions is studied using a backward error analysis (modified equations) combined with {KAM} theory. The book is illustrated by many figures, it treats applications from physics and astronomy and contains many numerical experiments and comparisons of different approaches.},
	pagetotal = {534},
	publisher = {Springer Science \& Business Media},
	author = {Hairer, Ernst and Lubich, Christian and Wanner, Gerhard},
	date = {2002},
	langid = {english},
	note = {Google-Books-{ID}: O5CfNSGTP\_EC},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Differential Equations / General, Mathematics / Functional Analysis}
}

@article{hairer_reversible_1997,
	title = {Reversible Long-Term Integration with Variable Stepsizes},
	volume = {18},
	issn = {1064-8275},
	url = {http://epubs.siam.org/doi/abs/10.1137/S1064827595285494},
	doi = {10.1137/S1064827595285494},
	abstract = {The numerical integration of reversible dynamical systems is considered. A backward analysis for variable stepsize one-step methods is developed, and it is shown that the numerical solution of a symmetric one-step method, implemented with a reversible stepsize strategy, is formally equal to the exact solution of a perturbed differential equation, which again is reversible. This explains geometrical properties of the numerical flow, such as the nearby preservation of invariants. In a second part, the efficiency of symmetric implicit Runge--Kutta methods (linear error growth when applied to integrable systems) is compared with explicit nonsymmetric integrators (quadratic error growth).},
	pages = {257--269},
	number = {1},
	journaltitle = {{SIAM} Journal on Scientific Computing},
	shortjournal = {{SIAM} J. Sci. Comput.},
	author = {Hairer, E. and Stoffer, D.},
	urldate = {2017-09-23},
	date = {1997-01-01},
	file = {Full Text PDF:/home/samuel/Zotero/storage/WBTDX6JA/Hairer and Stoffer - 1997 - Reversible Long-Term Integration with Variable Ste.pdf:application/pdf;Snapshot:/home/samuel/Zotero/storage/ZLAF5Q85/S1064827595285494.html:text/html}
}

@article{iserles_generalized_1986,
	title = {Generalized Leapfrog Methods},
	volume = {6},
	issn = {0272-4979},
	url = {https://academic-oup-com.proxy.ulib.uits.iu.edu/imajna/article/6/4/381/732207/Generalized-Leapfrog-Methods},
	doi = {10.1093/imanum/6.4.381},
	abstract = {In the present paper we generalize the classical leapfrog method for ur = ux. For every r≥0 and S≥0 with r + s≥l, we obtain an r + S-order explicit two-step method that uses r points to the left and s points to the right. The stability of such methods is analysed by using the order-star theory. It follows that the methods are stable and conservative if and only ifr≤s≤r +2 . We also present results on the group velocity of these methods.},
	pages = {381--392},
	number = {4},
	journaltitle = {{IMA} Journal of Numerical Analysis},
	shortjournal = {{IMA} J Numer Anal},
	author = {Iserles, A.},
	urldate = {2017-09-23},
	date = {1986-10-01},
	file = {Snapshot:/home/samuel/Zotero/storage/6BXPH8UZ/Generalized-Leapfrog-Methods.html:text/html}
}

@article{rao_information_1945,
	title = {Information and the Accuracy Attainable in the Estimation of Statistical Parameters},
	volume = {37},
	pages = {81--91},
	number = {3},
	author = {Rao, C. Radhakrishna},
	date = {1945}
}

@book{jain_functional_1995,
	title = {Functional Analysis},
	isbn = {978-81-224-0801-0},
	abstract = {The Book Is Intended To Serve As A Textbook For An Introductory Course In Functional Analysis For The Senior Undergraduate And Graduate Students. It Can Also Be Useful For The Senior Students Of Applied Mathematics, Statistics, Operations Research, Engineering And Theoretical Physics. The Text Starts With A Chapter On Preliminaries Discussing Basic Concepts And Results Which Would Be Taken For Granted Later In The Book. This Is Followed By Chapters On Normed And Banach Spaces, Bounded Linear Operators, Bounded Linear Functionals. The Concept And Specific Geometry Of Hilbert Spaces, Functionals And Operators On Hilbert Spaces And Introduction To Spectral Theory. An Appendix Has Been Given On Schauder Bases.The Salient Features Of The Book Are: * Presentation Of The Subject In A Natural Way * Description Of The Concepts With Justification * Clear And Precise Exposition Avoiding Pendantry * Various Examples And Counter Examples * Graded Problems Throughout Each {ChapterNotes} And Remarks Within The Text Enhances The Utility Of The Book For The Students.},
	pagetotal = {338},
	publisher = {New Age International},
	author = {Jain, P. K. and Ahmad, Khalil and Ahuja, Om P.},
	date = {1995},
	langid = {english},
	note = {Google-Books-{ID}: {yZ}68h97pnAkC}
}

@article{barndorff-nielsen_role_1986,
	title = {The Role of Differential Geometry in Statistical Theory},
	volume = {54},
	issn = {0306-7734},
	url = {http://www.jstor.org/stable/1403260},
	doi = {10.2307/1403260},
	abstract = {There has been increasing emphasis recently on the use of differential geometry in statistical theory, especially in asymptotic theory. In this paper a brief relatively nontechnical account is given of some relevant ideas in differential geometry. Some of the early work applying differential geometry in statistics is then sketched. Recent developments are outlined and finally directions of current and possible future work are indicated. /// Récemment on a appuyé sur l'usage de géometrie différentielle en théorie statistique, particulièrement en théorie asymptotique. Dans cet article on décrit, relativement court et non-technique, quelques idées de la géométrie différentielle qui ont rapport à la statistique. Une partie du travail antérieur est ensuite esquissée. Le contour des développements récents est dessiné et enfin les directions du travail courant et éventuel à l'avenir sont indiquées.},
	pages = {83--96},
	number = {1},
	journaltitle = {International Statistical Review / Revue Internationale de Statistique},
	author = {Barndorff-Nielsen, O. E. and Cox, D. R. and Reid, N.},
	urldate = {2017-09-22},
	date = {1986},
	file = {JSTOR Full Text PDF:/home/samuel/Zotero/storage/ZP75IY4D/Barndorff-Nielsen et al. - 1986 - The Role of Differential Geometry in Statistical T.pdf:application/pdf}
}

@book{riemann_hypotheses_2016,
	title = {On the Hypotheses Which Lie at the Bases of Geometry},
	url = {http://www.springer.com/gp/book/9783319260402},
	abstract = {This book presents William Clifford’s English translation of Bernhard Riemann’s classic text together with detailed mathematical, historical and...},
	publisher = {Springer},
	author = {Riemann, Bernhard},
	urldate = {2017-09-22},
	date = {2016},
	file = {Snapshot:/home/samuel/Zotero/storage/YZEJMQ7E/9783319260402.html:text/html}
}

@book{kilmister_general_2013,
	title = {General Theory of Relativity: The Commonwealth and International Library: Selected Readings in Physics},
	isbn = {978-1-4831-5465-7},
	shorttitle = {General Theory of Relativity},
	abstract = {General Theory of Relativity deals with the general theory of relativity and covers topics ranging from the principle of equivalence and the space-theory of matter to the hypotheses which lie at the bases of geometry, along with the effect of gravitation on the propagation of light. The motion of particles in general relativity theory is also discussed. This book is comprised of 14 chapters and begins with a review of the principle of equivalence, paying particular attention to the question of the existence of inertial frames in Newtonian mechanics. The beginnings and foundations of general relativity are then considered, together with modern developments in the field. Subsequent chapters explore the general notion of multiply extended magnitudes; the space-theory of matter; the effect of gravitation on light propagation; gravitational waves and the motion of particles in general relativity theory; and homogeneity and covariance. An invariant formulation of gravitational radiation theory is also presented. The last three chapters examine continued gravitational contraction, a spinor approach to general relativity, and gravitational red-shift in nuclear resonance. This monograph will be of interest to physicists and mathematicians.},
	pagetotal = {376},
	publisher = {Elsevier},
	author = {Kilmister, C. W.},
	date = {2013-10-22},
	langid = {english},
	note = {Google-Books-{ID}: 5Z83BQAAQBAJ},
	keywords = {Science / Mechanics / General, Science / Physics / General, Science / Energy}
}

@book{laugwitz_bernhard_2008,
	title = {Bernhard Riemann 1826-1866: Turning Points in the Conception of Mathematics},
	isbn = {978-0-8176-4776-6},
	shorttitle = {Bernhard Riemann 1826-1866},
	abstract = {The name of Bernard Riemann is well known to mathematicians and physicists around the world. College students encounter the Riemann integral early in their studies. Real and complex function theories are founded on Riemann’s work. Einstein’s theory of gravitation would be unthinkable without Riemannian geometry. In number theory, Riemann’s famous conjecture stands as one of the classic challenges to the best mathematical minds and continues to stimulate deep mathematical research. The name is indelibly stamped on the literature of mathematics and physics. This book, originally written in German and presented here in an English-language translation, examines Riemann’s scientific work from a single unifying perspective. Laugwitz describes Riemann’s development of a conceptual approach to mathematics at a time when conventional algorithmic thinking dictated that formulas and figures, rigid constructs, and transformations of terms were the only legitimate means of studying mathematical objects. David Hilbert gave prominence to the Riemannian principle of utilizing thought, not calculation, to achieve proofs. Hermann Weyl interpreted the Riemann principle — for mathematics and physics alike — to be a matter of "understanding the world through its behavior in the infinitely small." This remarkable work, rich in insight and scholarship, is addressed to mathematicians, physicists, and philosophers interested in mathematics. It seeks to draw those readers closer to the underlying ideas of Riemann’s work and to the development of them in their historical context. This illuminating English-language version of the original German edition will be an important contribution to the literature of the history of mathematics. "There is excellent referencing throughout... Quotes are given almost always both in English and in the original German. Many readers will feel the original German brings them a bit closer to Riemann and his contemporaries... Laugwitz’s expertise on historical matters is most impressive... Thanks are due to both author and translator for making it much easier to enter into the literature on Riemann." —{MAA} Online "...the author has succeeded admirably...stating the technical details clearly and correctly while writing an engaging and readable account of Riemann’s life and work... Any reader of this book with even a passing interest in the history or philosophy of mathematics is certain to become engaged in a mental conversation with the author... The format of the book is excellent, especially the plentiful supply of photographs of people and places... The book will serve as an interesting read and also a useful reference... It is highly recommended." —Bulletin of the {AMS}},
	pagetotal = {374},
	publisher = {Springer Science \& Business Media},
	author = {Laugwitz, Detlef},
	date = {2008-01-21},
	langid = {english},
	note = {Google-Books-{ID}: {bwCXND}2g178C},
	keywords = {Science / Physics / General, Mathematics / General, Biography \& Autobiography / General, Mathematics / History \& Philosophy, Philosophy / General}
}

@thesis{calderhead_differential_2011,
	title = {Differential geometric {MCMC} methods and applications},
	url = {http://encore.lib.gla.ac.uk/iii/encore/record/C__Rb2922509},
	abstract = {This thesis presents novel Markov chain Monte Carlo methodology that exploits the natural representation of a statistical model as a Riemannian manifold.  The methods developed provide generalisations of the Metropolis-adjusted Langevin algorithm and the Hybrid Monte Carlo algorithm for Bayesian statistical inference, and resolve many shortcomings of existing Monte Carlo algorithms when sampling from target densities that may be high dimensional and exhibit strong correlation structure.  The performance of these Riemannian manifold Markov chain Monte Carlo algorithms is rigorously assessed by performing Bayesian inference on logistic regression models, log-Gaussian Cox point process models, stochastic volatility models, and both parameter and model level inference of dynamical systems described by nonlinear differential equations.},
	institution = {University of Glasgow},
	type = {phdthesis},
	author = {Calderhead, Ben},
	urldate = {2017-09-22},
	date = {2011},
	langid = {english},
	file = {Full Text PDF:/home/samuel/Zotero/storage/CVQVPQ6R/Calderhead - 2011 - Differential geometric MCMC methods and applicatio.pdf:application/pdf;Snapshot:/home/samuel/Zotero/storage/BDJY93J2/3258.html:text/html}
}

@book{leimkuhler_simulating_2004,
	title = {Simulating Hamiltonian Dynamics},
	isbn = {978-0-521-77290-7},
	abstract = {Geometric integrators are time-stepping methods, designed such that they exactly satisfy conservation laws, symmetries or symplectic properties of a system of differential equations. In this book the authors outline the principles of geometric integration and demonstrate how they can be applied to provide efficient numerical methods for simulating conservative models. Beginning from basic principles and continuing with discussions regarding the advantageous properties of such schemes, the book introduces methods for the N-body problem, systems with holonomic constraints, and rigid bodies. More advanced topics treated include high-order and variable stepsize methods, schemes for treating problems involving multiple time-scales, and applications to molecular dynamics and partial differential equations. The emphasis is on providing a unified theoretical framework as well as a practical guide for users. The inclusion of examples, background material and exercises enhance the usefulness of the book for self-instruction or as a text for a graduate course on the subject.},
	pagetotal = {398},
	publisher = {Cambridge University Press},
	author = {Leimkuhler, Benedict and Reich, Sebastian},
	date = {2004},
	langid = {english},
	keywords = {Science / Mechanics / General, Mathematics / Geometry / General, Mathematics / Mathematical Analysis, Mathematics / Numerical Analysis, Mathematics / Study \& Teaching, Science / Chemistry / Physical \& Theoretical, Technology \& Engineering / Materials Science}
}

@article{beskos_acceptance_2010,
	title = {The Acceptance Probability of the Hybrid Monte Carlo Method in High‐Dimensional Problems},
	volume = {1281},
	issn = {0094-243X},
	url = {http://aip.scitation.org/doi/abs/10.1063/1.3498436},
	doi = {10.1063/1.3498436},
	pages = {23--26},
	number = {1},
	journaltitle = {{AIP} Conference Proceedings},
	shortjournal = {{AIP} Conference Proceedings},
	author = {Beskos, A. and Pillai, N. S. and Roberts, G. O. and Sanz‐Serna, J. M. and Stuart, A. M.},
	urldate = {2017-09-12},
	date = {2010-09-17},
	file = {Snapshot:/home/samuel/Zotero/storage/SEU5YBXQ/1.html:text/html}
}

@article{mackenze_improved_1989,
  title={An {Improved} {Hybrid} {Monte} {Carlo} {Method}},
  author={Mackenze, Paul B},
  journal={Physics Letters B},
  volume={226},
  number={3-4},
  pages={369--371},
  year={1989},
  publisher={Elsevier}
}

@article{fahrmeir_consistency_1985,
  title={Consistency and {Asymptotic} {Normality} of the {Maximum} {Likelihood} {Estimator} in {Generalized} {Linear} {Models}},
  author={Fahrmeir, Ludwig and Kaufmann, Heinz and others},
  journal={The Annals of Statistics},
  volume={13},
  number={1},
  pages={342--368},
  year={1985},
  publisher={Institute of Mathematical Statistics}
}

@article{cances_theoretical_2007,
	title = {Theoretical and numerical comparison of some sampling methods for molecular dynamics},
	volume = {41},
	issn = {0764-583X, 1290-3841},
	url = {https://www.cambridge.org/core/journals/esaim-mathematical-modelling-and-numerical-analysis/article/theoretical-and-numerical-comparison-of-some-sampling-methods-for-molecular-dynamics/42FE5128990BB701F70E6912427F9AC6},
	doi = {10.1051/m2an:2007014},
	abstract = {The purpose of the present article is to compare different phase-space
sampling methods,
such as purely stochastic methods (Rejection method, Metropolized
independence sampler, Importance Sampling),
stochastically perturbed Molecular Dynamics methods
(Hybrid Monte Carlo, Langevin Dynamics, Biased Random Walk), and purely
deterministic methods (Nosé-Hoover chains, Nosé-Poincaré and Recursive
Multiple Thermostats ({RMT}) methods). After recalling
some theoretical convergence properties for
the various methods, we provide some new convergence results
for the Hybrid Monte Carlo scheme, requiring weaker (and easier to
check) conditions than previously known conditions. We then turn to the numerical
efficiency of the sampling schemes for a benchmark model of linear
alkane molecules.
In particular, the numerical
distributions that are generated are compared in a systematic way, on the basis
of some quantitative
convergence indicators.

,

The purpose of the present article is to compare different phase-space
sampling methods,
such as purely stochastic methods (Rejection method, Metropolized
independence sampler, Importance Sampling),
stochastically perturbed Molecular Dynamics methods
(Hybrid Monte Carlo, Langevin Dynamics, Biased Random Walk), and purely
deterministic methods (Nosé-Hoover chains, Nosé-Poincaré and Recursive
Multiple Thermostats ({RMT}) methods). After recalling
some theoretical convergence properties for
the various methods, we provide some new convergence results
for the Hybrid Monte Carlo scheme, requiring weaker (and easier to
check) conditions than previously known conditions. We then turn to the numerical
efficiency of the sampling schemes for a benchmark model of linear
alkane molecules.
In particular, the numerical
distributions that are generated are compared in a systematic way, on the basis
of some quantitative
convergence indicators.},
	pages = {351--389},
	number = {2},
	journaltitle = {{ESAIM}: Mathematical Modelling and Numerical Analysis},
	author = {Cancès, Eric and Legoll, Frédéric and Stoltz, Gabriel},
	urldate = {2017-09-12},
	date = {2007-03},
	keywords = {/div\&gt, \&gt, \&lt, canonical ensemble, div class=\&quot, kwd\&quot, Molecular Dynamics., Sampling methods},
	file = {Full Text PDF:/home/samuel/Zotero/storage/WJFW48JD/Cancès et al. - 2007 - Theoretical and numerical comparison of some sampl.pdf:application/pdf}
}

@article{durmus_convergence_2017,
  title={On the {Convergence} of {Hamiltonian} {Monte} {Carlo}},
  author={Durmus, Alain and Moulines, Eric and Saksman, Eero},
  journal={arXiv preprint arXiv:1705.00166},
  pages={1--45},
  year={2017}
}

@article{hairer_geometric_2003,
	title = {Geometric {Numerical} {Integration} {Illustrated} by the {Störmer}–{Verlet} {Method}},
	volume = {12},
	issn = {1474-0508, 0962-4929},
	url = {https://www.cambridge.org/core/journals/acta-numerica/article/geometric-numerical-integration-illustrated-by-the-stormerverlet-method/E55395D5DD7A4E0526D10EA74DA6C46B},
	doi = {10.1017/S0962492902000144},
	abstract = {The subject of geometric numerical integration deals with numerical integrators that preserve geometric properties of the flow of a differential equation, and it explains how structure preservation leads to improved long-time behaviour. This article illustrates concepts and results of geometric numerical integration on the important example of the Störmer–Verlet method. It thus presents a cross-section of the recent monograph by the authors, enriched by some additional material.After an introduction to the Newton–Störmer–Verlet–leapfrog method and its various interpretations, there follows a discussion of geometric properties: reversibility, symplecticity, volume preservation, and conservation of first integrals. The extension to Hamiltonian systems on manifolds is also described. The theoretical foundation relies on a backward error analysis, which translates the geometric properties of the method into the structure of a modified differential equation, whose flow is nearly identical to the numerical method. Combined with results from perturbation theory, this explains the excellent long-time behaviour of the method: long-time energy conservation, linear error growth and preservation of invariant tori in near-integrable systems, a discrete virial theorem, and preservation of adiabatic invariants.},
	pages = {399--450},
	journaltitle = {Acta Numerica},
	author = {Hairer, Ernst and Lubich, Christian and Wanner, Gerhard},
	urldate = {2017-09-08},
	date = {2003-05},
	file = {Snapshot:/home/samuel/Zotero/storage/SDD2KYFJ/E55395D5DD7A4E0526D10EA74DA6C46B.html:text/html}
}

@book{serway_college_2012,
	title = {College Physics},
	isbn = {978-0-8400-6848-4},
	abstract = {While physics can seem challenging, its true quality is the sheer simplicity of fundamental physical theories--theories and concepts that can enrich your view of the world around you. {COLLEGE} {PHYSICS}, Ninth Edition, provides a clear strategy for connecting those theories to a consistent problem-solving approach, carefully reinforcing this methodology throughout the text and connecting it to real-world examples. For students planning to take the {MCAT} exam, the text includes exclusive test prep and review tools to help you prepare.Important Notice: Media content referenced within the product description or the product text may not be available in the ebook version.},
	pagetotal = {630},
	publisher = {Cengage Learning},
	author = {Serway, Raymond A. and Vuille, Chris},
	date = {2012-07-24},
	langid = {english},
	keywords = {Science / Physics / General}
}

@article{channell_symplectic_1990,
	title = {Symplectic {Integration} of {Hamiltonian} {Systems}},
	volume = {3},
	issn = {0951-7715},
	url = {http://stacks.iop.org/0951-7715/3/i=2/a=001},
	doi = {10.1088/0951-7715/3/2/001},
	abstract = {The authors survey past work and present new algorithms to numerically integrate the trajectories of Hamiltonian dynamical systems. These algorithms exactly preserve the symplectic 2-form, i.e. they preserve all the Poincare invariants. The algorithms have been tested on a variety of examples and results are presented for the Fermi-Pasta-Ulam nonlinear string, the Henon-Heiles system, a four-vortex problem, and the geodesic flow on a manifold of constant negative curvature. In all cases the algorithms possess long-time stability and preserve global geometrical structures in phase space.},
	pages = {231},
	number = {2},
	journal = {Nonlinearity},
	year = {1990},
	journaltitle = {Nonlinearity},
	shortjournal = {Nonlinearity},
	author = {Channell, P. J. and Scovel, C.},
	urldate = {2017-09-08},
	date = {1990},
	langid = {english}
}

@book{abraham_foundations_1978,
	title = {Foundations of Mechanics},
	isbn = {978-0-8218-4438-0},
	abstract = {This book is the American Mathematical Society printing of this title, which was first published in 1907 by W. A. Benjamin and whose second edition was published by Benjamin Cummings in 1978. The book was also distributed by Perseus Press for the last decade. It is the updated 1985 (fifth) printing that is reproduced here. It includes most of the basic results in manifold theory, as well as some key facts from point set topology and Lie group theory. Introductory chapters offer background in differential theory and calculus on manifolds. Later chapters are organized in sections on analytical dynamics, qualitative dynamics, and celestial mechanics. Chapter exercises are included. The book can be used as a textbook and as a basic reference for the foundations of differentiable and Hamiltonian dynamics. Readership includes mathematicians, physicists, and engineers interested in geometrical methods in mechanics, assuming a background in calculus, linear algebra, some classical analysis, and point set topology. Author information is not given.},
	pagetotal = {862},
	publisher = {American Mathematical Soc.},
	author = {Abraham, Ralph and Marsden, Jerrold E.},
	date = {1978},
	langid = {english},
	keywords = {Science / Mechanics / General, Mathematics / Mathematical Analysis}
}

@book{gibbs_elementary_1902,
	title = {Elementary Principles in Statistical Mechanics: Developed with Especial Reference to the Rational Foundations of Thermodynamics},
	shorttitle = {Elementary Principles in Statistical Mechanics},
	pagetotal = {238},
	publisher = {C. Scribner's sons},
	author = {Gibbs, Josiah Willard},
	date = {1902},
	langid = {english}
}

@article{nose_molecular_1984,
	title = {A molecular dynamics method for simulations in the canonical ensemble},
	volume = {52},
	issn = {0026-8976},
	url = {http://dx.doi.org/10.1080/00268978400101201},
	doi = {10.1080/00268978400101201},
	abstract = {A molecular dynamics simulation method which can generate configurations belonging to the canonical (T, V, N) ensemble or the constant temperature constant pressure (T, P, N) ensemble, is proposed. The physical system of interest consists of N particles (f degrees of freedom), to which an external, macroscopic variable and its conjugate momentum are added. This device allows the total energy of the physical system to fluctuate. The equilibrium distribution of the energy coincides with the canonical distribution both in momentum and in coordinate space. The method is tested for an atomic fluid (Ar) and works well.},
	pages = {255--268},
	number = {2},
	journaltitle = {Molecular Physics},
	author = {Nosé, Shūichi},
	urldate = {2017-09-08},
	date = {1984-06-10},
	file = {Snapshot:/home/samuel/Zotero/storage/IC3YFU8V/00268978400101201.html:text/html}
}

@book{nakahara_geometry_2003,
	title = {Geometry, Topology and Physics, Second Edition},
	isbn = {978-0-7503-0606-5},
	abstract = {Differential geometry and topology have become essential tools for many theoretical physicists. In particular, they are indispensable in theoretical studies of condensed matter physics, gravity, and particle physics. Geometry, Topology and Physics, Second Edition introduces the ideas and techniques of differential geometry and topology at a level suitable for postgraduate students and researchers in these fields.The second edition of this popular and established text incorporates a number of changes designed to meet the needs of the reader and reflect the development of the subject. The book features a considerably expanded first chapter, reviewing aspects of path integral quantization and gauge theories. Chapter 2 introduces the mathematical concepts of maps, vector spaces, and topology. The following chapters focus on more elaborate concepts in geometry and topology and discuss the application of these concepts to liquid crystals, superfluid helium, general relativity, and bosonic string theory. Later chapters unify geometry and topology, exploring fiber bundles, characteristic classes, and index theorems. New to this second edition is the proof of the index theorem in terms of supersymmetric quantum mechanics. The final two chapters are devoted to the most fascinating applications of geometry and topology in contemporary physics, namely the study of anomalies in gauge field theories and the analysis of Polakov's bosonic string theory from the geometrical point of view.Geometry, Topology and Physics, Second Edition is an ideal introduction to differential geometry and topology for postgraduate students and researchers in theoretical and mathematical physics.},
	pagetotal = {598},
	publisher = {{CRC} Press},
	author = {Nakahara, Mikio},
	date = {2003-06-04},
	langid = {english},
	keywords = {Science / Physics / General, Science / Physics / Mathematical \& Computational, Mathematics / Geometry / General}
}

@book{arnold_mathematical_2013,
	title = {Mathematical Methods of Classical Mechanics},
	isbn = {978-1-4757-2063-1},
	abstract = {In this text, the author constructs the mathematical apparatus of classical mechanics from the beginning, examining all the basic problems in dynamics, including the theory of oscillations, the theory of rigid body motion, and the Hamiltonian formalism. This modern approch, based on the theory of the geometry of manifolds, distinguishes iteself from the traditional approach of standard textbooks. Geometrical considerations are emphasized throughout and include phase spaces and flows, vector fields, and Lie groups. The work includes a detailed discussion of qualitative methods of the theory of dynamical systems and of asymptotic methods like perturbation techniques, averaging, and adiabatic invariance.},
	pagetotal = {530},
	publisher = {Springer Science \& Business Media},
	author = {Arnol'd, V. I.},
	date = {2013-04-09},
	langid = {english},
	note = {Google-Books-{ID}: 5OQlBQAAQBAJ},
	keywords = {Science / Physics / Mathematical \& Computational, Mathematics / Mathematical Analysis, Mathematics / Calculus}
}

@article{neyts_combining_2013,
	title = {Combining molecular dynamics with Monte Carlo simulations: implementations and applications},
	volume = {132},
	issn = {1432-881X, 1432-2234},
	url = {https://link.springer.com/article/10.1007/s00214-012-1320-x},
	doi = {10.1007/s00214-012-1320-x},
	shorttitle = {Combining molecular dynamics with Monte Carlo simulations},
	abstract = {In this contribution, we present an overview of the various techniques for combining atomistic molecular dynamics with Monte Carlo simulations, mainly in the context of condensed matter systems, as well as a brief summary of the main accelerated dynamics techniques. Special attention is given to the force bias Monte Carlo technique and its combination with molecular dynamics, in view of promising recent developments, including a definable timescale. Various examples of the application of combined molecular dynamics / Monte Carlo simulations are given, in order to demonstrate the enhanced simulation efficiency with respect to either pure molecular dynamics or Monte Carlo.},
	pages = {1320},
	number = {2},
	journaltitle = {Theoretical Chemistry Accounts},
	shortjournal = {Theor Chem Acc},
	author = {Neyts, Erik C. and Bogaerts, Annemie},
	urldate = {2017-09-07},
	date = {2013-02-01},
	langid = {english},
	file = {Snapshot:/home/samuel/Zotero/storage/PF862K6J/s00214-012-1320-x.html:text/html}
}

@article{sexton_hamiltonian_1992,
	title = {Hamiltonian evolution for the hybrid Monte Carlo algorithm},
	volume = {380},
	issn = {0550-3213},
	url = {http://www.sciencedirect.com/science/article/pii/055032139290263B},
	doi = {10.1016/0550-3213(92)90263-B},
	abstract = {We discuss a class of reversible, discrete approximations to Hamilton's equations for use in the hybrid Monte Carlo algorithm and derive an asymptotic formula for the step-size-dependent errors arising from this family of approximations. For lattice {QCD} with Wilson fermions, we construct several different updates in which the effect of fermion vacuum polarization is given a longer time step than the gauge field's self-interaction. On a 44 lattice, one of these algorithms with an optimal choice of step size is 30\% to 40\% faster than the standard leapfrog update with an optimal step size.},
	pages = {665--677},
	number = {3},
	journaltitle = {Nuclear Physics B},
	shortjournal = {Nuclear Physics B},
	author = {Sexton, J. C. and Weingarten, D. H.},
	urldate = {2017-09-07},
	date = {1992-08-10},
	file = {ScienceDirect Snapshot:/home/samuel/Zotero/storage/EU9XX8Q5/055032139290263B.html:text/html}
}

@article{efron_geometry_1978,
	title = {The Geometry of Exponential Families},
	volume = {6},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1176344130},
	doi = {10.1214/aos/1176344130},
	abstract = {There are two important spaces connected with every multivariate exponential family, the natural parameter space and the expectation parameter space. We describe some geometric results relating the two. (In the simplest case, that of a normal translation family, the two spaces coincide and the geometry is the familiar Euclidean one.) Maximum likelihood estimation, within one-parameter curved subfamilies of the multivariate family, has two simple and useful geometric interpretations. The geometry also relates to the Fisherian question: to what extent can the Fisher information be replaced by −∂2/∂θ2[logfθ(x)]∣θ=θ{\textasciicircum}−∂2/∂θ2[logafθ(x)]∣θ=θ{\textasciicircum}-{\textbackslash}partial{\textasciicircum}2/{\textbackslash}partial{\textbackslash}theta{\textasciicircum}2{\textbackslash}lbrack{\textbackslash}log f\_{\textbackslash}theta(x){\textbackslash}rbrack{\textbackslash}mid\_\{{\textbackslash}theta={\textbackslash}hat\{{\textbackslash}theta\}\} in the variance bound for θ{\textasciicircum}θ{\textasciicircum}{\textbackslash}hat\{{\textbackslash}theta\}, the maximum likelihood estimator?},
	pages = {362--376},
	number = {2},
	journaltitle = {The Annals of Statistics},
	shortjournal = {Ann. Statist.},
	author = {Efron, Bradley},
	urldate = {2017-09-07},
	date = {1978-03},
	mrnumber = {MR471152},
	zmnumber = {0436.62027},
	keywords = {Curvature, duality, Kullback-Leibler distance, maximum likelihood estimation},
	file = {Snapshot:/home/samuel/Zotero/storage/C6FXMPJK/1176344130.html:text/html}
}

@article{turner_method_2013,
	title = {A Method for Efficiently Sampling From Distributions With Correlated Dimensions},
	volume = {18},
	issn = {1082-989X},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4140408/},
	doi = {10.1037/a0032222},
	abstract = {Bayesian estimation has played a pivotal role in the understanding of individual differences. However, for many models in psychology, Bayesian estimation of model parameters can be difficult. One reason for this difficulty is that conventional sampling algorithms, such as Markov chain Monte Carlo ({MCMC}), can be inefficient and impractical when little is known about the target distribution—particularly the target distribution’s covariance structure. In this article, we highlight some reasons for this inefficiency and advocate the use of a population {MCMC} algorithm, called differential evolution Markov chain Monte Carlo ({DE}-{MCMC}), as a means of efficient proposal generation. We demonstrate in a simulation study that the performance of the {DE}-{MCMC} algorithm is unaffected by the correlation of the target distribution, whereas conventional {MCMC} performs substantially worse as the correlation increases. We then show that the {DE}-{MCMC} algorithm can be used to efficiently fit a hierarchical version of the linear ballistic accumulator model to response time data, which has proven to be a difficult task when conventional {MCMC} is used.},
	pages = {368--384},
	number = {3},
	journaltitle = {Psychological methods},
	shortjournal = {Psychol Methods},
	author = {Turner, Brandon M. and Sederberg, Per B. and Brown, Scott D. and Steyvers, Mark},
	urldate = {2017-09-07},
	date = {2013-09},
	pmid = {23646991},
	pmcid = {PMC4140408},
	file = {PubMed Central Full Text PDF:/home/samuel/Zotero/storage/HJW8AMHM/Turner et al. - 2013 - A Method for Efficiently Sampling From Distributio.pdf:application/pdf}
}

@article{duane_hybrid_1987,
title = "Hybrid {Monte} {Carlo}",
journal = "Physics Letters B",
volume = "195",
number = "2",
pages = "216 - 222",
year = "1987",
issn = "0370-2693",
doi = "https://doi.org/10.1016/0370-2693(87)91197-X",
url = "http://www.sciencedirect.com/science/article/pii/037026938791197X",
author = "Simon Duane and A.D. Kennedy and Brian J. Pendleton and Duncan Roweth",
abstract = "We present a new method for the numerical simulation of lattice field theory. A hybrid (molecular dynamics/Langevin) algorithm is used to guide a Monte Carlo simulation. There are no discretization errors even for large step sizes. The method is especially efficient for systems such as quantum chromodynamics which contain fermionic degrees of freedom. Detailed results are presented for four-dimensional compact quantum electrodynamics including the dynamical effects of electrons."
}

@article{frigessi_convergence_1993,
	title = {Convergence Rates of the Gibbs Sampler, the Metropolis Algorithm and Other Single-Site Updating Dynamics},
	volume = {55},
	issn = {0035-9246},
	url = {http://www.jstor.org/stable/2346076},
	abstract = {Sampling from a Markov random field {II} can be performed efficiently via Monte Carlo methods by simulating a Markov chain that converges weakly to {II}. We consider a class of local updating dynamics that are reversible with respect to {II}. It includes the Metropolis algorithm ({MA}) and the Gibbs sampler ({GS}). We investigate the speed of weak convergence of these Markov chains in terms of their second-largest eigenvalues in absolute value. We study the general algebraic structure and then the stochastic Ising model in detail. We conclude the following: the {GS} is faster than locally updating twice by the {MA}; in the Ising case, the {MA} is the best at low temperature but the worst at high temperature; there are dynamics faster than the {GS} at high temperature. The results clear up some intuitive misconceptions.},
	pages = {205--219},
	number = {1},
	journaltitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Frigessi, Arnoldo and Di Stefano, Patrizia and Hwang, Chii-Ruey and Sheu, Shuenn-Jyi},
	urldate = {2017-09-06},
	date = {1993}
}

@article{sahlin_estimating_2011,
	title = {Estimating convergence of Markov chain Monte Carlo simulations},
	url = {https://pdfs.semanticscholar.org/37bc/24b20bbcc4e058602369444535c440ee7830.pdf},
	journaltitle = {Stockholm University, Master Thesis},
	author = {Sahlin, Kristoffer},
	urldate = {2017-09-05},
	date = {2011},
	file = {[PDF] from semanticscholar.org:/home/samuel/Zotero/storage/M65EAD8I/Sahlin - 2011 - Estimating convergence of Markov chain Monte Carlo.pdf:application/pdf}
}

@article{smith_boa:_2007,
	title = {boa: An R Package for {MCMC} Output Convergence Assessment and Posterior Inference},
	volume = {21},
	url = {https://www.jstatsoft.org/article/view/v021i11},
	doi = {10.18637/jss.v021.i11},
	shorttitle = {boa},
	number = {11},
	journaltitle = {Journal of Statistical Software},
	author = {Smith, Brian},
	urldate = {2017-09-05},
	date = {2007},
	file = {Snapshot:/home/samuel/Zotero/storage/BBKK8C53/v021i11.html:text/html}
}

@article{cowles_possible_1999,
	title = {Possible biases induced by mcmc convergence diagnostics},
	volume = {64},
	issn = {0094-9655},
	url = {http://dx.doi.org/10.1080/00949659908811968},
	doi = {10.1080/00949659908811968},
	abstract = {Convergence diagnostics are widely used to determine how many initial “burn-in” iterations should be discarded from the output of a Markov chain Monte Carlo ({MCMC}) sampler in the hope that the remaining samples are representative of the target distribution of interest. This paper demonstrates that some ways of applying convergence diagnostics may actually introduce bias into estimation based on the sampler output. To avoid this possibility, we recommend choosing the number of burn-in iterations r by applying convergence diagnostics to one or more pilot chains, and then basing estimation and inference on a separate long chain from which the first r iterations have been discarded.},
	pages = {87--104},
	number = {1},
	journaltitle = {Journal of Statistical Computation and Simulation},
	author = {Cowles, Mary Kathryn and Roberts, Gareth O. and Rosenthal, Jeffrey S.},
	urldate = {2017-09-05},
	date = {1999-08-01},
	keywords = {Markov chain Monte Carlo, batch means, bias, convergence diagnostic, estimation},
	file = {Snapshot:/home/samuel/Zotero/storage/QNG5W53M/00949659908811968.html:text/html}
}

@article{nylander_awty_2008,
	title = {{AWTY} (are we there yet?): a system for graphical exploration of {MCMC} convergence in Bayesian phylogenetics},
	volume = {24},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/24/4/581/206036/AWTY-are-we-there-yet-a-system-for-graphical},
	doi = {10.1093/bioinformatics/btm388},
	shorttitle = {{AWTY} (are we there yet?},
	abstract = {Summary: A key element to a successful Markov chain Monte Carlo ({MCMC}) inference is the programming and run performance of the Markov chain. However, the explicit use of quality assessments of the {MCMC} simulations—convergence diagnostics—in phylogenetics is still uncommon. Here, we present a simple tool that uses the output from {MCMC} simulations and visualizes a number of properties of primary interest in a Bayesian phylogenetic analysis, such as convergence rates of posterior split probabilities and branch lengths. Graphical exploration of the output from phylogenetic {MCMC} simulations gives intuitive and often crucial information on the success and reliability of the analysis. The tool presented here complements convergence diagnostics already available in other software packages primarily designed for other applications of {MCMC}. Importantly, the common practice of using trace-plots of a single parameter or summary statistic, such as the likelihood score of sampled trees, can be misleading for assessing the success of a phylogenetic {MCMC} simulation.Availability: The program is available as source under the {GNU} General Public License and as a web application at http://ceb.scs.fsu.edu/{awtyContact}:jwilgenb@scs.fsu.edu},
	pages = {581--583},
	number = {4},
	journaltitle = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Nylander, Johan A. A. and Wilgenbusch, James C. and Warren, Dan L. and Swofford, David L.},
	urldate = {2017-09-05},
	date = {2008-02-15},
	file = {Full Text PDF:/home/samuel/Zotero/storage/VSFE8VQV/Nylander et al. - 2008 - AWTY (are we there yet) a system for graphical e.pdf:application/pdf;Snapshot:/home/samuel/Zotero/storage/VYKCZ78P/AWTY-are-we-there-yet-a-system-for-graphical.html:text/html}
}

@article{gelfand_sampling-based_1990,
	title = {Sampling-Based Approaches to Calculating Marginal Densities},
	volume = {85},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/2289776},
	abstract = {Stochastic substitution, the Gibbs sampler, and the sampling-importance-resampling algorithm can be viewed as three alternative sampling- (or Monte Carlo-) based approaches to the calculation of numerical estimates of marginal probability distributions. The three approaches will be reviewed, compared, and contrasted in relation to various joint probability structures frequently encountered in applications. In particular, the relevance of the approaches to calculating Bayesian posterior densities for a variety of structured models will be discussed and illustrated.},
	pages = {398--409},
	number = {410},
	journaltitle = {Journal of the American Statistical Association},
	author = {Gelfand, Alan E. and Smith, Adrian F. M.},
	urldate = {2017-09-05},
	date = {1990}
}

@article{plummer_coda:_2006,
	title = {{CODA}:  Convergence Diagnosis and Output Analysis for {MCMC}},
	volume = {6},
	url = {https://journal.r-project.org/archive/},
	pages = {7--11},
	number = {1},
	journaltitle = {R News},
	author = {Plummer, Martyn and Best, Nicky and Cowles, Kate and Vines, Karen},
	urldate = {2017-09-05},
	date = {2006},
	file = {coda citation info:/home/samuel/Zotero/storage/V42QQ3UG/citation.html:text/html}
}

@report{plummer_coda:_1995,
	title = {{CODA}:  Convergence Diagnosis and Output Analysis Software for Gibbs Sampling Output, Version 0.30},
	shorttitle = {{CODA}},
	abstract = {[1st paragraph] At first sight, Bayesian inference with Markov Chain Monte Carlo ({MCMC}) appears to be straightforward. The user defines a full probability model, perhaps using one of the programs discussed in this issue; an underlying sampling engine takes the model definition and returns a sequence of dependent samples from the posterior distribution of the model parameters, given the supplied data. The user can derive any summary of the posterior distribution from this sample. For example, to calculate a 95\% credible interval for a parameter α, it suffices to take 1000 {MCMC} iterations of α and sort them so that α$_{\textrm{1}}${\textless}α$_{\textrm{2}}${\textless}...{\textless}α$_{\textrm{1000}}$. The credible interval estimate is then (α$_{\textrm{25}}$, α$_{\textrm{975}}$). However, there is a price to be paid for this simplicity. Unlike most numerical methods used in statistical inference, {MCMC} does not give a clear indication of whether it has converged. The underlying Markov chain theory only guarantees that the distribution of the output will converge to the posterior in the limit as the number of iterations increases to infinity. The user is generally ignorant about how quickly convergence occurs, and therefore has to fall back on post hoc testing of the sampled output. By convention, the sample is divided into two parts: a “burn in” period during which all samples are discarded, and the remainder of the run in which the chain is considered to have converged sufficiently close to the limiting distribution to be used. Two questions then arise: 1. How long should the burn in period be? 2. How many samples are required to accurately estimate posterior quantities of interest? The \textbf{coda} package for R contains a set of functions designed to help the user answer these questions. Some of these convergence diagnostics are simple graphical ways of summarizing the data. Others are formal statistical tests.},
	institution = {University of Cambridge, {MRC} Biostatistics Unit},
	type = {Technical Report},
	author = {Plummer, Martyn and Best, Nicky and Cowles, Kate and Vines, Karen},
	date = {1995},
	file = {Snapshot:/home/samuel/Zotero/storage/UCQ7KB39/22547.html:text/html}
}

@article{rosenthal_rates_1993,
	title = {Rates of Convergence for Data Augmentation on Finite Sample Spaces},
	volume = {3},
	issn = {1050-5164, 2168-8737},
	url = {https://projecteuclid.org/euclid.aoap/1177005366},
	doi = {10.1214/aoap/1177005366},
	abstract = {We consider a version of the data augmentation algorithm of Tanner and Wong, which is a special case of the Gibbs sampler. Using ideas from Harris recurrence, we derive quantitative, a priori bounds on the number of iterations required to achieve convergence. Our analysis involves relating the Markov chain to an associated dynamical system.},
	pages = {819--839},
	number = {3},
	journaltitle = {The Annals of Applied Probability},
	shortjournal = {Ann. Appl. Probab.},
	author = {Rosenthal, Jeffrey S.},
	urldate = {2017-09-05},
	date = {1993-08},
	mrnumber = {MR1233628},
	zmnumber = {0780.60067},
	keywords = {Gibbs sampler, convergence rate, Data augmentation, Harris recurrence},
	file = {Snapshot:/home/samuel/Zotero/storage/6G28L85P/1177005366.html:text/html}
}

@article{polson_convergence_1996,
	title = {Convergence of Markov chain Monte Carlo algorithms (with discussion)},
	volume = {5},
	url = {http://ci.nii.ac.jp/naid/10010345504/},
	pages = {297--321},
	journaltitle = {Bayesian Statistics},
	author = {{POLSON}, N. G.},
	urldate = {2017-09-05},
	date = {1996},
	file = {Convergence of Markov chain Monte Carlo algorithms (with discussion) Snapshot:/home/samuel/Zotero/storage/8Y6KRMQC/10010345504.html:text/html}
}

@article{heidelberger_spectral_1981,
	title = {A Spectral Method for Confidence Interval Generation and Run Length Control in Simulations},
	volume = {24},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/358598.358630},
	doi = {10.1145/358598.358630},
	pages = {233--245},
	number = {4},
	journaltitle = {Commun. {ACM}},
	author = {Heidelberger, Philip and Welch, Peter D.},
	urldate = {2017-09-05},
	date = {1981-04},
	keywords = {batch means, confidence interval, simulation, spectral analysis, variance estimation}
}

@report{geweke_evaluating_1991,
	title = {Evaluating the accuracy of sampling-based approaches to the calculation of posterior moments},
	url = {https://ideas.repec.org/p/fip/fedmsr/148.html},
	abstract = {Data augmentation and Gibbs sampling are two closely related, sampling-based approaches to the calculation of posterior moments. The fact that each produces a sample whose constituents are neither independent nor identically distributed complicates the assessment of convergence and numerical accuracy of the approximations to the expected value of functions of interest under the posterior. In this paper methods for spectral analysis are used to evaluate numerical accuracy formally and construct diagnostics for convergence. These methods are illustrated in the normal linear model with informative priors, and in the Tobit-censored regression model.},
	number = {148},
	institution = {Federal Reserve Bank of Minneapolis},
	author = {Geweke, John F.},
	urldate = {2017-09-05},
	date = {1991},
	keywords = {Sampling (Statistics)},
	file = {Fullext PDF:/home/samuel/Zotero/storage/HGRMZB4B/Geweke - 1991 - Evaluating the accuracy of sampling-based approach.pdf:application/pdf;Snapshot:/home/samuel/Zotero/storage/GAGETTQD/148.html:text/html}
}

@report{neal_probabilistic_1993,
	title = {{Probabilistic} {Inference} {Using} {Markov} {Chain} {Monte} {Carlo} {Methods}},
	abstract = {Probabilistic inference is an attractive approach to uncertain reasoning and empirical learning in artificial intelligence. Computational difficulties arise, however, because probabilistic models with the necessary realism and flexibility lead to complex distributions over high-dimensional spaces. Related problems in other fields have been tackled using Monte Carlo methods based on sampling using Markov chains, providing a rich array of techniques that can be applied to problems in artificial intelligence. The "Metropolis algorithm" has been used to solve difficult problems in statistical physics for over forty years, and, in the last few years, the related method of "Gibbs sampling" has been applied to problems of statistical inference. Concurrently, an alternative method for solving problems in statistical physics by means of dynamical simulation has been developed as well, and has recently been unified with the Metropolis algorithm to produce the "hybrid Monte Carlo" method. In computer science, Markov chain sampling is the basis of the heuristic optimization technique of "simulated annealing", and has recently been used in randomized algorithms for approximate counting of large sets. In this review, I outline the role of probabilistic inference in artificial intelligence, and present the theory of Markov chains, and describe various Markov chain Monte Carlo algorithms, along with a number of supporting techniques. I try to present a comprehensive picture of the range of methods that have been developed, including techniques from the varied literature that have not yet seen wide application in artificial intelligence, but which appear relevant. As illustrative examples, I use the problems of probabilistic inference in expert systems, discovery of latent classes from data, and Bayesian learning for neural networks.},
	number = {Technical Report {CRG}-{TR}-93-1},
	institution = {University of Toronto, Department of Computer Science},
	author = {Neal, Radford M.},
	date = {1993},
	file = {Citeseer - Snapshot:/home/samuel/Zotero/storage/TNQNZPWY/summary.html:text/html}
}

@article{brooks_general_1998,
	title = {General Methods for Monitoring Convergence of Iterative Simulations},
	volume = {7},
	issn = {1061-8600},
	url = {http://www.tandfonline.com/doi/abs/10.1080/10618600.1998.10474787},
	doi = {10.1080/10618600.1998.10474787},
	abstract = {We generalize the method proposed by Gelman and Rubin (1992a) for monitoring the convergence of iterative simulations by comparing between and within variances of multiple chains, in order to obtain a family of tests for convergence. We review methods of inference from simulations in order to develop convergence-monitoring summaries that are relevant for the purposes for which the simulations are used. We recommend applying a battery of tests for mixing based on the comparison of inferences from individual sequences and from the mixture of sequences. Finally, we discuss multivariate analogues, for assessing convergence of several parameters simultaneously.},
	pages = {434--455},
	number = {4},
	journaltitle = {Journal of Computational and Graphical Statistics},
	author = {Brooks, Stephen P. and Gelman, Andrew},
	urldate = {2017-09-05},
	date = {1998-12-01},
	keywords = {Markov chain Monte Carlo, Convergence diagnosis, Inference},
	file = {Snapshot:/home/samuel/Zotero/storage/GZ535FEL/10618600.1998.html:text/html}
}

@article{roberts_optimal_2001,
	title = {Optimal scaling for various Metropolis-Hastings algorithms},
	volume = {16},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/euclid.ss/1015346320},
	doi = {10.1214/ss/1015346320},
	abstract = {We review and extend results related to optimal scaling of Metropolis–Hastings algorithms. We present various theoretical results for the high-dimensional limit. We also present simulation studies which confirm the theoretical results in finite-dimensional contexts.},
	pages = {351--367},
	number = {4},
	journaltitle = {Statistical Science},
	shortjournal = {Statist. Sci.},
	author = {Roberts, Gareth O. and Rosenthal, Jeffrey S.},
	urldate = {2017-09-05},
	date = {2001-11},
	mrnumber = {MR1888450},
	zmnumber = {1127.65305},
	file = {Snapshot:/home/samuel/Zotero/storage/W43SKG2U/1015346320.html:text/html}
}

@article{roberts_geometric_1994,
	title = {On the {Geometric} {Convergence} of the {Gibbs} Sampler},
	volume = {56},
	issn = {0035-9246},
	url = {http://www.jstor.org/stable/2345908},
	abstract = {The rate of convergence of the Gibbs sampler is discussed. The Gibbs sampler is a Monte Carlo simulation method with extensive application to computational issues in the Bayesian paradigm. Conditions for the geometric rate of convergence of the algorithm for discrete and continuous parameter spaces are derived, and an illustrative exponential family example is given.},
	pages = {377--384},
	number = {2},
	journaltitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Roberts, Gareth O. and Polson, Nicholas G.},
	urldate = {2017-09-05},
	date = {1994},
	file = {JSTOR Full Text PDF:/home/samuel/Zotero/storage/IGMBMBJY/Roberts and Polson - 1994 - On the Geometric Convergence of the Gibbs Sampler.pdf:application/pdf}
}

@article{gelman_inference_1992,
	title = {Inference from Iterative Simulation Using Multiple Sequences},
	volume = {7},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/euclid.ss/1177011136},
	doi = {10.1214/ss/1177011136},
	abstract = {The Gibbs sampler, the algorithm of Metropolis and similar iterative simulation methods are potentially very helpful for summarizing multivariate distributions. Used naively, however, iterative simulation can give misleading answers. Our methods are simple and generally applicable to the output of any iterative simulation; they are designed for researchers primarily interested in the science underlying the data and models they are analyzing, rather than for researchers interested in the probability theory underlying the iterative simulations themselves. Our recommended strategy is to use several independent sequences, with starting points sampled from an overdispersed distribution. At each step of the iterative simulation, we obtain, for each univariate estimand of interest, a distributional estimate and an estimate of how much sharper the distributional estimate might become if the simulations were continued indefinitely. Because our focus is on applied inference for Bayesian posterior distributions in real problems, which often tend toward normality after transformations and marginalization, we derive our results as normal-theory approximations to exact Bayesian inference, conditional on the observed simulations. The methods are illustrated on a random-effects mixture model applied to experimental measurements of reaction times of normal and schizophrenic patients.},
	pages = {457--472},
	number = {4},
	journaltitle = {Statistical Science},
	shortjournal = {Statist. Sci.},
	author = {Gelman, Andrew and Rubin, Donald B.},
	urldate = {2017-09-02},
	date = {1992-11},
	keywords = {Gibbs sampler, Bayesian inference, convergence of stochastic processes, {ECM}, {EM}, importance sampling, Metropolis algorithm, multiple imputation, random-effects model, {SIR}},
	file = {Snapshot:/home/samuel/Zotero/storage/B54K5F86/1177011136.html:text/html}
}

@article{thompson_covariance-adaptive_2010,
	title = {Covariance-Adaptive Slice Sampling},
	url = {http://arxiv.org/abs/1003.3201},
	abstract = {We describe two slice sampling methods for taking multivariate steps using the crumb framework. These methods use the gradients at rejected proposals to adapt to the local curvature of the log-density surface, a technique that can produce much better proposals when parameters are highly correlated. We evaluate our methods on four distributions and compare their performance to that of a non-adaptive slice sampling method and a Metropolis method. The adaptive methods perform favorably on low-dimensional target distributions with highly-correlated parameters.},
	journaltitle = {{arXiv}:1003.3201 [stat]},
	author = {Thompson, Madeleine and Neal, Radford M.},
	urldate = {2017-09-01},
	date = {2010-03-16},
	eprinttype = {arxiv},
	eprint = {1003.3201},
	keywords = {Statistics - Computation, 65C05},
	file = {arXiv\:1003.3201 PDF:/home/samuel/Zotero/storage/XGPRWZWF/Thompson and Neal - 2010 - Covariance-Adaptive Slice Sampling.pdf:application/pdf;arXiv.org Snapshot:/home/samuel/Zotero/storage/2AJSPE8C/1003.html:text/html}
}

@article{gilks_adaptive_1992,
	title = {Adaptive Rejection Sampling for Gibbs Sampling},
	volume = {41},
	issn = {0035-9254},
	url = {http://www.jstor.org/stable/2347565},
	doi = {10.2307/2347565},
	abstract = {We propose a method for rejection sampling from any univariate log-concave probability density function. The method is adaptive: as sampling proceeds, the rejection envelope and the squeezing function converge to the density function. The rejection envelope and squeezing function are piece-wise exponential functions, the rejection envelope touching the density at previously sampled points, and the squeezing function forming arcs between those points of contact. The technique is intended for situations where evaluation of the density is computationally expensive, in particular for applications of Gibbs sampling to Bayesian models with non-conjugacy. We apply the technique to a Gibbs sampling analysis of monoclonal antibody reactivity.},
	pages = {337--348},
	number = {2},
	journaltitle = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
	author = {Gilks, W. R. and Wild, P.},
	urldate = {2017-09-01},
	date = {1992}
}

@article{neal_slice_2003,
	title = {Slice sampling},
	volume = {31},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1056562461},
	doi = {10.1214/aos/1056562461},
	abstract = {Markov chain sampling methods that adapt to characteristics of the distribution being sampled can be constructed using the principle that one can ample from a distribution by sampling uniformly from the region under the plot of its density function. A Markov chain that converges to this uniform distribution can be constructed by alternating uniform sampling in the vertical direction with uniform sampling from the horizontal "slice" defined by the current vertical position, or more generally, with some update that leaves the uniform distribution over this slice invariant. Such "slice sampling" methods are easily implemented for univariate distributions, and can be used to sample from a multivariate distribution by updating each variable in turn. This approach is often easier to implement than Gibbs sampling and more efficient than simple Metropolis updates, due to the ability of slice sampling to adaptively choose the magnitude of changes made. It is therefore attractive for routine and automated use. Slice sampling methods that update all variables simultaneously are also possible. These methods can adaptively choose the magnitudes of changes made to each variable, based on the local properties of the density function. More ambitiously, such methods could potentially adapt to the dependencies between variables by constructing local quadratic approximations. Another approach is to improve sampling efficiency by suppressing random walks. This can be done for univariate slice sampling by "overrelaxation," and for multivariate slice sampling by "reflection" from the edges of the slice.},
	pages = {705--767},
	number = {3},
	journaltitle = {The Annals of Statistics},
	shortjournal = {Ann. Statist.},
	author = {Neal, Radford M.},
	urldate = {2017-09-01},
	date = {2003-06},
	mrnumber = {MR1994729},
	zmnumber = {1051.65007},
	keywords = {Markov chain Monte Carlo, Gibbs sampling, Metropolis algorithm, adaptive methods, auxiliary variables, dynamical methods, overrelaxation},
	file = {Snapshot:/home/samuel/Zotero/storage/2FS5J2U7/DPubS.html:text/html}
}

@book{lunn_bugs_2012,
	title = {The {BUGS} Book: A Practical Introduction to Bayesian Analysis},
	isbn = {978-1-58488-849-9},
	shorttitle = {The {BUGS} Book},
	abstract = {Bayesian statistical methods have become widely used for data analysis and modelling in recent years, and the {BUGS} software has become the most popular software for Bayesian analysis worldwide. Authored by the team that originally developed this software, The {BUGS} Book provides a practical introduction to this program and its use. The text presents complete coverage of all the functionalities of {BUGS}, including prediction, missing data, model criticism, and prior sensitivity. It also features a large number of worked examples and a wide range of applications from various disciplines.  The book introduces regression models, techniques for criticism and comparison, and a wide range of modelling issues before going into the vital area of hierarchical models, one of the most common applications of Bayesian methods. It deals with essentials of modelling without getting bogged down in complexity. The book emphasises model criticism, model comparison, sensitivity analysis to alternative priors, and thoughtful choice of prior distributions—all those aspects of the "art" of modelling that are easily overlooked in more theoretical expositions.   More pragmatic than ideological, the authors systematically work through the large range of "tricks" that reveal the real power of the {BUGS} software, for example, dealing with missing data, censoring, grouped data, prediction, ranking, parameter constraints, and so on. Many of the examples are biostatistical, but they do not require domain knowledge and are generalisable to a wide range of other application areas.   Full code and data for examples, exercises, and some solutions can be found on the book’s website.},
	pagetotal = {402},
	publisher = {{CRC} Press},
	author = {Lunn, David and Jackson, Chris and Best, Nicky and Thomas, Andrew and Spiegelhalter, David},
	date = {2012-10-02},
	langid = {english},
	note = {Google-Books-{ID}: Cthz3XMa\_VQC},
	keywords = {Mathematics / Probability \& Statistics / General, Computers / Mathematical \& Statistical Software}
}

@book{coffey_langevin_2004,
	title = {The Langevin Equation: With Applications to Stochastic Problems in Physics, Chemistry, and Electrical Engineering},
	isbn = {978-981-279-509-0},
	shorttitle = {The Langevin Equation},
	abstract = {This volume is the second edition of the first-ever elementary book on the Langevin equation method for the solution of problems involving the Brownian motion in a potential, with emphasis on modern applications in the natural sciences, electrical engineering and so on. It has been substantially enlarged to cover in a succinct manner a number of new topics, such as anomalous diffusion, continuous time random walks, stochastic resonance etc, which are of major current interest in view of the large number of disparate physical systems exhibiting these phenomena. The book has been written in such a way that all the material should be accessible to an advanced undergraduate or beginning graduate student. It draws together, in a coherent fashion, a variety of results which have hitherto been available only in the form of research papers or scattered review articles. Contents: Historical Background and Introductory Concepts; Langevin Equations and Methods of Solution; Brownian Motion of a Free Particle and a Harmonic Oscillator; Two-Dimensional Rotational Brownian Motion in N -Fold Cosine Potentials; Brownian Motion in a Tilted Cosine Potential: Application to the Josephson Tunnelling Junction; Translational Brownian Motion in a Double-Well Potential; Three-Dimensional Rotational Brownian Motion in an External Potential: Application to the Theory of Dielectric and Magnetic Relaxation; Rotational Brownian Motion in Axially Symmetric Potentials: Matrix Continued Fraction Solutions; Rotational Brownian Motion in Non-Axially Symmetric Potentials; Inertial Langevin Equations: Application to Orientational Relaxation in Liquids; Anomalous Diffusion. Readership: Advanced undergraduates, graduate students, academics and researchers in statistical physics, condensed matter physics and magnetism, the physics of fluids, theoretical chemistry and applied mathematics.},
	pagetotal = {706},
	publisher = {World Scientific},
	author = {Coffey, William and Kalmykov, Yu P. and Waldron, J. T.},
	date = {2004},
	langid = {english},
	note = {Google-Books-{ID}: {dFBZMLJsQ}5gC},
	keywords = {Science / Physics / Mathematical \& Computational}
}

@book{reif_fundamentals_2009,
	title = {Fundamentals of Statistical and Thermal Physics},
	isbn = {978-1-4786-1005-2},
	abstract = {All macroscopic systems consist ultimately of atoms obeying the laws of quantum mechanics. That premise forms the basis for this comprehensive text, intended for a first upper-level course in statistical and thermal physics. Reif emphasizes that the combination of microscopic concepts with some statistical postulates leads readily to conclusions on a purely macroscopic level. The authors writing style and penchant for description energize interest in condensed matter physics as well as provide a conceptual grounding with information that is crystal clear and memorable.Reif first introduces basic probability concepts and statistical methods used throughout all of physics. Statistical ideas are then applied to systems of particles in equilibrium to enhance an understanding of the basic notions of statistical mechanics, from which derive the purely macroscopic general statements of thermodynamics. Next, he turns to the more complicated equilibrium situations, such as phase transformations and quantum gases, before discussing nonequilibrium situations in which he treats transport theory and dilute gases at varying levels of sophistication. In the last chapter, he addresses some general questions involving irreversible processes and fluctuations.A large amount of material is presented to facilitate students later access to more advanced works, to allow those with higher levels of curiosity to read beyond the minimum given on a topic, and to enhance understanding by presenting several ways of looking at a particular question. Formatting within the text either signals material that instructors can assign at their own discretion or highlights important results for easy reference to them. Additionally, by solving many of the 230 problems contained in the text, students activate and embed their knowledge of the subject matter.},
	pagetotal = {672},
	publisher = {Waveland Press},
	author = {Reif, F.},
	date = {2009-01-05},
	langid = {english},
	note = {Google-Books-{ID}: {ObsbAAAAQBAJ}},
	keywords = {Science / Mechanics / Thermodynamics}
}

@book{rogers_diffusions_1994,
	title = {Diffusions, Markov Processes, and Martingales: Volume 1, Foundations},
	isbn = {978-0-521-77594-6},
	shorttitle = {Diffusions, Markov Processes, and Martingales},
	abstract = {Now available in paperback, this celebrated book remains a key systematic guide to a large part of the modern theory of Probability. The authors not only present the subject of Brownian motion as a dry part of mathematical analysis, but convey its real meaning and fascination. The opening, heuristic chapter does just this, and it is followed by a comprehensive and self-contained account of the foundations of theory of stochastic processes. Chapter 3 is a lively presentation of the theory of Markov processes. Together with its companion volume, this book equips graduate students for research into a subject of great intrinsic interest and wide applications.},
	pagetotal = {412},
	publisher = {Cambridge University Press},
	author = {Rogers, L. C. G. and Williams, David},
	date = {1994},
	langid = {english},
	note = {Google-Books-{ID}: {eJp}330pIvQ4C},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Number Theory, Medical / General}
}

@article{roberts_optimal_1998,
	title = {Optimal scaling of discrete approximations to Langevin diffusions},
	volume = {60},
	issn = {1467-9868},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/1467-9868.00123/abstract},
	doi = {10.1111/1467-9868.00123},
	abstract = {We consider the optimal scaling problem for proposal distributions in Hastings–Metropolis algorithms derived from Langevin diffusions. We prove an asymptotic diffusion limit theorem and show that the relative efficiency of the algorithm can be characterized by its overall acceptance rate, independently of the target distribution. The asymptotically optimal acceptance rate is 0.574. We show that, as a function of dimension n, the complexity of the algorithm is O(n1/3), which compares favourably with the O(n) complexity of random walk Metropolis algorithms. We illustrate this comparison with some example simulations.},
	pages = {255--268},
	number = {1},
	journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Roberts, Gareth O. and Rosenthal, Jeffrey S.},
	urldate = {2017-09-01},
	date = {1998-01-01},
	langid = {english},
	keywords = {Hastings–Metropolis algorithm, Langevin algorithm, Markov chain Monte Carlo method, Weak convergence},
	file = {Snapshot:/home/samuel/Zotero/storage/GPKE976I/abstract.html:text/html}
}

@book{gidas_metropolis-type_1991,
	title = {Metropolis-type Monte Carlo Simulation Algorithms and Simulated Annealing},
	pagetotal = {88},
	publisher = {Brown Univ.},
	author = {Gidas, Basilis},
	date = {1991},
	langid = {english},
	note = {Google-Books-{ID}: {MVtUPwAACAAJ}}
}

@book{ripley_stochastic_1987,
	title = {Stochastic simulation},
	isbn = {978-0-471-81884-7},
	abstract = {{WILEY}-{INTERSCIENCE} {PAPERBACK} {SERIESThe} Wiley-Interscience Paperback Series consists of selected books that have been made more accessible to consumers in an effort to increase global appeal and general circulation. With these new unabridged softcover volumes, Wiley hopes to extend the lives of these works by making them available to future generations of statisticians, mathematicians, and scientists.". . .this is a very competently written and useful addition to the statistical literature; a book every statistician should look at and that many should study!"-Short Book Reviews, International Statistical Institute". . .reading this book was an enjoyable learning experience. The suggestions and recommendations on the methods [make] this book an excellent reference for anyone interested in simulation. With its compact structure and good coverage of material, it [is] an excellent textbook for a simulation course."-Technometrics". . .this work is an excellent comprehensive guide to simulation methods, written by a very competent author. It is especially recommended for those users of simulation methods who want more than a 'cook book'. "-Mathematics {AbstractsThis} book is a comprehensive guide to simulation methods with explicit recommendations of methods and algorithms. It covers both the technical aspects of the subject, such as the generation of random numbers, non-uniform random variates and stochastic processes, and the use of simulation. Supported by the relevant mathematical theory, the text contains a great deal of unpublished research material, including coverage of the analysis of shift-register generators, sensitivity analysis of normal variate generators, analysis of simulation output, and more.},
	pagetotal = {270},
	publisher = {J. Wiley},
	author = {Ripley, Brian D.},
	date = {1987-02-04},
	langid = {english},
	note = {Google-Books-{ID}: {VW}1HAAAAMAAJ},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes, Stochastic processes, Digital computer simulation}
}

@article{besag_bayesian_1995,
	title = {Bayesian Computation and Stochastic Systems},
	volume = {10},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/euclid.ss/1177010123},
	doi = {10.1214/ss/1177010123},
	abstract = {Markov chain Monte Carlo ({MCMC}) methods have been used extensively in statistical physics over the last 40 years, in spatial statistics for the past 20 and in Bayesian image analysis over the last decade. In the last five years, {MCMC} has been introduced into significance testing, general Bayesian inference and maximum likelihood estimation. This paper presents basic methodology of {MCMC}, emphasizing the Bayesian paradigm, conditional probability and the intimate relationship with Markov random fields in spatial statistics. Hastings algorithms are discussed, including Gibbs, Metropolis and some other variations. Pairwise difference priors are described and are used subsequently in three Bayesian applications, in each of which there is a pronounced spatial or temporal aspect to the modeling. The examples involve logistic regression in the presence of unobserved covariates and ordinal factors; the analysis of agricultural field experiments, with adjustment for fertility gradients; and processing of low-resolution medical images obtained by a gamma camera. Additional methodological issues arise in each of these applications and in the Appendices. The paper lays particular emphasis on the calculation of posterior probabilities and concurs with others in its view that {MCMC} facilitates a fundamental breakthrough in applied Bayesian modeling.},
	pages = {3--41},
	number = {1},
	journaltitle = {Statistical Science},
	shortjournal = {Statist. Sci.},
	author = {Besag, Julian and Green, Peter and Higdon, David and Mengersen, Kerrie},
	urldate = {2017-09-01},
	date = {1995-02},
	mrnumber = {MR1349818},
	zmnumber = {0955.62552},
	keywords = {Markov chain Monte Carlo, Gibbs sampler, Bayesian inference, Markov random fields, Agricultural field experiments, conditional distributions, deconvolution, gamma-camera imaging, Hastings algorithms, image analysis, logistic regression, Metropolis method, prostate cancer, simultaneous credible regions, spatial statistics, time reversibility, unobserved covariates, variety trials},
	file = {Snapshot:/home/samuel/Zotero/storage/GUUPQKBS/1177010123.html:text/html}
}

@article{gilks_adaptive_1995,
	title = {Adaptive Rejection Metropolis Sampling within Gibbs Sampling},
	volume = {44},
	issn = {0035-9254},
	url = {http://www.jstor.org/stable/2986138},
	abstract = {Gibbs sampling is a powerful technique for statistical inference. It involves little more than sampling from full conditional distributions, which can be both complex and computationally expensive to evaluate. Gilks and Wild have shown that in practice full conditionals are often log-concave, and they proposed a method of adaptive rejection sampling for efficiently sampling from univariate log-concave distributions. In this paper, to deal with non-log-concave full conditional distributions, we generalize adaptive rejection sampling to include a Hastings-Metropolis algorithm step. One important field of application in which statistical models may lead to non-log-concave full conditionals is population pharmacokinetics. Here, the relationship between drug dose and blood or plasma concentration in a group of patients typically is modelled by using non-linear mixed effects models. Often, the data used for analysis are routinely collected hospital measurements, which tend to be noisy and irregular. Consequently, a robust (t-distributed) error structure is appropriate to account for outlying observations and/or patients. We propose a robust non-linear full probability model for population pharmacokinetic data. We demonstrate that our method enables Bayesian inference for this model, through an analysis of antibiotic administration in new-born babies.},
	pages = {455--472},
	number = {4},
	journaltitle = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
	author = {Gilks, W. R. and Best, N. G. and Tan, K. K. C.},
	urldate = {2017-09-01},
	date = {1995},
	file = {JSTOR Full Text PDF:/home/samuel/Zotero/storage/8HQZGAQG/Gilks et al. - 1995 - Adaptive Rejection Metropolis Sampling within Gibb.pdf:application/pdf}
}

@book{bonamente_statistics_2016,
	title = {Statistics and Analysis of Scientific Data},
	isbn = {978-1-4939-6572-4},
	abstract = {The revised second edition of this textbook provides the reader with a solid foundation in probability theory and statistics as applied to the physical sciences, engineering and related fields. It covers a broad range of numerical and analytical methods that are essential for the correct analysis of scientific data, including probability theory, distribution functions of statistics, fits to two-dimensional data and parameter estimation, Monte Carlo methods and Markov chains.  Features new to this edition include:  • a discussion of statistical techniques employed in business science, such as multiple regression analysis of multivariate datasets.• a new chapter on the various measures of the mean including logarithmic averages.• new chapters on systematic errors and intrinsic scatter, and on the fitting of data with bivariate errors.• a new case study and additional worked examples.• mathematical derivations and theoretical background material have been appropriately marked, to improve the readability of the text.• end-of-chapter summary boxes, for easy reference. As in the first edition, the main pedagogical method is a theory-then-application approach, where emphasis is placed first on a sound understanding of the underlying theory of a topic, which becomes the basis for an efficient and practical application of the material. The level is appropriate for undergraduates and beginning graduate students, and as a reference for the experienced researcher. Basic calculus is used in some of the derivations, and no previous background in probability and statistics is required. The book includes many numerical tables of data, as well as exercises and examples to aid the readers' understanding of the topic.},
	pagetotal = {323},
	publisher = {Springer},
	author = {Bonamente, Massimiliano},
	date = {2016-11-08},
	langid = {english},
	note = {Google-Books-{ID}: i9F2DQAAQBAJ},
	keywords = {Mathematics / Probability \& Statistics / General, Science / Physics / General, Business \& Economics / Statistics, Mathematics / Applied, Science / Physics / Mathematical \& Computational}
}

@article{liu_covariance_1994,
	title = {Covariance structure of the Gibbs sampler with applications to the comparisons of estimators and augmentation schemes},
	volume = {81},
	issn = {0006-3444},
	url = {https://academic.oup.com/biomet/article/81/1/27/252043/Covariance-structure-of-the-Gibbs-sampler-with},
	doi = {10.1093/biomet/81.1.27},
	abstract = {We study the covariance structure of a Markov chain generated by the Gibbs sampler, with emphasis on data augmentation. When applied to a Bayesian missing data problem, the Gibbs sampler produces two natural approximations for the posterior distribution of the parameter vector: the empirical distribution based on the sampled values of the parameter vector, and a mixture of complete data posteriors. We prove that Rao-Blackwellization causes a one-lag delay for the autocovariances among dependent samples obtained from data augmentation, and consequently, the mixture approximation produces estimates with smaller variances than the empirical approximation. The covariance structure results are used to compare different augmentation schemes. It is shown that collapsing and grouping random components in a Gibbs sampler with two or three components usually result in more efficient sampling schemes.},
	pages = {27--40},
	number = {1},
	journaltitle = {Biometrika},
	shortjournal = {Biometrika},
	author = {Liu, Jun S. and Wong, Wing Hung and Kong, Augustine},
	urldate = {2017-08-31},
	date = {1994-03-01},
	file = {Snapshot:/home/samuel/Zotero/storage/RMDFNJEM/252043.html:text/html}
}

@book{robert_bayesian_2001,
	title = {The {Bayesian} {Choice}: {From} {Decision}-{Theoretic} {Foundations} to {Computational} {Implementation}},
	isbn = {978-0-387-95231-4},
	year = 2001,
	shorttitle = {The Bayesian Choice},
	abstract = {{WINNER} {OF} {THE} 2003 {DEGROOT} {PRIZE}! The {DeGroot} Prize is awarded every two years by the International Society for Bayesian Analysis in recognition of an important, timely, thorough and notably original contribution to the statistics literature. This graduate-level textbook presents an introduction to Bayesian statistics and decision theory. Its scope covers both the basic ideas of statistical theory, and also some of the more modern and advanced topics of Bayesian statistics such as complete class theorems, the Stein effect, Bayesian model choice, hierarchical and empirical Bayes modeling, Monte Carlo integration, including Gibbs sampling and other {MCMC} techniques. The second edition includes a new chapter on model choice (Chapter 7) and the chapter on Bayesian calculations (6) has been extensively revised. Chapter 4 includes a new section on dynamic models. In Chapter 3, the material on noninformative priors has been expanded, and Chapter 10 has been supplemented with more examples. The Bayesian Choice will be suitable as a text for courses on Bayesian analysis, decision theory or a combination of them.},
	pagetotal = {640},
	publisher = {Springer Science \& Business Media},
	author = {Robert, Christian},
	date = {2001-05-25},
	langid = {english},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes}
}

@article{schervish_convergence_1992,
	title = {On The Convergence Of Successive Substitution Sampling},
	volume = {1},
	issn = {1061-8600},
	url = {https://experts.umn.edu/en/publications/on-the-convergence-of-successive-substitution-sampling},
	doi = {10.1080/10618600.1992.10477008},
	pages = {111--127},
	number = {2},
	journaltitle = {Journal of Computational and Graphical Statistics},
	author = {Schervish, Mark J. and Carlin, Bradley P.},
	urldate = {2017-08-31},
	date = {1992},
	file = {Snapshot:/home/samuel/Zotero/storage/X62S9VWY/on-the-convergence-of-successive-substitution-sampling.html:text/html}
}

@article{diaconis_gibbs_2010,
	title = {Gibbs Sampling, Conjugate Priors and Coupling},
	volume = {72},
	issn = {0976-836X},
	url = {http://www.jstor.org/stable/41941459},
	doi = {10.2307/41941459},
	abstract = {We give a large family of simple examples where a sharp analysis of the Gibbs sampler can be proved by coupling. These examples involve standard statistical models – exponential families with conjugate priors or location families with natural priors. Our main approach uses a single eigenfunction (always explicitly available in the examples in question) and stochastic monotonicity. We give a satisfactory treatment of several examples that have defeated previous attempts at analysis.},
	pages = {136--169},
	number = {1},
	journaltitle = {Sankhyā: The Indian Journal of Statistics, Series A (2008-)},
	author = {Diaconis, Persi and Khare, Kshitij and Saloff-Coste, Laurent},
	urldate = {2017-08-31},
	date = {2010}
}

@article{diaconis_gibbs_2008,
	title = {Gibbs Sampling, Exponential Families and Orthogonal Polynomials},
	volume = {23},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/euclid.ss/1219339107},
	doi = {10.1214/07-STS252},
	abstract = {We give families of examples where sharp rates of convergence to stationarity of the widely used Gibbs sampler are available. The examples involve standard exponential families and their conjugate priors. In each case, the transition operator is explicitly diagonalizable with classical orthogonal polynomials as eigenfunctions.},
	pages = {151--178},
	number = {2},
	journaltitle = {Statistical Science},
	shortjournal = {Statist. Sci.},
	author = {Diaconis, Persi and Khare, Kshitij and Saloff-Coste, Laurent},
	urldate = {2017-08-31},
	date = {2008-05},
	mrnumber = {MR2446500},
	zmnumber = {1327.62058},
	keywords = {Gibbs sampler, conjugate priors, exponential families, location families, orthogonal polynomials, running time analyses, singular value decomposition},
	file = {Snapshot:/home/samuel/Zotero/storage/Q3FZDCU7/1219339107.html:text/html}
}

@article{arnold_compatible_1989,
	title = {Compatible Conditional Distributions},
	volume = {84},
	issn = {0162-1459},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1989.10478750},
	doi = {10.1080/01621459.1989.10478750},
	abstract = {Consider two families of candidate conditional densities (or probability mass functions), f(x {\textbar} y);y ∈ S y and f(y {\textbar} x): x ∈ S x. This article investigates necessary and sufficient conditions for the existence of a joint density (or joint probability mass function) f(x, y) with the given families as its associated conditional densities. This supplements previous work that has addressed the question of uniqueness of f(x, y) assuming its existence.},
	pages = {152--156},
	number = {405},
	journaltitle = {Journal of the American Statistical Association},
	author = {Arnold, Barry C. and Press, S. James},
	urldate = {2017-08-31},
	date = {1989-03-01},
	keywords = {Characterizations, Compatibility},
	file = {Snapshot:/home/samuel/Zotero/storage/XCJHYU4E/01621459.1989.html:text/html}
}

@article{minlos_limiting_1967,
	title = {Limiting Gibbs' distribution},
	volume = {1},
	issn = {0016-2663, 1573-8485},
	url = {https://link.springer.com/article/10.1007/BF01076086},
	doi = {10.1007/BF01076086},
	abstract = {No Abstract available for this article.},
	pages = {140--150},
	number = {2},
	journaltitle = {Functional Analysis and Its Applications},
	shortjournal = {Funct Anal Its Appl},
	author = {Minlos, R. A.},
	urldate = {2017-08-31},
	date = {1967-04-01},
	langid = {english},
	file = {Snapshot:/home/samuel/Zotero/storage/EFIBJRJH/BF01076086.html:text/html}
}

@article{diaconis_conjugate_1979,
	title = {Conjugate Priors for Exponential Families},
	volume = {7},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1176344611},
	doi = {10.1214/aos/1176344611},
	abstract = {Let {XXX} be a random vector distributed according to an exponential family with natural parameter θ∈Θθ∈Θ{\textbackslash}theta {\textbackslash}in {\textbackslash}Theta. We characterize conjugate prior measures on ΘΘ{\textbackslash}Theta through the property of linear posterior expectation of the mean parameter of X:E\{E(X{\textbar}θ){\textbar}X=x\}=ax+{bX}:E\{E(X{\textbar}θ){\textbar}X=x\}=ax+{bX} : E{\textbackslash}\{E(X{\textbar}{\textbackslash}theta){\textbar}X = x{\textbackslash}\} = ax + b. We also delineate which hyperparameters permit such conjugate priors to be proper.},
	pages = {269--281},
	number = {2},
	journaltitle = {The Annals of Statistics},
	shortjournal = {Ann. Statist.},
	author = {Diaconis, Persi and Ylvisaker, Donald},
	urldate = {2017-08-31},
	date = {1979-03},
	mrnumber = {MR520238},
	zmnumber = {0405.62011},
	keywords = {exponential families, admissibility, Bayesian analysis, characterization theorems, Conjugate priors, credibility theory, linearity of regression},
	file = {Snapshot:/home/samuel/Zotero/storage/6433IZTR/1176344611.html:text/html}
}

@article{roberts_geometric_1996,
	title = {Geometric convergence and central limit theorems for multidimensional Hastings and Metropolis algorithms},
	volume = {83},
	issn = {0006-3444},
	url = {https://academic.oup.com/biomet/article/83/1/95/255487/Geometric-convergence-and-central-limit-theorems},
	doi = {10.1093/biomet/83.1.95},
	abstract = {We develop results on geometric ergodicity of Markov chains and apply these and other recent results in Markov chain theory to multidimensional Hastings and Metropolis algorithms. For those based on random walk candidate distributions, we find sufficient conditions for moments and moment generating functions to converge at a geometric rate to a prescribed distribution π. By phrasing the conditions in terms of the curvature of the densities we show that the results apply to all distributions with positive densities in a large class which encompasses many commonly-used statistical forms. From these results we develop central limit theorems for the Metropolis algorithm. Converse results, showing non-geometric convergence rates for chains where the rejection rate is not bounded away from unity, are also given; these show that the negative-definiteness property is not redundant.},
	pages = {95--110},
	number = {1},
	journaltitle = {Biometrika},
	shortjournal = {Biometrika},
	author = {Roberts, G. O. and Tweedie, R. L.},
	urldate = {2017-08-25},
	date = {1996-03-01},
	file = {Snapshot:/home/samuel/Zotero/storage/R9YFUKTN/Geometric-convergence-and-central-limit-theorems.html:text/html}
}

@book{ohagan_kendalls_2010,
	location = {Chichester},
	edition = {1 edition},
	title = {Kendalls Advanced Theory of Statistic 2B},
	isbn = {978-0-470-68569-3},
	pagetotal = {496},
	publisher = {Wiley},
	author = {O'Hagan, Anthony},
	date = {2010-03-08}
}

@book{fahrmeir_regression_2009,
  title={Regression: {Models}, {Methods} and {Applications}},
  author={Fahrmeir, Ludwig and Kneib, Thomas and Lang, Stefan and Marx, Brian},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@misc{banerjee_mcmc_2008,
	title = {{MCMC} algorithms for fitting Bayesian models},
	url = {http://www.biostat.umn.edu/~ph7440/},
	publisher = {University of Minnesota School of Public Health},
	author = {Banerjee, Sudipto},
	urldate = {2017-10-07},
	date = {2008}
}

@phdthesis{phdthesis_cowles,
  author       = {M. K. Cowles},
  title        = {Practical issues in Gibbs sampler implementation with application to Bayesian hierarchical modelling of clinical trial data},
  school       = {Division of Biostatistics, University of Minnesota},
  year         = 1994
}

@article{plummer2006coda,
  title={CODA: convergence diagnosis and output analysis for MCMC},
  author={Plummer, Martyn and Best, Nicky and Cowles, Kate and Vines, Karen},
  journal={R news},
  volume={6},
  number={1},
  pages={7--11},
  year={2006}
}

@incollection{beskos2009computational,
  title={Computational complexity of Metropolis-Hastings methods in high dimensions},
  author={Beskos, Alexandros and Stuart, Andrew},
  booktitle={Monte Carlo and Quasi-Monte Carlo Methods 2008},
  pages={61--71},
  year={2009},
  publisher={Springer}
}

@article{sherlock2010random,
  title={The {Random} {Walk} {Metropolis}: {Linking} {Theory} and {Practice} through a {Case} {Study}},
  author={Sherlock, Chris and Fearnhead, Paul and Roberts, Gareth O and others},
  journal={Statistical Science},
  volume={25},
  number={2},
  pages={172--190},
  year={2010},
  publisher={Institute of Mathematical Statistics}
}

@article{gelman1996efficient,
  title={Efficient {Metropolis} {Jumping} {Rules}},
  author={Gelman, Andrew and Roberts, Gareth O and Gilks, Walter R and others},
  journal={Bayesian Statistics},
  volume={5},
  number={599-608},
  pages={42},
  year={1996}
}

@book{strang2007computational,
  title={Computational {Science} and {Engineering}},
  author={Strang, Gilbert},
  volume={791},
  year={2007},
  publisher={Wellesley-Cambridge Press Wellesley}
}

@article{gelman1993iterative,
  title={Iterative and {Non}-{Iterative} {Simulation} {Algorithms}},
  author={Gelman, Andrew},
  journal={Computing Science and Statistics},
  pages={433--433},
  year={1993},
  publisher={PROCEEDINGS PUBLISHED BY VARIOUS PUBLISHERS}
}

@misc{xing2012,
  author        = {Eric Xing, Jessica Chemali, Seungwhan Moon},
  title         = {Advanced topics in MCMC},
  year          = {2014},
  publisher={Carnegie Mellon University}
}

@article{hairer2003geometric,
  title={Geometric {Numerical} {Integration} {Illustrated} by the {St{\"o}rmer}--{Verlet} {Method}},
  author={Hairer, Ernst and Lubich, Christian and Wanner, Gerhard},
  journal={Acta Numerica},
  volume={12},
  pages={399--450},
  year={2003},
  publisher={Cambridge University Press}
}

@book{feynman2017character,
  title={The {Character} of {Physical} {Law}},
  author={Feynman, Richard},
  year={2017},
  publisher={MIT press}
}

@book{newton1833philosophiae,
  title={Philosophiae {Naturalis} {Principia} {Mathematica}},
  author={Newton, Isaac},
  volume={1},
  year={1833},
  publisher={G. Brookman}
}

@article{kass1998markov,
  title={{Markov} {Chain} {Monte} {Carlo} in {Practice}: a {Roundtable} {Discussion}},
  author={Kass, Robert E and Carlin, Bradley P and Gelman, Andrew and Neal, Radford M},
  journal={The American Statistician},
  volume={52},
  number={2},
  pages={93--100},
  year={1998},
  publisher={Taylor \& Francis Group}
}

@article{pakman2014exact,
  title={Exact {Hamiltonian} {Monte} {Carlo} for {Truncated} {Multivariate} {Gaussians}},
  author={Pakman, Ari and Paninski, Liam},
  journal={Journal of Computational and Graphical Statistics},
  volume={23},
  number={2},
  pages={518--542},
  year={2014},
  publisher={Taylor \& Francis}
}

@misc{stan2018stan,
  title={Stan {Modeling} {Language}: {User's} {Guide} and {Reference} {Manual}},
  author={Stan Development Team},
  year={2018},
  publisher={Version 2.18.0}
}

@misc{betancourt2017qr,
  title = {The {QR} {Decomposition} For {Regression} {Models}},
  howpublished = {\url{http://mc-stan.org/users/documentation/case-studies/qr_regression.html}},
  author="Michael Betancourt",
  year = "2017",
  note = {Accessed: 2018-10-09}
}

@Misc{rstanarm2016,
  title = {{Rstanarm}: {Bayesian} {Applied} {Regression} {Modeling} via
    {Stan}.},
  author = {{Stan Development Team}},
  note = {R package version 2.13.1},
  year = {2016},
  url = {http://mc-stan.org/},
}

@article{tran2016edward,
  author = {Dustin Tran and Alp Kucukelbir and Adji B. Dieng and Maja Rudolph and Dawen Liang and David M. Blei},
  title = {{Edward: {A} {Library} for {Probabilistic} {Modeling}, {Inference}, and {Criticism}}},
  journal = {arXiv preprint arXiv:1610.09787},
  year = {2016}
}


@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Systems}},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Martin~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Viegas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@Article{brms2017,
    title = {{Brms}: An {R} {Package} for {Bayesian} {Multilevel} {Models}
      {Using} {Stan}},
    author = {Paul-Christian Burkner},
    journal = {Journal of Statistical Software},
    year = {2017},
    volume = {80},
    number = {1},
    pages = {1--28},
    doi = {10.18637/jss.v080.i01},
    encoding = {UTF-8},
}

@article{blei2017variational,
  title={Variational inference: A review for statisticians},
  author={Blei, David M and Kucukelbir, Alp and McAuliffe, Jon D},
  journal={Journal of the American Statistical Association},
  volume={112},
  number={518},
  pages={859--877},
  year={2017},
  publisher={Taylor \& Francis}
}

@article{jordan1999introduction,
  title={An introduction to variational methods for graphical models},
  author={Jordan, Michael I and Ghahramani, Zoubin and Jaakkola, Tommi S and Saul, Lawrence K},
  journal={Machine learning},
  volume={37},
  number={2},
  pages={183--233},
  year={1999},
  publisher={Springer}
}

